{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Example 1 Chat Prompt Templates\n"
      ],
      "metadata": {
        "id": "cNqmb5KBsDDj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1w7ytqPsjDO",
        "outputId": "91c3947e-091f-458d-88d6-cd0521ee5a44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.3.74)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.99.9)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-0.3.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
        "\n",
        "# Define the chat prompt template (system + human)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful AI assistant that responds in English.\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Format the prompt with the question\n",
        "formatted_prompt = prompt.format_messages(question=\"What is Python?\")\n",
        "\n",
        "print(type(prompt))\n",
        "print(type(formatted_prompt))\n",
        "\n",
        "# Invoke the model with the formatted prompt\n",
        "response = llm.invoke(formatted_prompt)\n",
        "\n",
        "print(type(response))\n",
        "\n",
        "#print(response)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7hPYTTwsFdb",
        "outputId": "52f4f946-a0cd-476a-e842-96611777fbfb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
            "<class 'list'>\n",
            "<class 'langchain_core.messages.ai.AIMessage'>\n",
            "Python is a high-level, interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It is widely used for web development, data analysis, artificial intelligence, scientific computing, automation, and more. Python's extensive standard library and large community contribute to its popularity and versatility.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2"
      ],
      "metadata": {
        "id": "H6INQ2q5shKv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
        "\n",
        "\n",
        "# Define the chat prompt template (system + human)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful trainer. Be concise and generate simple explainations.\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# RAM , In-Memory Databases\n",
        "chat_history =( [\n",
        "                HumanMessage(content=\"Can you help me learn what is Langchain?\"),\n",
        "                AIMessage(content=\"OfCourse Yes! What in langchain you want to learn?\")\n",
        "            ])\n",
        "\n",
        "\n",
        "messages = prompt.format_messages(chat_history=chat_history, question=\"What is Chat Prompt Templates Component?\")\n",
        "\n",
        "# Invoke the model with the formatted prompt\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "#print(response)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvTJY7uYt1Kl",
        "outputId": "2eb00212-cfaf-43dc-d617-9cc10b3cadab"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Chat Prompt Templates component helps you create and manage reusable prompts for chat models. It allows you to define a template with placeholders, so you can easily customize prompts for different situations. This makes your interactions with chat models more consistent and efficient.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning Resources - YT, Udemy, any other -> diff syntax, and diff libraries/packages\n",
        "# We are using latest packages"
      ],
      "metadata": {
        "id": "KXAU6qnnzhMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Maintain History - manually ask LLM to loop in the history (list) for context\n",
        "\n",
        "# Chains examples , no chains at all for chat prompt templates"
      ],
      "metadata": {
        "id": "H74ogBWzvx2b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0.1)\n",
        "\n",
        "# Define the prompt with a placeholder for history\n",
        "prompt_with_history = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a friendly assistant. Keep your responses brief.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# Create a sample chat history\n",
        "sample_history = [\n",
        "    HumanMessage(content=\"What is the capital of India?\"),\n",
        "    AIMessage(content=\"The capital of India is New Delhi.\"),\n",
        "    HumanMessage(content=\"What is a famous landmark there?\"),\n",
        "    AIMessage(content=\"A famous landmark in New Delhi is the India Gate.\")\n",
        "]\n",
        "\n",
        "\n",
        "# Format the prompt with the history and new user input\n",
        "messages = prompt_with_history.format_messages(\n",
        "    chat_history=sample_history,\n",
        "    user_input=\"Share one interesting fact about the landmark?\"\n",
        ")\n",
        "\n",
        "# Red Fort\n",
        "\n",
        "# Invoke the model\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h3mi3jM4gRC",
        "outputId": "3ac63043-0427-461e-c5a3-adebda5f5589"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India Gate is a war memorial dedicated to soldiers who fought in World War I and the Third Anglo-Afghan War.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0.1)\n",
        "\n",
        "# Define the prompt with a placeholder for history\n",
        "prompt_with_history = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a friendly assistant. Keep your responses brief.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# Initialize an empty chat history\n",
        "chat_history = []\n",
        "\n",
        "# First turn\n",
        "user_input_1 = \"What is the capital of India?\"\n",
        "messages_1 = prompt_with_history.format_messages(\n",
        "    chat_history=chat_history,\n",
        "    user_input=user_input_1\n",
        ")\n",
        "response_1 = llm.invoke(messages_1)\n",
        "print(f\"User: {user_input_1}\")\n",
        "print(f\"Assistant: {response_1.content}\")\n",
        "\n",
        "# Append the first turn to history\n",
        "chat_history.append(HumanMessage(content=user_input_1))\n",
        "chat_history.append(AIMessage(content=response_1.content))\n",
        "\n",
        "# Second turn\n",
        "user_input_2 = \"What is a famous landmark there?\"\n",
        "messages_2 = prompt_with_history.format_messages(\n",
        "    chat_history=chat_history,\n",
        "    user_input=user_input_2\n",
        ")\n",
        "response_2 = llm.invoke(messages_2)\n",
        "print(f\"\\nUser: {user_input_2}\")\n",
        "print(f\"Assistant: {response_2.content}\")\n",
        "\n",
        "# Append the second turn to history\n",
        "chat_history.append(HumanMessage(content=user_input_2))\n",
        "chat_history.append(AIMessage(content=response_2.content))\n",
        "\n",
        "# You can continue adding turns similarly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqG0_Rg04gOv",
        "outputId": "ad2aa0aa-ea4b-4b97-8fa0-f036fcda2510"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What is the capital of India?\n",
            "Assistant: The capital of India is New Delhi.\n",
            "\n",
            "User: What is a famous landmark there?\n",
            "Assistant: A famous landmark in New Delhi is the India Gate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0.1)\n",
        "\n",
        "# Define the prompt with a placeholder for history\n",
        "prompt_with_history = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a friendly assistant. Keep your responses brief.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# Initialize an empty chat history\n",
        "chat_history = []\n",
        "\n",
        "# Start an interactive loop\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    messages = prompt_with_history.format_messages(\n",
        "        chat_history=chat_history,\n",
        "        user_input=user_input\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    print(f\"Assistant: {response.content}\")\n",
        "\n",
        "    # Append the current turn to history\n",
        "    chat_history.append(HumanMessage(content=user_input))\n",
        "    chat_history.append(AIMessage(content=response.content))\n",
        "\n",
        "print(\"Chat ended.\")\n",
        "\n",
        "print(chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfrKoAUR6iFp",
        "outputId": "e1bcfd30-2d30-4625-ad65-6f088eb000ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: which city is he from\n",
            "Assistant: Could you please specify who you're referring to?\n",
            "User: quir\n",
            "Assistant: Could you please clarify who \"Quir\" is? I need more information to help you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chains"
      ],
      "metadata": {
        "id": "l8-OWxZD8KvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XockkVS-OdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}