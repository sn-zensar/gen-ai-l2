{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Example 1 Chat Prompt Templates\n"
      ],
      "metadata": {
        "id": "cNqmb5KBsDDj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1w7ytqPsjDO",
        "outputId": "91c3947e-091f-458d-88d6-cd0521ee5a44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.3.74)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.99.9)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-0.3.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
        "\n",
        "# Define the chat prompt template (system + human)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful AI assistant that responds in English.\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Format the prompt with the question\n",
        "formatted_prompt = prompt.format_messages(question=\"What is Python?\")\n",
        "\n",
        "print(type(prompt))\n",
        "print(type(formatted_prompt))\n",
        "\n",
        "# Invoke the model with the formatted prompt\n",
        "response = llm.invoke(formatted_prompt)\n",
        "\n",
        "print(type(response))\n",
        "\n",
        "#print(response)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7hPYTTwsFdb",
        "outputId": "52f4f946-a0cd-476a-e842-96611777fbfb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
            "<class 'list'>\n",
            "<class 'langchain_core.messages.ai.AIMessage'>\n",
            "Python is a high-level, interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It is widely used for web development, data analysis, artificial intelligence, scientific computing, automation, and more. Python's extensive standard library and large community contribute to its popularity and versatility.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2"
      ],
      "metadata": {
        "id": "H6INQ2q5shKv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
        "\n",
        "\n",
        "# Define the chat prompt template (system + human)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful trainer. Be concise and generate simple explainations.\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# RAM , In-Memory Databases\n",
        "chat_history =( [\n",
        "                HumanMessage(content=\"Can you help me learn what is Langchain?\"),\n",
        "                AIMessage(content=\"OfCourse Yes! What in langchain you want to learn?\")\n",
        "            ])\n",
        "\n",
        "\n",
        "messages = prompt.format_messages(chat_history=chat_history, question=\"What is Chat Prompt Templates Component?\")\n",
        "\n",
        "# Invoke the model with the formatted prompt\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "#print(response)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvTJY7uYt1Kl",
        "outputId": "2eb00212-cfaf-43dc-d617-9cc10b3cadab"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Chat Prompt Templates component helps you create and manage reusable prompts for chat models. It allows you to define a template with placeholders, so you can easily customize prompts for different situations. This makes your interactions with chat models more consistent and efficient.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning Resources - YT, Udemy, any other -> diff syntax, and diff libraries/packages\n",
        "# We are using latest packages"
      ],
      "metadata": {
        "id": "KXAU6qnnzhMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Maintain History - manually ask LLM to loop in the history (list) for context\n",
        "\n",
        "# Chains examples , no chains at all for chat prompt templates"
      ],
      "metadata": {
        "id": "H74ogBWzvx2b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0.1)\n",
        "\n",
        "# Define the prompt with a placeholder for history\n",
        "prompt_with_history = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a friendly assistant. Keep your responses brief.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# Create a sample chat history\n",
        "sample_history = [\n",
        "    HumanMessage(content=\"What is the capital of India?\"),\n",
        "    AIMessage(content=\"The capital of India is New Delhi.\"),\n",
        "    HumanMessage(content=\"What is a famous landmark there?\"),\n",
        "    AIMessage(content=\"A famous landmark in New Delhi is the India Gate.\")\n",
        "]\n",
        "\n",
        "\n",
        "# Format the prompt with the history and new user input\n",
        "messages = prompt_with_history.format_messages(\n",
        "    chat_history=sample_history,\n",
        "    user_input=\"Share one interesting fact about the landmark?\"\n",
        ")\n",
        "\n",
        "# Red Fort\n",
        "\n",
        "# Invoke the model\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h3mi3jM4gRC",
        "outputId": "638b79ad-f1b7-4a9d-9de2-591b0350645b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! The Eiffel Tower can grow taller by about 6 inches in hot weather due to metal expansion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0.1)\n",
        "\n",
        "# Define the prompt with a placeholder for history\n",
        "prompt_with_history = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a friendly assistant. Keep your responses brief.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# Initialize an empty chat history\n",
        "chat_history = []\n",
        "\n",
        "# First turn\n",
        "user_input_1 = \"What is the capital of India?\"\n",
        "messages_1 = prompt_with_history.format_messages(\n",
        "    chat_history=chat_history,\n",
        "    user_input=user_input_1\n",
        ")\n",
        "response_1 = llm.invoke(messages_1)\n",
        "print(f\"User: {user_input_1}\")\n",
        "print(f\"Assistant: {response_1.content}\")\n",
        "\n",
        "# Append the first turn to history\n",
        "chat_history.append(HumanMessage(content=user_input_1))\n",
        "chat_history.append(AIMessage(content=response_1.content))\n",
        "\n",
        "# Second turn\n",
        "user_input_2 = \"What is a famous landmark there?\"\n",
        "messages_2 = prompt_with_history.format_messages(\n",
        "    chat_history=chat_history,\n",
        "    user_input=user_input_2\n",
        ")\n",
        "response_2 = llm.invoke(messages_2)\n",
        "print(f\"\\nUser: {user_input_2}\")\n",
        "print(f\"Assistant: {response_2.content}\")\n",
        "\n",
        "# Append the second turn to history\n",
        "chat_history.append(HumanMessage(content=user_input_2))\n",
        "chat_history.append(AIMessage(content=response_2.content))\n",
        "\n",
        "# You can continue adding turns similarly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqG0_Rg04gOv",
        "outputId": "ad2aa0aa-ea4b-4b97-8fa0-f036fcda2510"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What is the capital of India?\n",
            "Assistant: The capital of India is New Delhi.\n",
            "\n",
            "User: What is a famous landmark there?\n",
            "Assistant: A famous landmark in New Delhi is the India Gate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0.1)\n",
        "\n",
        "# Define the prompt with a placeholder for history\n",
        "prompt_with_history = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an expert and highly experienced ai trainer.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "# Initialize an empty chat history\n",
        "chat_history = []\n",
        "\n",
        "# Start an interactive loop\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    messages = prompt_with_history.format_messages(\n",
        "        chat_history=chat_history,\n",
        "        user_input=user_input\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    print(f\"Assistant: {response.content}\")\n",
        "\n",
        "    # Append the current turn to history\n",
        "    chat_history.append(HumanMessage(content=user_input))\n",
        "    chat_history.append(AIMessage(content=response.content))\n",
        "\n",
        "print(\"Chat ended.\")\n",
        "\n",
        "print(chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfrKoAUR6iFp",
        "outputId": "8bb8a87c-db75-418b-9f06-53895d842ea8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What are chains in Langchain\n",
            "Assistant: In LangChain, **chains** are fundamental building blocks that enable the composition of multiple language model operations into a cohesive, automated workflow. They allow you to combine various components—such as prompts, models, memory, and other processing steps—into a single, streamlined process to accomplish complex tasks efficiently.\n",
            "\n",
            "### Key Concepts of Chains in LangChain:\n",
            "\n",
            "1. **Sequential Processing:**  \n",
            "   Chains typically execute a series of steps in order. For example, a chain might generate a prompt, send it to a language model, then process the output further.\n",
            "\n",
            "2. **Modularity:**  \n",
            "   Chains are modular, meaning you can build simple chains for straightforward tasks or compose multiple chains to handle more complex workflows.\n",
            "\n",
            "3. **Types of Chains:**  \n",
            "   - **Simple Chains:** Execute a linear sequence of operations, such as prompt creation followed by model inference.  \n",
            "   - **Composable Chains:** Combine multiple chains or components, enabling more sophisticated workflows like question answering, summarization, or multi-step reasoning.\n",
            "\n",
            "4. **Custom Chains:**  \n",
            "   You can create custom chains by subclassing or composing existing chain classes, tailoring the workflow to specific needs.\n",
            "\n",
            "### Example Use Cases:\n",
            "- Automating customer support responses.\n",
            "- Summarizing lengthy documents.\n",
            "- Extracting structured data from unstructured text.\n",
            "- Multi-turn conversational agents.\n",
            "\n",
            "### Basic Example:\n",
            "```python\n",
            "from langchain.chains import LLMChain\n",
            "from langchain.prompts import PromptTemplate\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# Define a prompt template\n",
            "prompt = PromptTemplate.from_template(\"What is {topic}?\")\n",
            "\n",
            "# Create a chain with the prompt and language model\n",
            "chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
            "\n",
            "# Run the chain\n",
            "response = chain.run(topic=\"artificial intelligence\")\n",
            "print(response)\n",
            "```\n",
            "\n",
            "In this example, the chain encapsulates the process of filling in a prompt template and passing it to the language model, simplifying repeated tasks.\n",
            "\n",
            "---\n",
            "\n",
            "**In summary:**  \n",
            "Chains in LangChain are structured workflows that combine prompts, models, and other components to automate and streamline complex language tasks. They promote reusability, modularity, and clarity in building AI-powered applications.\n",
            "User: quit\n",
            "Chat ended.\n",
            "[HumanMessage(content='What are chains in Langchain', additional_kwargs={}, response_metadata={}), AIMessage(content='In LangChain, **chains** are fundamental building blocks that enable the composition of multiple language model operations into a cohesive, automated workflow. They allow you to combine various components—such as prompts, models, memory, and other processing steps—into a single, streamlined process to accomplish complex tasks efficiently.\\n\\n### Key Concepts of Chains in LangChain:\\n\\n1. **Sequential Processing:**  \\n   Chains typically execute a series of steps in order. For example, a chain might generate a prompt, send it to a language model, then process the output further.\\n\\n2. **Modularity:**  \\n   Chains are modular, meaning you can build simple chains for straightforward tasks or compose multiple chains to handle more complex workflows.\\n\\n3. **Types of Chains:**  \\n   - **Simple Chains:** Execute a linear sequence of operations, such as prompt creation followed by model inference.  \\n   - **Composable Chains:** Combine multiple chains or components, enabling more sophisticated workflows like question answering, summarization, or multi-step reasoning.\\n\\n4. **Custom Chains:**  \\n   You can create custom chains by subclassing or composing existing chain classes, tailoring the workflow to specific needs.\\n\\n### Example Use Cases:\\n- Automating customer support responses.\\n- Summarizing lengthy documents.\\n- Extracting structured data from unstructured text.\\n- Multi-turn conversational agents.\\n\\n### Basic Example:\\n```python\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.llms import OpenAI\\n\\n# Define a prompt template\\nprompt = PromptTemplate.from_template(\"What is {topic}?\")\\n\\n# Create a chain with the prompt and language model\\nchain = LLMChain(llm=OpenAI(), prompt=prompt)\\n\\n# Run the chain\\nresponse = chain.run(topic=\"artificial intelligence\")\\nprint(response)\\n```\\n\\nIn this example, the chain encapsulates the process of filling in a prompt template and passing it to the language model, simplifying repeated tasks.\\n\\n---\\n\\n**In summary:**  \\nChains in LangChain are structured workflows that combine prompts, models, and other components to automate and streamline complex language tasks. They promote reusability, modularity, and clarity in building AI-powered applications.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gen-AI - Generate responses to the user prompts in the form Text\n",
        "\n",
        "# Applications - backed by LLMs\n",
        "\n",
        "# Langchain\n",
        "          # 1 Models\n",
        "          # 2 Prompts (PT, CPT, Messsages, Message Place Holders)"
      ],
      "metadata": {
        "id": "Wwt7XS0DCGeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chains"
      ],
      "metadata": {
        "id": "l8-OWxZD8KvB"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tasks -> Input to LLM, INTERPRET INPUT , SEARCH IN KNOWLEDGE and response"
      ],
      "metadata": {
        "id": "B6aqPLlMD7Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chains -> Types of Chains\n",
        "            # 1 Sequential Chain\n",
        "            # 2 Parallel Chain\n",
        "\n",
        "# Lannguage (Scripting) => | - LCEL - Langchain Expression Language"
      ],
      "metadata": {
        "id": "Vs0V_khAFKau"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input-Prompt -> LLM --> Response --> LLM --> Response\n",
        "\n",
        "# Use - Case -> Stock Suggestions\n",
        "\n",
        "# 1. Input (Events/News) -> LLM --> response - decision  (BUY) ->\n",
        "# 2. Input (Historical Data) -> LLM -> response - decision (BUY) ->"
      ],
      "metadata": {
        "id": "2XockkVS-OdP"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Example of Chains - 1"
      ],
      "metadata": {
        "id": "x2ciFfnNFvuc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Runnables , OutPutParsers()"
      ],
      "metadata": {
        "id": "-adAOG_9HGEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== SETUP (run first in a fresh Colab) =====\n",
        "!pip -q install langchain langchain-openai langchain-community grandalf\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.runnables import (\n",
        "    RunnableLambda, RunnableSequence, RunnableParallel\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "print(\"Setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "632RslbHHk-b",
        "outputId": "6976805d-47c3-43ac-b9dd-fa9530920d11"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSetup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Example 1: Basic LCEL Chain (Prompt -> LLM -> Parser) ===\n",
        "basic_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a expert tutor.\"),\n",
        "    (\"human\", \"Explain {topic} in 2-3 lines with one simple example.\")\n",
        "])\n",
        "\n",
        "# formatted-promt = prompt.\n",
        "\n",
        "basic_chain = basic_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# | StrOutputParser() # response\n",
        "\n",
        "print(\"\\nGraph (ASCII):\")\n",
        "basic_chain.get_graph().print_ascii()\n",
        "\n",
        "print(\"Output:\")\n",
        "print(basic_chain.invoke({\"topic\": \"Chains in Langchain\"}))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnJmx83dGpf9",
        "outputId": "e8c29a89-482b-4bc2-fe03-fcce31b922dc"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Graph (ASCII):\n",
            "     +-------------+       \n",
            "     | PromptInput |       \n",
            "     +-------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "  +--------------------+   \n",
            "  | ChatPromptTemplate |   \n",
            "  +--------------------+   \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n",
            "Output:\n",
            "In Langchain, a chain is a sequence of operations or components that process input data step-by-step, allowing for complex workflows to be built easily. For example, a simple chain could take user input, pass it through a language model to generate a response, and then send that response to a text summarizer for a concise output. \n",
            "\n",
            "**Example:** \n",
            "1. Input: \"Tell me about Langchain.\"\n",
            "2. Chain: \n",
            "   - Step 1: Language Model generates a detailed response.\n",
            "   - Step 2: Summarizer condenses the response into a brief overview. \n",
            "3. Output: \"Langchain is a framework for building applications with language models.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Example 1: Basic LCEL Chain (Prompt -> LLM -> Parser) ===\n",
        "basic_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a cricket expert.\"),\n",
        "    (\"human\", \"Explain {topic} in 2 lines with one simple example.\")\n",
        "])\n",
        "\n",
        "# formatted-promt = prompt.\n",
        "\n",
        "basic_chain1  = basic_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# | StrOutputParser() # response\n",
        "\n",
        "print(\"\\nGraph (ASCII):\")\n",
        "basic_chain.get_graph().print_ascii()\n",
        "\n",
        "print(\"Output:\")\n",
        "print(basic_chain1.invoke({\"topic\": \"What is Leg-Spin\"}))\n",
        "\n",
        "\n",
        "# === Example 2: Basic LCEL Chain (Prompt -> LLM -> Parser) ===\n",
        "basic_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a football expert.\"),\n",
        "    (\"human\", \"Explain {topic} in 2 lines with one simple example.\")\n",
        "])\n",
        "\n",
        "# formatted-promt = prompt.\n",
        "\n",
        "basic_chain2 = basic_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# | StrOutputParser() # response\n",
        "\n",
        "print(\"\\nGraph (ASCII):\")\n",
        "basic_chain.get_graph().print_ascii()\n",
        "\n",
        "print(\"Output:\")\n",
        "print(basic_chain2.invoke({\"topic\": \"what is red card and yellow card\"}))"
      ],
      "metadata": {
        "id": "Vh-CIGEFMxJk",
        "outputId": "5884bf01-ca21-4713-b55e-05c7a932ab33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Graph (ASCII):\n",
            "     +-------------+       \n",
            "     | PromptInput |       \n",
            "     +-------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "  +--------------------+   \n",
            "  | ChatPromptTemplate |   \n",
            "  +--------------------+   \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n",
            "Output:\n",
            "Leg-spin is a type of bowling in cricket where the bowler spins the ball from the leg side to the off side, typically using wrist rotation. An example is Shane Warne's famous \"Ball of the Century,\" which spun sharply to dismiss Mike Gatting.\n",
            "\n",
            "Graph (ASCII):\n",
            "     +-------------+       \n",
            "     | PromptInput |       \n",
            "     +-------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "  +--------------------+   \n",
            "  | ChatPromptTemplate |   \n",
            "  +--------------------+   \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n",
            "Output:\n",
            "A yellow card is a warning given to a player for unsporting behavior, while a red card results in a player being sent off the field for serious fouls or misconduct. For example, a player might receive a yellow card for a reckless tackle and a red card for violent conduct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 - Task 1 (cricket info) and Task 2 (Football Info) => Merge the info about Cricket & Football"
      ],
      "metadata": {
        "id": "ZmpvwX1RNqba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# End of the Exmaples"
      ],
      "metadata": {
        "id": "XuKfY93eNqZO"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below Examples are for information only, will be discussed later."
      ],
      "metadata": {
        "id": "2w92VvxWOL8m"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Example 3: Simple Sequential Chain ===\n",
        "\n",
        "# This example demonstrates a basic sequential chain where the output of one\n",
        "# component becomes the input for the next.\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "# First part of the chain: generate a topic sentence\n",
        "topic_prompt = PromptTemplate.from_template(\"Generate a concise topic sentence about {subject}.\")\n",
        "\n",
        "topic_chain = topic_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Second part of the chain: expand on the topic sentence\n",
        "expansion_prompt = PromptTemplate.from_template(\"Expand on this topic sentence: {topic_sentence}\")\n",
        "expansion_chain = expansion_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Combine the chains sequentially using RunnableSequence\n",
        "# The output of topic_chain (topic_sentence) becomes the input for expansion_chain\n",
        "sequential_chain = RunnableSequence(\n",
        "    topic_chain,\n",
        "    expansion_chain\n",
        ")\n",
        "\n",
        "print(\"\\nSequential Chain Output:\")\n",
        "print(sequential_chain.invoke({\"subject\": \"artificial intelligence\"}))\n",
        "\n",
        "print(\"\\nSequential Chain Graph (ASCII):\")\n",
        "sequential_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsKELzRDK09v",
        "outputId": "8e811d5c-7438-416c-fcbc-2fa0483df301"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sequential Chain Output:\n",
            "Artificial intelligence is revolutionizing industries by enhancing efficiency, enabling data-driven decision-making, and transforming the way we interact with technology. In manufacturing, for instance, AI-powered automation systems streamline production processes, reducing human error and increasing output while minimizing costs. These intelligent systems can predict equipment failures before they occur, allowing for proactive maintenance that further boosts operational efficiency. \n",
            "\n",
            "In the realm of healthcare, AI algorithms analyze vast amounts of patient data to assist doctors in diagnosing diseases more accurately and swiftly, leading to improved patient outcomes. Machine learning models can identify patterns in medical records that might go unnoticed by human practitioners, facilitating personalized treatment plans tailored to individual patient needs. \n",
            "\n",
            "Moreover, AI is reshaping the financial sector by providing advanced analytics that empower businesses to make informed decisions based on real-time market data. Financial institutions leverage AI to detect fraudulent activities, assess credit risks, and optimize investment strategies, thereby enhancing overall security and profitability.\n",
            "\n",
            "The way we interact with technology is also undergoing a significant transformation, as AI-driven virtual assistants and chatbots become increasingly integrated into our daily lives. These tools not only improve customer service by providing instant responses to inquiries but also learn from user interactions to offer personalized experiences. \n",
            "\n",
            "As AI continues to evolve, its impact on industries will only deepen, driving innovation and creating new opportunities while also posing challenges that require careful consideration of ethical implications and workforce adaptation. Overall, the integration of artificial intelligence into various sectors is not just a trend; it represents a fundamental shift in how we operate, make decisions, and connect with the world around us.\n",
            "\n",
            "Sequential Chain Graph (ASCII):\n",
            "     +-------------+       \n",
            "     | PromptInput |       \n",
            "     +-------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Example 4: Parallel Chains ===\n",
        "\n",
        "# This example demonstrates how to run multiple chains in parallel using RunnableParallel.\n",
        "# The input is passed to both chains simultaneously, and the output is a dictionary\n",
        "# containing the results from each chain.\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "# First chain: generate a concise summary\n",
        "summary_prompt = PromptTemplate.from_template(\"Provide a concise summary of {topic}.\")\n",
        "summary_chain = summary_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Second chain: generate a list of key points\n",
        "points_prompt = PromptTemplate.from_template(\"List 3 key points about {topic}.\")\n",
        "points_chain = points_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Combine the chains in parallel using RunnableParallel\n",
        "parallel_chain = RunnableParallel(\n",
        "    summary=summary_chain,\n",
        "    key_points=points_chain\n",
        ")\n",
        "\n",
        "print(\"\\nParallel Chain Output:\")\n",
        "print(parallel_chain.invoke({\"topic\": \"LangChain LCEL\"}))\n",
        "\n",
        "print(\"\\nParallel Chain Graph (ASCII):\")\n",
        "parallel_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztyLjznhK07q",
        "outputId": "0664e025-2840-45ea-ee4d-62a68ea8ee93"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parallel Chain Output:\n",
            "{'summary': 'LangChain LCEL (LangChain Embedding and LLM) is a framework designed to facilitate the development of applications that leverage large language models (LLMs) and embeddings. It provides tools and components for integrating LLMs with various data sources, enabling developers to create applications that can understand and generate human-like text. LCEL focuses on simplifying the process of building conversational agents, chatbots, and other AI-driven applications by offering modular components for data processing, model interaction, and response generation. Its architecture supports flexibility and scalability, making it suitable for a wide range of use cases in natural language processing.', 'key_points': 'LangChain LCEL (LangChain Event Logging) is a framework designed to facilitate the logging and tracking of events in applications built with LangChain. Here are three key points about it:\\n\\n1. **Event Tracking**: LCEL provides a structured way to log events that occur within a LangChain application, allowing developers to monitor and analyze the behavior of their applications. This can include tracking user interactions, API calls, and other significant events that can help in debugging and improving the application.\\n\\n2. **Integration with LangChain Components**: LCEL is designed to work seamlessly with various components of the LangChain ecosystem, enabling developers to easily integrate event logging into their existing workflows. This integration helps in maintaining a consistent approach to logging across different parts of the application.\\n\\n3. **Enhanced Observability**: By utilizing LCEL, developers gain enhanced observability into their applications, which can lead to better insights and decision-making. The logged events can be used for performance monitoring, error tracking, and understanding user behavior, ultimately contributing to a more robust and user-friendly application.\\n\\nThese points highlight the importance of LCEL in improving the development and maintenance of applications built with LangChain.'}\n",
            "\n",
            "Parallel Chain Graph (ASCII):\n",
            "        +-----------------------------------+        \n",
            "        | Parallel<summary,key_points>Input |        \n",
            "        +-----------------------------------+        \n",
            "                 **               **                 \n",
            "              ***                   ***              \n",
            "            **                         **            \n",
            "+----------------+                +----------------+ \n",
            "| PromptTemplate |                | PromptTemplate | \n",
            "+----------------+                +----------------+ \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "  +------------+                    +------------+   \n",
            "  | ChatOpenAI |                    | ChatOpenAI |   \n",
            "  +------------+                    +------------+   \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "+-----------------+              +-----------------+ \n",
            "| StrOutputParser |              | StrOutputParser | \n",
            "+-----------------+              +-----------------+ \n",
            "                 **               **                 \n",
            "                   ***         ***                   \n",
            "                      **     **                      \n",
            "       +------------------------------------+        \n",
            "       | Parallel<summary,key_points>Output |        \n",
            "       +------------------------------------+        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzBR4phjK0zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XRaxkaOkK0xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUCIrR4oK0u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2"
      ],
      "metadata": {
        "id": "g1b0kZRWHhLb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Example 3: Parallel + Nested Chains (CONFIRMED) ===\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "\n",
        "# Branch 1: explanation\n",
        "explain_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Be clear.\"),\n",
        "    (\"human\", \"Explain {topic} in one line.\")\n",
        "])\n",
        "explain_chain = explain_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Branch 2: bullets\n",
        "bullets_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Be concise.\"),\n",
        "    (\"human\", \"Give exactly 3 bullet points about {topic}.\")\n",
        "])\n",
        "bullets_chain = bullets_prompt | llm | StrOutputParser()\n",
        "\n",
        "# PARALLEL: run both branches and return a dict\n",
        "parallel_chain = RunnableParallel(\n",
        "    explanation=explain_chain,\n",
        "    bullets=bullets_chain\n",
        ")\n",
        "\n",
        "print(\"Parallel result:\")\n",
        "print(parallel_chain.invoke({\"topic\": \"LangChain Chains\"}))\n",
        "\n",
        "print(\"\\nParallel Graph (ASCII):\")\n",
        "parallel_chain.get_graph().print_ascii()\n",
        "\n",
        "# NESTED: add a small pre-step, then run the parallel branches\n",
        "def add_note(x: dict) -> dict:\n",
        "    return {**x, \"topic\": x.get(\"topic\", \"the topic\") + \" (beginner focus)\"}\n",
        "\n",
        "nested_chain = RunnableLambda(add_note) | parallel_chain\n",
        "\n",
        "print(\"\\nNested result:\")\n",
        "print(nested_chain.invoke({\"topic\": \"RunnableParallel\"}))\n",
        "\n",
        "print(\"\\nNested Graph (ASCII):\")\n",
        "nested_chain.get_graph().print_ascii()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI7l-JLiJCMe",
        "outputId": "53a0e04a-6188-42ea-8c92-be733caee8a9"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parallel result:\n",
            "{'explanation': 'LangChain Chains are sequences of interconnected components that facilitate the construction of complex applications by linking together various tasks and processes in natural language processing workflows.', 'bullets': '- **Modular Components**: LangChain Chains consist of interconnected components that allow for the creation of complex workflows by combining various tasks and models.\\n- **Flexibility**: Users can customize chains to suit specific applications, enabling tailored interactions with language models and data sources.\\n- **Integration**: Chains can easily integrate with external APIs, databases, and other tools, enhancing the functionality and versatility of language model applications.'}\n",
            "\n",
            "Parallel Graph (ASCII):\n",
            "          +------------------------------------+           \n",
            "          | Parallel<explanation,bullets>Input |           \n",
            "          +------------------------------------+           \n",
            "                   ***               ***                   \n",
            "                ***                     ***                \n",
            "              **                           **              \n",
            "+--------------------+              +--------------------+ \n",
            "| ChatPromptTemplate |              | ChatPromptTemplate | \n",
            "+--------------------+              +--------------------+ \n",
            "           *                                   *           \n",
            "           *                                   *           \n",
            "           *                                   *           \n",
            "    +------------+                      +------------+     \n",
            "    | ChatOpenAI |                      | ChatOpenAI |     \n",
            "    +------------+                      +------------+     \n",
            "           *                                   *           \n",
            "           *                                   *           \n",
            "           *                                   *           \n",
            "  +-----------------+                 +-----------------+  \n",
            "  | StrOutputParser |                 | StrOutputParser |  \n",
            "  +-----------------+                 +-----------------+  \n",
            "                   ***               ***                   \n",
            "                      ***         ***                      \n",
            "                         **     **                         \n",
            "          +-------------------------------------+          \n",
            "          | Parallel<explanation,bullets>Output |          \n",
            "          +-------------------------------------+          \n",
            "\n",
            "Nested result:\n",
            "{'explanation': 'RunnableParallel is a programming concept that allows multiple tasks to run simultaneously, improving efficiency by utilizing available resources effectively.', 'bullets': '- **Definition**: RunnableParallel is a programming construct that allows multiple tasks to run simultaneously, improving efficiency and performance in applications by leveraging parallel processing.\\n\\n- **Usage**: It is commonly used in scenarios where tasks are independent and can be executed concurrently, such as data processing, simulations, or handling multiple user requests.\\n\\n- **Implementation**: In languages like Java, RunnableParallel can be implemented using threads or frameworks like ExecutorService, making it easier to manage and control parallel execution of tasks.'}\n",
            "\n",
            "Nested Graph (ASCII):\n",
            "                   +----------------+                    \n",
            "                   | add_note_input |                    \n",
            "                   +----------------+                    \n",
            "                            *                            \n",
            "                            *                            \n",
            "                            *                            \n",
            "                      +----------+                       \n",
            "                      | add_note |                       \n",
            "                      +----------+                       \n",
            "                            *                            \n",
            "                            *                            \n",
            "                            *                            \n",
            "         +------------------------------------+          \n",
            "         | Parallel<explanation,bullets>Input |          \n",
            "         +------------------------------------+          \n",
            "                   **               **                   \n",
            "                ***                   ***                \n",
            "              **                         **              \n",
            "+--------------------+            +--------------------+ \n",
            "| ChatPromptTemplate |            | ChatPromptTemplate | \n",
            "+--------------------+            +--------------------+ \n",
            "           *                                 *           \n",
            "           *                                 *           \n",
            "           *                                 *           \n",
            "    +------------+                    +------------+     \n",
            "    | ChatOpenAI |                    | ChatOpenAI |     \n",
            "    +------------+                    +------------+     \n",
            "           *                                 *           \n",
            "           *                                 *           \n",
            "           *                                 *           \n",
            "  +-----------------+               +-----------------+  \n",
            "  | StrOutputParser |               | StrOutputParser |  \n",
            "  +-----------------+               +-----------------+  \n",
            "                   **               **                   \n",
            "                     ***         ***                     \n",
            "                        **     **                        \n",
            "        +-------------------------------------+          \n",
            "        | Parallel<explanation,bullets>Output |          \n",
            "        +-------------------------------------+          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Article on Gen-AI\n",
        "\n",
        "# Task 1 : Notes from the article\n",
        "\n",
        "# Task 2 : Generate some FAQs/Questions from the article\n",
        "\n",
        "# Task 3 : Combine / Merge -> Notes and FAQs"
      ],
      "metadata": {
        "id": "iFkdE5LFJFkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3102825"
      },
      "source": [
        "# === Example 2: Chain with History (Prompt with Placeholder -> LLM -> Parser) ===\n",
        "\n",
        "# This example demonstrates how to create a chain that incorporates chat history\n",
        "# using MessagesPlaceholder, similar to previous examples but within the LCEL framework.\n",
        "\n",
        "history_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "history_chain = history_prompt | llm | StrOutputParser()\n",
        "\n",
        "# To use this chain, you would pass both chat_history and user_input\n",
        "sample_history_for_chain = [\n",
        "    HumanMessage(content=\"Hi there!\"),\n",
        "    AIMessage(content=\"Hello! How can I help you today?\")\n",
        "]\n",
        "\n",
        "print(\"\\nHistory Chain Output:\")\n",
        "print(history_chain.invoke({\n",
        "    \"chat_history\": sample_history_for_chain,\n",
        "    \"user_input\": \"What is the weather like?\"\n",
        "}))\n",
        "\n",
        "print(\"\\nHistory Chain Graph (ASCII):\")\n",
        "history_chain.get_graph().print_ascii()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}