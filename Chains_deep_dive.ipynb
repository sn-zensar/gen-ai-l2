{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===== SETUP (run first in a fresh Colab) =====\n",
        "!pip -q install langchain langchain-openai langchain-community grandalf\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.runnables import (\n",
        "    RunnableLambda, RunnableSequence, RunnableParallel\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "print(\"Setup complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boUxZ-bjxK43",
        "outputId": "8d9e9122-ab9c-4353-c178-4276d1d27fb2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Essential Imports"
      ],
      "metadata": {
        "id": "vtme8PZLx65R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from langchain_core.prompts import ChatPromptTemplate\n",
        "#from langchain_core.output_parsers import StrOutputParser\n",
        "#from langchain_openai import ChatOpenAI\n",
        "#from google.colab import userdata\n",
        "#import os"
      ],
      "metadata": {
        "id": "khiCF30ryhkr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1"
      ],
      "metadata": {
        "id": "9CcuhAO5yqiy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Prompt\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template = \"Generate 5 interesting facts about {topic}\",\n",
        "    input_variables = [\"topic\"]\n",
        "    )\n",
        "\n",
        "\n",
        "# Step 2 - Choose and Call a LLM Model\n",
        "\n",
        "llm = ChatOpenAI(model= 'gpt-4.1-nano', temperature = 0.2)\n",
        "\n",
        "# Step 3 - Output (Response) Parser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "# Step 4 - Chains\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "result = chain.invoke({\"topic\": \"cricket\"})\n",
        "\n",
        "#print(type(prompt))\n",
        "#print(type(parser))\n",
        "#print(type(chain))\n",
        "#print(type(result))\n",
        "\n",
        "# Visualize the Chain (Graph)\n",
        "\n",
        "chain.get_graph().print_ascii()\n",
        "\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkuRibjs13SH",
        "outputId": "92923096-5f62-4924-9eae-bfcb6dc123b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     +-------------+       \n",
            "     | PromptInput |       \n",
            "     +-------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n",
            "Did you know that the longest cricket match in history lasted for 12 days? The \"Timeless Test\" between South Africa and England in 1939 was played without a time limit, and it ended in a draw after 12 days of play!\n",
            "     +-------------+       \n",
            "     | PromptInput |       \n",
            "     +-------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n",
            "Did you know that the fastest goal in football history was scored just 2.4 seconds after kickoff? This incredible feat was achieved by Ricardo Oliveira during a match in Brazil in 2009!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Prompt\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template = \"Generate 1 interesting facts about {topic}\",\n",
        "    input_variables = [\"topic\"]\n",
        "    )\n",
        "\n",
        "\n",
        "# Step 2 - Choose and Call a LLM Model\n",
        "\n",
        "llm = ChatOpenAI(model= 'gpt-4.1-nano', temperature = 0.2)\n",
        "\n",
        "# Step 3 - Output (Response) Parser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "# Step 4 - Chains\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "result = chain.invoke({\"topic\": \"cricket\"})\n",
        "\n",
        "#print(type(prompt))\n",
        "#print(type(parser))\n",
        "#print(type(chain))\n",
        "#print(type(result))\n",
        "\n",
        "# Visualize the Chain (Graph)\n",
        "\n",
        "chain.get_graph().print_ascii()\n",
        "\n",
        "print(result)\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template = \"Generate 1 interesting facts about {topic}\",\n",
        "    input_variables = [\"topic\"]\n",
        "    )\n",
        "\n",
        "\n",
        "# Step 2 - Choose and Call a LLM Model\n",
        "\n",
        "llm = ChatOpenAI(model= 'gpt-4.1-nano', temperature = 0.2)\n",
        "\n",
        "# Step 3 - Output (Response) Parser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "# Step 4 - Chains\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "result = chain.invoke({\"topic\": \"Football\"})\n",
        "\n",
        "#print(type(prompt))\n",
        "#print(type(parser))\n",
        "#print(type(chain))\n",
        "#print(type(result))\n",
        "\n",
        "# Visualize the Chain (Graph)\n",
        "\n",
        "chain.get_graph().print_ascii()\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Nfw30MGn2kvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Types of Chains\n",
        "    # 1 Sequential\n",
        "    # 2 Parallel\n",
        "    # 3 Conditional"
      ],
      "metadata": {
        "id": "mykPgE-I5lUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Sequential Chain"
      ],
      "metadata": {
        "id": "toS7OobJ5iVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Prompt\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template = \"Generate 10 interesting facts about {topic}\",\n",
        "    input_variables = [\"topic\"]\n",
        "    )\n",
        "\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template = \"Summarize the interesting facts generated in 2 bullet points for {text}\",\n",
        "    input_variables = [\"text\"]\n",
        "    )\n",
        "\n",
        "# Step 2 - Choose and Call a LLM Model\n",
        "\n",
        "llm1 = ChatOpenAI(model= 'gpt-4.1-nano', temperature = 0.1)\n",
        "\n",
        "llm2 = ChatOpenAI(model= 'gpt-4o-mini', temperature = 0.1)\n",
        "\n",
        "# Step 3 - Output (Response) Parser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "# Step 4 - Chains\n",
        "\n",
        "chain = prompt1 | llm1 | parser | prompt2 | llm2 | parser\n",
        "\n",
        "result = chain.invoke({\"topic\": \"Gen-AI\"})\n",
        "\n",
        "#print(type(prompt))\n",
        "#print(type(parser))\n",
        "#print(type(chain))\n",
        "#print(type(result))\n",
        "\n",
        "# Visualize the Chain (Graph)\n",
        "\n",
        "#chain.get_graph().print_ascii()\n",
        "\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev3RHUx55iQm",
        "outputId": "bb1815e5-20eb-4796-9ba8-4ea0e88844bf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- **Creative and Versatile Applications**: Generative AI can produce original content across various mediums, including images, music, and text, while also being utilized for data augmentation and personalized experiences in multiple industries.\n",
            "\n",
            "- **Ethical and Rapidly Evolving Landscape**: The technology presents ethical challenges related to misinformation and intellectual property, and it is rapidly advancing, with new models and applications emerging frequently, impacting diverse fields from entertainment to healthcare.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 -> Parallel Chain"
      ],
      "metadata": {
        "id": "T39S6Lr77GoF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Prompt\n",
        "\n",
        "prompt_notes = PromptTemplate(\n",
        "    template = \"Generate some notes for studying from the given {text}\",\n",
        "    input_variables = [\"text\"]\n",
        "    )\n",
        "\n",
        "\n",
        "prompt_faqs = PromptTemplate(\n",
        "    template = \"Generate 5 questions from the given {text}\",\n",
        "    input_variables = [\"text\"]\n",
        "    )\n",
        "\n",
        "# Step 2 - Choose and Call a LLM Model\n",
        "\n",
        "llm_notes = ChatOpenAI(model= 'gpt-5', temperature = 0.1)\n",
        "llm_faqs = ChatOpenAI(model= 'gpt-4o-mini', temperature = 0.1)\n",
        "\n",
        "\n",
        "# Step 3 - Output (Response) Parser\n",
        "\n",
        "parser1 = StrOutputParser()\n",
        "\n",
        "parser2 = StrOutputParser()\n",
        "\n",
        "# Step 4 - Chains\n",
        "chain_notes = prompt_notes| llm_notes| parser1\n",
        "\n",
        "chain_faqs = prompt_faqs | llm_faqs | parser2\n",
        "\n",
        "\n",
        "# Step 5\n",
        "\n",
        "parallel = RunnableParallel(notes=llm_notes, faqs=llm_faqs)\n",
        "\n",
        "text = \"\"\"\n",
        "Here's a more detailed breakdown:\n",
        "What it is:\n",
        "GenAI models are trained on massive datasets of existing content (like text, images, etc.) and learn to generate new content that resembles the training data.\n",
        "They leverage techniques like large language models, which are capable of understanding and generating human-like text.\n",
        "GenAI can be used to create a wide range of content, including text, images, videos, audio, code, and 3D designs.\n",
        "How it works:\n",
        "Users provide prompts (natural language requests) to the GenAI model.\n",
        "The model analyzes the prompt and generates a corresponding response based on its training data.\n",
        "This process involves prompt engineering, where users refine their prompts to get the desired output.\n",
        "Key applications and use cases:\n",
        "Content creation:\n",
        "GenAI can be used to generate articles, blog posts, marketing copy, social media content, and more.\n",
        "Image and video generation:\n",
        "It can be used to create images, videos, and even short-form video content.\n",
        "Software development:\n",
        "GenAI can assist with code generation, debugging, and even automating parts of the software development process.\n",
        "Customer service:\n",
        "GenAI-powered chatbots can provide 24/7 support, answer queries, and even handle complex customer interactions.\n",
        "Data analysis and insights:\n",
        "GenAI can help analyze large datasets, identify patterns, and generate insights that would be difficult for humans to discover.\n",
        "Translation:\n",
        "GenAI can improve language translation services by providing more accurate and contextually relevant translations.\n",
        "\"\"\"\n",
        "\n",
        "# Step 4 - Chains (Parallel)\n",
        "parallel_chain = RunnableParallel(\n",
        "    notes=chain_notes,\n",
        "    faqs=chain_faqs\n",
        ")\n",
        "\n",
        "# Step 5 - Invoke the parallel chain\n",
        "result_parallel = parallel_chain.invoke({\"text\": text})\n",
        "\n",
        "parallel_chain.get_graph().print_ascii()\n",
        "\n",
        "# Print the results\n",
        "print(\"Notes:\")\n",
        "print(result_parallel[\"notes\"])\n",
        "print(\"\\nFAQs:\")\n",
        "print(result_parallel[\"faqs\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a9DZbbp9tnj",
        "outputId": "92ee3c85-3da1-41a9-f07d-138b6235b682"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            +---------------------------+            \n",
            "            | Parallel<notes,faqs>Input |            \n",
            "            +---------------------------+            \n",
            "                 **               **                 \n",
            "              ***                   ***              \n",
            "            **                         **            \n",
            "+----------------+                +----------------+ \n",
            "| PromptTemplate |                | PromptTemplate | \n",
            "+----------------+                +----------------+ \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "  +------------+                    +------------+   \n",
            "  | ChatOpenAI |                    | ChatOpenAI |   \n",
            "  +------------+                    +------------+   \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "+-----------------+              +-----------------+ \n",
            "| StrOutputParser |              | StrOutputParser | \n",
            "+-----------------+              +-----------------+ \n",
            "                 **               **                 \n",
            "                   ***         ***                   \n",
            "                      **     **                      \n",
            "           +----------------------------+            \n",
            "           | Parallel<notes,faqs>Output |            \n",
            "           +----------------------------+            \n",
            "Notes:\n",
            "Study notes: Generative AI (GenAI)\n",
            "\n",
            "What it is\n",
            "- AI models trained on massive datasets (text, images, audio, etc.) that learn patterns and generate new, similar content.\n",
            "- Often built on large language models (LLMs) that can understand and produce human-like text.\n",
            "- Can create diverse outputs: text, images, videos, audio, code, and 3D designs.\n",
            "\n",
            "How it works\n",
            "- Users provide prompts (natural-language instructions or questions).\n",
            "- The model analyzes the prompt and produces a response based on patterns learned during training.\n",
            "- Prompt engineering: refining and structuring prompts to steer the model toward desired outputs.\n",
            "\n",
            "Key applications and use cases\n",
            "- Content creation: articles, blog posts, marketing copy, social media content.\n",
            "- Image and video generation: still images, videos, short-form video content.\n",
            "- Software development: code generation, debugging assistance, automation of parts of the development process.\n",
            "- Customer service: chatbots for 24/7 support, answering queries, handling complex interactions.\n",
            "- Data analysis and insights: processing large datasets, pattern detection, generating insights.\n",
            "- Translation: improving accuracy and contextual relevance of language translations.\n",
            "\n",
            "Key takeaways\n",
            "- GenAI transforms prompts into coherent outputs across many media types.\n",
            "- Effectiveness depends on training data and the quality of prompts.\n",
            "- Broad utility spans creative, technical, analytical, and customer-facing domains.\n",
            "\n",
            "FAQs:\n",
            "Here are five questions based on the provided information about GenAI models:\n",
            "\n",
            "1. What types of content can GenAI models generate, and how do they learn to produce this content?\n",
            "   \n",
            "2. How does the process of prompt engineering enhance the interaction between users and GenAI models?\n",
            "\n",
            "3. In what ways can GenAI be utilized in software development, and what specific tasks can it assist with?\n",
            "\n",
            "4. How do GenAI-powered chatbots improve customer service, and what advantages do they offer over traditional support methods?\n",
            "\n",
            "5. What role does GenAI play in data analysis, and how can it help in identifying patterns and generating insights from large datasets?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Prompt\n",
        "\n",
        "prompt_notes = PromptTemplate(\n",
        "    template = \"Generate some notes for studying from the given {text}\",\n",
        "    input_variables = [\"text\"]\n",
        "    )\n",
        "\n",
        "\n",
        "prompt_faqs = PromptTemplate(\n",
        "    template = \"Generate 5 questions from the given {text}\",\n",
        "    input_variables = [\"text\"]\n",
        "    )\n",
        "\n",
        "# Step 2 - Choose and Call a LLM Model\n",
        "\n",
        "llm_notes = ChatOpenAI(model= 'gpt-5', temperature = 0.1)\n",
        "llm_faqs = ChatOpenAI(model= 'gpt-4o-mini', temperature = 0.1)\n",
        "\n",
        "\n",
        "# Step 3 - Output (Response) Parser\n",
        "\n",
        "parser1 = StrOutputParser()\n",
        "\n",
        "parser2 = StrOutputParser()\n",
        "\n",
        "# Step 4 - Chains\n",
        "chain_notes = prompt_notes| llm_notes| parser1\n",
        "\n",
        "chain_faqs = prompt_faqs | llm_faqs | parser2\n",
        "\n",
        "\n",
        "# Step 5\n",
        "\n",
        "parallel = RunnableParallel(notes=llm_notes, faqs=llm_faqs)\n",
        "\n",
        "text = \"\"\"\n",
        "Here's a more detailed breakdown:\n",
        "What it is:\n",
        "GenAI models are trained on massive datasets of existing content (like text, images, etc.) and learn to generate new content that resembles the training data.\n",
        "They leverage techniques like large language models, which are capable of understanding and generating human-like text.\n",
        "GenAI can be used to create a wide range of content, including text, images, videos, audio, code, and 3D designs.\n",
        "How it works:\n",
        "Users provide prompts (natural language requests) to the GenAI model.\n",
        "The model analyzes the prompt and generates a corresponding response based on its training data.\n",
        "This process involves prompt engineering, where users refine their prompts to get the desired output.\n",
        "Key applications and use cases:\n",
        "Content creation:\n",
        "GenAI can be used to generate articles, blog posts, marketing copy, social media content, and more.\n",
        "Image and video generation:\n",
        "It can be used to create images, videos, and even short-form video content.\n",
        "Software development:\n",
        "GenAI can assist with code generation, debugging, and even automating parts of the software development process.\n",
        "Customer service:\n",
        "GenAI-powered chatbots can provide 24/7 support, answer queries, and even handle complex customer interactions.\n",
        "Data analysis and insights:\n",
        "GenAI can help analyze large datasets, identify patterns, and generate insights that would be difficult for humans to discover.\n",
        "Translation:\n",
        "GenAI can improve language translation services by providing more accurate and contextually relevant translations.\n",
        "\"\"\"\n",
        "\n",
        "# Step 4 - Chains (Parallel)\n",
        "parallel_chain = RunnableParallel(\n",
        "    notes=chain_notes,\n",
        "    faqs=chain_faqs\n",
        ")\n",
        "\n",
        "# Step 5 - Invoke the parallel chain\n",
        "result_parallel = parallel_chain.invoke({\"text\": text})\n",
        "\n",
        "# parallel_chain.get_graph().print_ascii()\n",
        "\n",
        "# Print the results\n",
        "#print(\"Notes:\")\n",
        "#print(result_parallel[\"notes\"])\n",
        "#print(\"\\nFAQs:\")\n",
        "#print(result_parallel[\"faqs\"])\n",
        "\n",
        "# Step 1 - Prompt for answering FAQs\n",
        "prompt_answers = PromptTemplate(\n",
        "    template=\"Based on these notes:\\n{notes}\\n\\nAnswer these questions:\\n{faqs}\\n\\nProvide concise answers.\",\n",
        "    input_variables=[\"notes\", \"faqs\"]\n",
        ")\n",
        "\n",
        "# Step 2 - Choose and Call a LLM Model for answering FAQs\n",
        "llm_answers = ChatOpenAI(model='gpt-4o-mini', temperature=0.1)\n",
        "\n",
        "# Step 3 - Output (Response) Parser\n",
        "parser3 = StrOutputParser()\n",
        "\n",
        "# Step 4 - Create the sequential chain to answer FAQs\n",
        "answer_chain = prompt_answers | llm_answers | parser3\n",
        "\n",
        "# Step 5 - Invoke the sequential chain with the output from the parallel chain\n",
        "result_answers = answer_chain.invoke(result_parallel)\n",
        "\n",
        "# Print the answers\n",
        "#print(\"\\nAnswers to FAQs:\")\n",
        "#print(result_answers)\n",
        "parallel_chain.get_graph().print_ascii()\n",
        "answer_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1G4lSk4As-U",
        "outputId": "32926ee1-7cf9-4c56-a5c0-535898cb0eda"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            +---------------------------+            \n",
            "            | Parallel<notes,faqs>Input |            \n",
            "            +---------------------------+            \n",
            "                 **               **                 \n",
            "              ***                   ***              \n",
            "            **                         **            \n",
            "+----------------+                +----------------+ \n",
            "| PromptTemplate |                | PromptTemplate | \n",
            "+----------------+                +----------------+ \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "  +------------+                    +------------+   \n",
            "  | ChatOpenAI |                    | ChatOpenAI |   \n",
            "  +------------+                    +------------+   \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "+-----------------+              +-----------------+ \n",
            "| StrOutputParser |              | StrOutputParser | \n",
            "+-----------------+              +-----------------+ \n",
            "                 **               **                 \n",
            "                   ***         ***                   \n",
            "                      **     **                      \n",
            "           +----------------------------+            \n",
            "           | Parallel<notes,faqs>Output |            \n",
            "           +----------------------------+            \n",
            "     +-------------+       \n",
            "     | PromptInput |       \n",
            "     +-------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "    +----------------+     \n",
            "    | PromptTemplate |     \n",
            "    +----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "   +-----------------+     \n",
            "   | StrOutputParser |     \n",
            "   +-----------------+     \n",
            "            *              \n",
            "            *              \n",
            "            *              \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Prompt\n",
        "\n",
        "prompt_notes = PromptTemplate(\n",
        "    template = \"Generate some notes for studying from the given {text}\",\n",
        "    input_variables = [\"text\"]\n",
        "    )\n",
        "\n",
        "\n",
        "prompt_faqs = PromptTemplate(\n",
        "    template = \"Generate 5 questions from the given {text}\",\n",
        "    input_variables = [\"text\"]\n",
        "    )\n",
        "\n",
        "# Step 2 - Choose and Call a LLM Model\n",
        "\n",
        "llm_notes = ChatOpenAI(model= 'gpt-5', temperature = 0.1)\n",
        "llm_faqs = ChatOpenAI(model= 'gpt-4o-mini', temperature = 0.1)\n",
        "\n",
        "\n",
        "# Step 3 - Output (Response) Parser\n",
        "\n",
        "parser1 = StrOutputParser()\n",
        "\n",
        "parser2 = StrOutputParser()\n",
        "\n",
        "# Step 4 - Chains\n",
        "chain_notes = prompt_notes| llm_notes| parser1\n",
        "\n",
        "chain_faqs = prompt_faqs | llm_faqs | parser2\n",
        "\n",
        "\n",
        "# Step 5\n",
        "\n",
        "parallel = RunnableParallel(notes=llm_notes, faqs=llm_faqs)\n",
        "\n",
        "text = \"\"\"\n",
        "Here's a more detailed breakdown:\n",
        "What it is:\n",
        "GenAI models are trained on massive datasets of existing content (like text, images, etc.) and learn to generate new content that resembles the training data.\n",
        "They leverage techniques like large language models, which are capable of understanding and generating human-like text.\n",
        "GenAI can be used to create a wide range of content, including text, images, videos, audio, code, and 3D designs.\n",
        "How it works:\n",
        "Users provide prompts (natural language requests) to the GenAI model.\n",
        "The model analyzes the prompt and generates a corresponding response based on its training data.\n",
        "This process involves prompt engineering, where users refine their prompts to get the desired output.\n",
        "Key applications and use cases:\n",
        "Content creation:\n",
        "GenAI can be used to generate articles, blog posts, marketing copy, social media content, and more.\n",
        "Image and video generation:\n",
        "It can be used to create images, videos, and even short-form video content.\n",
        "Software development:\n",
        "GenAI can assist with code generation, debugging, and even automating parts of the software development process.\n",
        "Customer service:\n",
        "GenAI-powered chatbots can provide 24/7 support, answer queries, and even handle complex customer interactions.\n",
        "Data analysis and insights:\n",
        "GenAI can help analyze large datasets, identify patterns, and generate insights that would be difficult for humans to discover.\n",
        "Translation:\n",
        "GenAI can improve language translation services by providing more accurate and contextually relevant translations.\n",
        "\"\"\"\n",
        "\n",
        "# Step 4 - Chains (Parallel)\n",
        "parallel_chain = RunnableParallel(\n",
        "    notes=chain_notes,\n",
        "    faqs=chain_faqs\n",
        ")\n",
        "\n",
        "# Step 5 - Invoke the parallel chain\n",
        "result_parallel = parallel_chain.invoke({\"text\": text})\n",
        "\n",
        "# parallel_chain.get_graph().print_ascii()\n",
        "\n",
        "# Print the results\n",
        "#print(\"Notes:\")\n",
        "#print(result_parallel[\"notes\"])\n",
        "#print(\"\\nFAQs:\")\n",
        "#print(result_parallel[\"faqs\"])\n",
        "\n",
        "# Step 1 - Prompt for answering FAQs\n",
        "prompt_answers = PromptTemplate(\n",
        "    template=\"Based on these notes:\\n{notes}\\n\\nAnswer these questions:\\n{faqs}\\n\\nProvide concise answers.\",\n",
        "    input_variables=[\"notes\", \"faqs\"]\n",
        ")\n",
        "\n",
        "# Step 2 - Choose and Call a LLM Model for answering FAQs\n",
        "llm_answers = ChatOpenAI(model='gpt-4o-mini', temperature=0.1)\n",
        "\n",
        "# Step 3 - Output (Response) Parser\n",
        "parser3 = StrOutputParser()\n",
        "\n",
        "# Step 4 - Create the sequential chain to answer FAQs\n",
        "answer_chain = prompt_answers | llm_answers | parser3\n",
        "\n",
        "final_chain = parallel_chain | answer_chain\n",
        "\n",
        "# Step 5 - Invoke the sequential chain with the output from the parallel chain\n",
        "result_answers = answer_chain.invoke(result_parallel)\n",
        "\n",
        "# Print the answers\n",
        "#print(\"\\nAnswers to FAQs:\")\n",
        "#print(result_answers)\n",
        "#parallel_chain.get_graph().print_ascii()\n",
        "final_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUWs3UO8CP8I",
        "outputId": "415b9dd0-27f6-4876-86bd-b1b9439e528b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            +---------------------------+            \n",
            "            | Parallel<notes,faqs>Input |            \n",
            "            +---------------------------+            \n",
            "                 **               **                 \n",
            "              ***                   ***              \n",
            "            **                         **            \n",
            "+----------------+                +----------------+ \n",
            "| PromptTemplate |                | PromptTemplate | \n",
            "+----------------+                +----------------+ \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "  +------------+                    +------------+   \n",
            "  | ChatOpenAI |                    | ChatOpenAI |   \n",
            "  +------------+                    +------------+   \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "+-----------------+              +-----------------+ \n",
            "| StrOutputParser |              | StrOutputParser | \n",
            "+-----------------+              +-----------------+ \n",
            "                 **               **                 \n",
            "                   ***         ***                   \n",
            "                      **     **                      \n",
            "           +----------------------------+            \n",
            "           | Parallel<notes,faqs>Output |            \n",
            "           +----------------------------+            \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "                 +----------------+                  \n",
            "                 | PromptTemplate |                  \n",
            "                 +----------------+                  \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "                   +------------+                    \n",
            "                   | ChatOpenAI |                    \n",
            "                   +------------+                    \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "                +-----------------+                  \n",
            "                | StrOutputParser |                  \n",
            "                +-----------------+                  \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "              +-----------------------+              \n",
            "              | StrOutputParserOutput |              \n",
            "              +-----------------------+              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5"
      ],
      "metadata": {
        "id": "KFygYAygFeni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel\n",
        "\n",
        "model1 = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
        "model2 = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
        "\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Generate short and simple notes from the following text \\n {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Generate 5 short question answers from the following text \\n {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "prompt3 = PromptTemplate(\n",
        "    template='Merge the provided notes and quiz into a single document \\n notes -> {notes} and quiz -> {quiz}',\n",
        "    input_variables=['notes', 'quiz']\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    'notes': prompt1 | model1 | parser,\n",
        "    'quiz': prompt2 | model2 | parser\n",
        "})\n",
        "\n",
        "merge_chain = prompt3 | model1 | parser # stateless -> does not store -> Brains (Memory)\n",
        "\n",
        "chain = parallel_chain | merge_chain\n",
        "\n",
        "text = \"\"\"\n",
        "\n",
        "Generative AI (GenAI) is a type of artificial intelligence that can\n",
        "create new content, such as text, images, audio, and video,\n",
        "by learning from existing data.\n",
        "It's a rapidly evolving field with applications across various industries,\n",
        "from content creation to software development.\n",
        "GenAI models are trained on massive datasets and\n",
        "can generate outputs that mimic human creativity and problem-solving.\n",
        "\n",
        "What it is: Generative AI refers to AI models that can generate\n",
        "new data based on patterns learned from training data. Unlike traditional AI,\n",
        "which often focuses on analysis and classification, GenAI creates original content.\n",
        "How it works: GenAI models, often based on deep learning and neural networks,\n",
        "are trained on vast amounts of data. They learn to identify patterns\n",
        "and relationships within the data, allowing them to generate\n",
        "new outputs that resemble the training data.\n",
        "Key characteristics:\n",
        "Creativity:\n",
        "GenAI can produce seemingly original and diverse outputs, pushing the boundaries of what's possible with AI.\n",
        "Versatility:\n",
        "GenAI models can handle various types of input and output data, making them relevant to many economic sectors.\n",
        "Learning capabilities:\n",
        "They can learn from diverse datasets, including text, images, audio, and more, enabling them to adapt to different tasks and domains.\n",
        "Contextual understanding:\n",
        "GenAI models can understand and respond to human language, generating contextually relevant responses.\n",
        "Fine-tuning:\n",
        "These models can be fine-tuned for specific tasks, further enhancing their capabilities.\n",
        "\"\"\"\n",
        "\n",
        "result = chain.invoke({'text':text})\n",
        "\n",
        "print(result)\n",
        "\n",
        "chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV5Lslt7EY9n",
        "outputId": "e28c9812-8eb9-41a7-f6a0-17a2d5412724"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Notes and Quiz on Generative AI (GenAI)\n",
            "\n",
            "### Notes on Generative AI (GenAI)\n",
            "\n",
            "- **Definition**: GenAI is a type of AI that creates new content (text, images, audio, video) by learning from existing data.\n",
            "  \n",
            "- **Field**: Rapidly evolving with applications in various industries (content creation, software development).\n",
            "\n",
            "- **Training**: Models are trained on large datasets to mimic human creativity and problem-solving.\n",
            "\n",
            "- **Key Differences**: \n",
            "  - GenAI generates original content, unlike traditional AI which focuses on analysis and classification.\n",
            "\n",
            "- **How It Works**: \n",
            "  - Based on deep learning and neural networks.\n",
            "  - Learns patterns and relationships in data to generate new outputs.\n",
            "\n",
            "- **Key Characteristics**:\n",
            "  - **Creativity**: Produces original and diverse outputs.\n",
            "  - **Versatility**: Handles various input/output types, applicable across sectors.\n",
            "  - **Learning Capabilities**: Adapts to different tasks using diverse datasets.\n",
            "  - **Contextual Understanding**: Responds to human language with relevant outputs.\n",
            "  - **Fine-tuning**: Can be adjusted for specific tasks to enhance performance.\n",
            "\n",
            "---\n",
            "\n",
            "### Quiz on Generative AI (GenAI)\n",
            "\n",
            "1. **What is Generative AI (GenAI)?**  \n",
            "   Generative AI is a type of artificial intelligence that creates new content, such as text, images, audio, and video, by learning from existing data.\n",
            "\n",
            "2. **How does Generative AI differ from traditional AI?**  \n",
            "   Unlike traditional AI, which focuses on analysis and classification, Generative AI creates original content based on patterns learned from training data.\n",
            "\n",
            "3. **What are some key characteristics of Generative AI?**  \n",
            "   Key characteristics include creativity, versatility, learning capabilities, contextual understanding, and the ability to be fine-tuned for specific tasks.\n",
            "\n",
            "4. **What technologies are commonly used in Generative AI models?**  \n",
            "   Generative AI models are often based on deep learning and neural networks.\n",
            "\n",
            "5. **In what industries can Generative AI be applied?**  \n",
            "   Generative AI has applications across various industries, including content creation and software development.\n",
            "            +---------------------------+            \n",
            "            | Parallel<notes,quiz>Input |            \n",
            "            +---------------------------+            \n",
            "                 **               **                 \n",
            "              ***                   ***              \n",
            "            **                         **            \n",
            "+----------------+                +----------------+ \n",
            "| PromptTemplate |                | PromptTemplate | \n",
            "+----------------+                +----------------+ \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "  +------------+                    +------------+   \n",
            "  | ChatOpenAI |                    | ChatOpenAI |   \n",
            "  +------------+                    +------------+   \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "          *                               *          \n",
            "+-----------------+              +-----------------+ \n",
            "| StrOutputParser |              | StrOutputParser | \n",
            "+-----------------+              +-----------------+ \n",
            "                 **               **                 \n",
            "                   ***         ***                   \n",
            "                      **     **                      \n",
            "           +----------------------------+            \n",
            "           | Parallel<notes,quiz>Output |            \n",
            "           +----------------------------+            \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "                 +----------------+                  \n",
            "                 | PromptTemplate |                  \n",
            "                 +----------------+                  \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "                   +------------+                    \n",
            "                   | ChatOpenAI |                    \n",
            "                   +------------+                    \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "                +-----------------+                  \n",
            "                | StrOutputParser |                  \n",
            "                +-----------------+                  \n",
            "                          *                          \n",
            "                          *                          \n",
            "                          *                          \n",
            "              +-----------------------+              \n",
            "              | StrOutputParserOutput |              \n",
            "              +-----------------------+              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional Chain"
      ],
      "metadata": {
        "id": "cerJqtC1FpYZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableBranch\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define the models\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Define different prompts for different topics\n",
        "sports_prompt = PromptTemplate(\n",
        "    template=\"Generate 3 interesting facts about {topic} in sports.\",\n",
        "    input_variables=[\"topic\"]\n",
        ")\n",
        "\n",
        "general_prompt = PromptTemplate(\n",
        "    template=\"Generate 3 interesting facts about {topic}.\",\n",
        "    input_variables=[\"topic\"]\n",
        ")\n",
        "\n",
        "# Define chains for each condition\n",
        "sports_chain = sports_prompt | llm | parser\n",
        "general_chain = general_prompt | llm | parser\n",
        "\n",
        "# Define a function to determine the route\n",
        "def route_topic(info):\n",
        "    if \"sports\" in info[\"topic\"].lower():\n",
        "        return \"sports\"\n",
        "    else:\n",
        "        return \"general\"\n",
        "\n",
        "# Create the conditional chain using RunnableBranch\n",
        "conditional_chain = RunnableBranch(\n",
        "    (lambda x: \"sports\" in x[\"topic\"].lower(), sports_chain),\n",
        "    general_chain # Default route if the condition is not met\n",
        ")\n",
        "\n",
        "# You can also visualize the graph (optional)\n",
        "conditional_chain.get_graph().print_ascii()\n",
        "\n",
        "# Invoke the conditional chain with different inputs\n",
        "print(\"--- Sports Topic ---\")\n",
        "result_sports = conditional_chain.invoke({\"topic\": \"cricket sports\"})\n",
        "print(result_sports)\n",
        "\n",
        "print(\"\\n--- General Topic ---\")\n",
        "result_general = conditional_chain.invoke({\"topic\": \"the moon\"})\n",
        "print(result_general)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvOB91v9FqwI",
        "outputId": "c23114c6-c5ff-457c-d7d4-7553b2dbef7c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+  \n",
            "| PromptInput |  \n",
            "+-------------+  \n",
            "        *        \n",
            "        *        \n",
            "        *        \n",
            "   +--------+    \n",
            "   | Branch |    \n",
            "   +--------+    \n",
            "        *        \n",
            "        *        \n",
            "        *        \n",
            "+--------------+ \n",
            "| BranchOutput | \n",
            "+--------------+ \n",
            "--- Sports Topic ---\n",
            "Sure! Here are three interesting facts about cricket:\n",
            "\n",
            "1. **The Longest Match in History**: The longest cricket match ever played lasted for 12 days! This match took place between England and South Africa in 1939 at Durban. It was a Test match that was ultimately drawn, as it was interrupted by rain and the match was called off due to the end of the season.\n",
            "\n",
            "2. **The Birthplace of Cricket**: Cricket is believed to have originated in England in the 16th century. The first recorded match took place in 1646, and the sport has since evolved into one of the most popular games worldwide, with a massive following in countries like India, Australia, and Pakistan.\n",
            "\n",
            "3. **The Unique Format of T20**: Introduced in the early 2000s, Twenty20 (T20) cricket has revolutionized the sport by condensing a traditional match into just three hours. Each team bats for a maximum of 20 overs, making it a fast-paced and exciting format that has attracted a new generation of fans and players, leading to the establishment of various T20 leagues around the world, including the Indian Premier League (IPL).\n",
            "\n",
            "--- General Topic ---\n",
            "Sure! Here are three interesting facts about the Moon:\n",
            "\n",
            "1. **Moonquakes**: Just like Earth has earthquakes, the Moon experiences \"moonquakes.\" These seismic activities can be caused by tidal forces from Earth, meteorite impacts, or thermal expansion of the lunar surface. Some moonquakes can be quite strong, with magnitudes reaching up to 5.5 on the Richter scale.\n",
            "\n",
            "2. **Lunar Maria**: The dark, flat plains on the Moon's surface, known as \"maria\" (Latin for \"seas\"), were formed by ancient volcanic activity. These areas are less cratered than the highlands, indicating they are younger in geological terms. The largest maria, such as the Sea of Tranquility, are visible from Earth and were once thought to be actual seas.\n",
            "\n",
            "3. **Tidal Locking**: The Moon is in synchronous rotation with Earth, meaning it takes the same amount of time to rotate on its axis as it does to orbit Earth. As a result, we only see one side of the Moon from our planet. The far side, often mistakenly called the \"dark side,\" is not perpetually dark; it receives sunlight just like the near side, but it remains hidden from our view.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# End of the Notebook"
      ],
      "metadata": {
        "id": "_-aZSrP2F7jc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9exTvOL4GmxW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}