{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c1akQ2DCzx3w"
      },
      "outputs": [],
      "source": [
        "# Welcome to Langchain Deep Dive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example #1 — “Chat history only” (minimal, no memory wrapper)"
      ],
      "metadata": {
        "id": "TOvbn7Wl0TzT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal dependencies (Colab-friendly)\n",
        "!pip install -q langchain langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10XupNiA0yiz",
        "outputId": "6b486265-9d2e-4e48-ca17-a9d8df873646"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m61.4/70.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Set your API key in Colab once with:\n",
        "# from google.colab import userdata; userdata.set(\"OPENAI_API_KEY\", \"sk-...\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "#llm = ChatOpenAI()  # uses OpenAI chat model via langchain-openai\n",
        "\n",
        "# Set your model + temperature here\n",
        "#llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0.0)\n",
        "\n",
        "# Start with a system instruction and an empty conversation\n",
        "history = [SystemMessage(content=\"You are a helpful, concise assistant.\")]\n",
        "\n",
        "print(\"💬 Chat started! Type 'exit' to quit.\\n\")\n",
        "while True:\n",
        "    user_text = input(\"You: \")\n",
        "    if user_text.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"👋 Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Add the user's message to history\n",
        "    history.append(HumanMessage(content=user_text))\n",
        "\n",
        "    # Ask the model, passing the full message list (history)\n",
        "    response = llm.invoke(history)\n",
        "\n",
        "    # Add the assistant's reply to history and print it\n",
        "    history.append(AIMessage(content=response.content))\n",
        "    print(\"Bot:\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH7j15Y_0qpR",
        "outputId": "f0a9ab3e-7729-4ced-d238-bcbec9caf14c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💬 Chat started! Type 'exit' to quit.\n",
            "\n",
            "You: hello this is Sandip \n",
            "Bot: Hello Sandip! How can I assist you today?\n",
            "You: who is Big B\n",
            "Bot: \"Big B\" is a popular nickname for Amitabh Bachchan, a legendary Indian actor and film producer known for his work in Bollywood.\n",
            "You: what did I ask\n",
            "Bot: You asked, \"who is Big B.\"\n",
            "You: exit\n",
            "👋 Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example #2  — Memory via RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "VqAqv0Tz1bNp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# 1) Model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.25)\n",
        "\n",
        "# 2) Prompt (history + current input)\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(\"history\"),     # <- message objects go here\n",
        "    (\"human\", \"{input}\")                # <- the latest user turn\n",
        "])\n",
        "\n",
        "# 3) Chain\n",
        "chain = prompt | llm\n",
        "\n",
        "# 4) History store (implements BaseChatMessageHistory)\n",
        "class SimpleChatHistory(BaseChatMessageHistory):\n",
        "    def __init__(self): self._messages = []\n",
        "    @property\n",
        "    def messages(self): return self._messages\n",
        "    def add_user_message(self, text): self._messages.append(HumanMessage(content=text))\n",
        "    def add_ai_message(self, text): self._messages.append(AIMessage(content=text))\n",
        "    def add_message(self, m): self._messages.append(m)\n",
        "    def add_messages(self, ms): self._messages.extend(ms)\n",
        "    def clear(self): self._messages.clear()\n",
        "\n",
        "# Wrap chain with memory\n",
        "chat = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: SimpleChatHistory(),  # return a history object per session\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")\n",
        "\n",
        "# Use it\n",
        "resp = chat.invoke({\"input\": \"Hi!\"}, config={\"configurable\": {\"session_id\": \"s1\"}})\n",
        "print(resp.content)\n"
      ],
      "metadata": {
        "id": "YWnNFVgO5Naf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example # 3"
      ],
      "metadata": {
        "id": "5ck-uj3y5M9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install (Colab)\n",
        "!pip install -q langchain langchain-openai\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import re\n",
        "\n",
        "# ── Config ─────────────────────────────────────────────────────────────────────\n",
        "# Set API key in Colab once:  userdata.set(\"OPENAI_API_KEY\", \"sk-...\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "MODEL = \"gpt-4.1-nano\"     # pick a ChatGPT-compatible model\n",
        "TEMPERATURE = 0.25        # 0.0–0.7 is a good working range\n",
        "\n",
        "# ── LLM ─────────────────────────────────────────────────────────────────────────\n",
        "llm = ChatOpenAI(model=MODEL, temperature=TEMPERATURE)\n",
        "\n",
        "# ── Prompt: include full message history AND a clean list of prior user questions\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"You are a helpful assistant.\\n\"\n",
        "     \"Below is the conversation so far (messages):\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"system\",\n",
        "     \"The user's previous questions (verbatim) are:\\n{user_questions}\\n\"\n",
        "     \"If the user asks what they asked earlier, list those questions as bullet points, \"\n",
        "     \"without adding extra ones.\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "# ── Minimal in-memory chat history ──────────────────────────────────────────────\n",
        "class SimpleChatHistory(BaseChatMessageHistory):\n",
        "    def __init__(self):\n",
        "        self._messages: list[BaseMessage] = []\n",
        "\n",
        "    @property\n",
        "    def messages(self):\n",
        "        return self._messages\n",
        "\n",
        "    def add_user_message(self, content: str):\n",
        "        self._messages.append(HumanMessage(content=content))\n",
        "\n",
        "    def add_ai_message(self, content: str):\n",
        "        self._messages.append(AIMessage(content=content))\n",
        "\n",
        "    def add_message(self, message: BaseMessage):\n",
        "        self._messages.append(message)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]):\n",
        "        self._messages.extend(messages)\n",
        "\n",
        "    def clear(self):\n",
        "        self._messages.clear()\n",
        "\n",
        "# Keep one history object per session id\n",
        "_histories = {}\n",
        "def get_history(session_id: str) -> SimpleChatHistory:\n",
        "    if session_id not in _histories:\n",
        "        _histories[session_id] = SimpleChatHistory()\n",
        "    return _histories[session_id]\n",
        "\n",
        "# Wrap chain with message history\n",
        "chat_with_memory = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: get_history(session_id),\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "# Helper: build a clean bullet list of prior user questions from history\n",
        "QUESTION_LIKE = re.compile(r\".*\\?$|^(who|what|when|where|why|how|do|does|did|can|could|should|is|are|was|were)\\b\", re.I)\n",
        "\n",
        "def prior_user_questions(history: SimpleChatHistory) -> str:\n",
        "    qs = []\n",
        "    for m in history.messages:\n",
        "        if isinstance(m, HumanMessage):\n",
        "            text = m.content.strip()\n",
        "            # treat anything that looks like a question as a question\n",
        "            if text.endswith(\"?\") or QUESTION_LIKE.match(text):\n",
        "                qs.append(text)\n",
        "    if not qs:\n",
        "        return \"- (no prior user questions yet)\"\n",
        "    return \"\\n\".join(f\"- {q}\" for q in qs)\n",
        "\n",
        "# ── Chat loop ───────────────────────────────────────────────────────────────────\n",
        "print(\"💬 Chat started! Type 'exit' to quit.\\n\")\n",
        "SESSION_ID = \"demo-session\"\n",
        "\n",
        "while True:\n",
        "    user_text = input(\"You: \")\n",
        "    if user_text.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"👋 Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Build the user-questions summary from current history\n",
        "    hist = get_history(SESSION_ID)\n",
        "    user_qs = prior_user_questions(hist)\n",
        "\n",
        "    # Invoke with both the normal input and the computed {user_questions}\n",
        "    response = chat_with_memory.invoke(\n",
        "        {\"input\": user_text, \"user_questions\": user_qs},\n",
        "        config={\"configurable\": {\"session_id\": SESSION_ID}},\n",
        "    )\n",
        "    print(\"Bot:\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq1Mq-Vn2Bh4",
        "outputId": "09ab4976-4c38-4689-b3c1-19df86249674"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💬 Chat started! Type 'exit' to quit.\n",
            "\n",
            "You: hi I am Sandip\n",
            "Bot: Hello Sandip! How can I assist you today?\n",
            "You: who is SRK\n",
            "Bot: SRK stands for Shah Rukh Khan, a famous Indian actor, film producer, and television personality. He is widely regarded as one of the biggest stars in Bollywood and has appeared in numerous popular movies. Would you like to know more about him?\n",
            "You: Who is Big B\n",
            "Bot: Big B is a popular nickname for Amitabh Bachchan, a legendary Indian actor, film producer, and television host. He is considered one of the greatest and most influential actors in the history of Indian cinema. Would you like to know more about him?\n",
            "You: Who is God of Cricket in India\n",
            "Bot: The title \"God of Cricket\" in India is commonly attributed to Sachin Tendulkar. He is regarded as one of the greatest cricketers of all time and has set numerous records in international cricket. Would you like more information about him?\n",
            "You: what all I ask u\n",
            "Bot: Here are the questions you asked earlier:\n",
            "- who is SRK\n",
            "- Who is Big B\n",
            "- Who is God of Cricket in India\n",
            "\n",
            "Is there anything else you'd like to know?\n",
            "You: exit\n",
            "👋 Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings , Vectors and FAISS"
      ],
      "metadata": {
        "id": "JwWK-8HG2DYj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Simple Embeddings"
      ],
      "metadata": {
        "id": "eis_6oSF662_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "!pip install -q langchain-openai\n",
        "\n",
        "# Imports\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# 1) Put your key once in Colab (in a separate cell):\n",
        "# from google.colab import userdata\n",
        "# userdata.set(\"OPENAI_API_KEY\", \"sk-...\")\n",
        "\n",
        "# 2) Load key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# 3) Create the embeddings model\n",
        "#   - \"text-embedding-3-small\": fast, cheap, 1536-dim\n",
        "#   - \"text-embedding-3-large\": higher quality, 3072-dim\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# 4) Take input and embed\n",
        "text = input(\"Enter text to embed: \").strip()\n",
        "vec = emb.embed_query(text)  # list[float]\n",
        "\n",
        "# 5) Show results\n",
        "print(f\"\\nText: {text!r}\")\n",
        "print(f\"Vector length (dimension): {len(vec)}\")\n",
        "print(\"First 8 numbers:\", [round(x, 6) for x in vec[:8]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt6JPj5i83HI",
        "outputId": "073a1d1c-d90e-4a5f-c4d6-067f221af573"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text to embed: hello I am Sandip, live in Pune\n",
            "\n",
            "Text: 'hello I am Sandip, live in Pune'\n",
            "Vector length (dimension): 1536\n",
            "First 8 numbers: [0.016876, -0.005168, 0.026773, 0.020525, -0.027236, -0.033387, -0.012087, 0.0159]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Create + STORE embeddings (no search)"
      ],
      "metadata": {
        "id": "5MQ3qkri8-AN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Basic: Create + STORE embeddings (no search)\n",
        "# ============================================\n",
        "\n",
        "# 0) Install deps\n",
        "!pip install -q langchain-openai faiss-cpu numpy pandas\n",
        "\n",
        "# 1) Imports & API key\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from google.colab import userdata\n",
        "import os, json, numpy as np, pandas as pd\n",
        "import faiss  # direct FAISS usage\n",
        "\n",
        "# Load key\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise RuntimeError(\n",
        "        \"❌ OPENAI_API_KEY not found in Colab userdata.\\n\"\n",
        "        \"Set it once with:\\n\"\n",
        "        \"from google.colab import userdata\\n\"\n",
        "        \"userdata.set('OPENAI_API_KEY', 'sk-...')\"\n",
        "    )\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# 2) Collect texts (user input or defaults)\n",
        "raw = input(\n",
        "    \"Enter texts separated by '||' (or press Enter to use defaults):\\n\"\n",
        ").strip()\n",
        "\n",
        "if raw:\n",
        "    texts = [t.strip() for t in raw.split(\"||\") if t.strip()]\n",
        "else:\n",
        "    texts = [\n",
        "        \"MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\",\n",
        "        \"SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\",\n",
        "        \"Amitabh Bachchan is nicknamed Big B in Indian cinema.\",\n",
        "        \"Shah Rukh Khan is popularly called King Khan.\",\n",
        "    ]\n",
        "\n",
        "print(f\"\\n📝 {len(texts)} texts to embed.\\n\")\n",
        "\n",
        "# 3) Create embeddings (vectors)\n",
        "emb_model = \"text-embedding-3-small\"  # 1536-dim; switch to -large for 3072-dim\n",
        "emb = OpenAIEmbeddings(model=emb_model)\n",
        "\n",
        "# embed_documents -> list of vectors (one per text)\n",
        "vectors = emb.embed_documents(texts)  # list[list[float]]\n",
        "dim = len(vectors[0]) if vectors else 0\n",
        "\n",
        "print(f\"✅ Created {len(vectors)} embeddings with dimension = {dim} (model: {emb_model}).\\n\")\n",
        "\n",
        "# 4) SHOW what we stored (table preview)\n",
        "rows = []\n",
        "for i, (t, v) in enumerate(zip(texts, vectors), start=1):\n",
        "    rows.append({\n",
        "        \"id\": f\"doc-{i}\",\n",
        "        \"text\": t,\n",
        "        \"dim\": len(v),\n",
        "        \"preview(first_8_dims)\": [round(x, 6) for x in v[:8]]\n",
        "    })\n",
        "df = pd.DataFrame(rows)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# 5) STORE embeddings plainly (files you can inspect)\n",
        "#    - vectors.npy: the raw numeric vectors (float32)\n",
        "#    - texts.jsonl: metadata (id + original text)\n",
        "vec_arr = np.array(vectors, dtype=\"float32\")\n",
        "np.save(\"vectors.npy\", vec_arr)\n",
        "with open(\"texts.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, t in enumerate(texts, start=1):\n",
        "        f.write(json.dumps({\"id\": f\"doc-{i}\", \"text\": t}, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"\\n💾 Saved raw vectors to ./vectors.npy and text metadata to ./texts.jsonl\")\n",
        "\n",
        "# 6) STORE embeddings in a FAISS index (no search yet)\n",
        "#    Build an L2 index and add our vectors.\n",
        "if len(vec_arr) > 0:\n",
        "    index = faiss.IndexFlatL2(dim)  # exact L2 index\n",
        "    index.add(vec_arr)              # add all vectors\n",
        "    print(f\"📦 FAISS index created. ntotal = {index.ntotal} vectors.\")\n",
        "\n",
        "    # (Optional) Save FAISS index to disk\n",
        "    faiss.write_index(index, \"index.faiss\")\n",
        "    print(\"💾 Saved FAISS index to ./index.faiss\")\n",
        "\n",
        "    # (Optional) Show how to reload it later\n",
        "    reloaded = faiss.read_index(\"index.faiss\")\n",
        "    print(f\"🔁 Reloaded FAISS index. ntotal = {reloaded.ntotal} vectors.\")\n",
        "else:\n",
        "    print(\"No vectors created; FAISS index not built.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pHEJr8s_QrP",
        "outputId": "87292963-b11e-41ba-bb9b-4753d592a82e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter texts separated by '||' (or press Enter to use defaults):\n",
            "\n",
            "\n",
            "📝 4 texts to embed.\n",
            "\n",
            "✅ Created 4 embeddings with dimension = 1536 (model: text-embedding-3-small).\n",
            "\n",
            "   id                                                             text  dim                                                               preview(first_8_dims)\n",
            "doc-1 MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain. 1536  [0.108236, -0.016901, 0.041789, 0.037184, -0.008478, 0.011751, 0.037075, 0.007594]\n",
            "doc-2     SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'. 1536  [0.047056, 0.011706, 0.035647, 0.005719, -0.037854, -0.018892, 0.030116, 0.020833]\n",
            "doc-3            Amitabh Bachchan is nicknamed Big B in Indian cinema. 1536  [0.052879, -0.021684, 0.005693, 0.058494, -0.06329, -0.003692, 0.026152, 0.047099]\n",
            "doc-4                    Shah Rukh Khan is popularly called King Khan. 1536 [0.016203, -0.09365, -0.018317, 0.037572, 0.015276, -0.009722, -0.006646, 0.055001]\n",
            "\n",
            "💾 Saved raw vectors to ./vectors.npy and text metadata to ./texts.jsonl\n",
            "📦 FAISS index created. ntotal = 4 vectors.\n",
            "💾 Saved FAISS index to ./index.faiss\n",
            "🔁 Reloaded FAISS index. ntotal = 4 vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Tiny Search"
      ],
      "metadata": {
        "id": "vb8KTMjB_WZh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Tiny similarity search with FAISS (Colab)\n",
        "# ============================================\n",
        "\n",
        "# 0) Install deps (if not already)\n",
        "!pip install -q langchain-openai faiss-cpu numpy\n",
        "\n",
        "# 1) Imports & API key\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from google.colab import userdata\n",
        "import os, json, numpy as np, faiss, sys\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    print(\"❌ OPENAI_API_KEY not found. Set it with:\\n\"\n",
        "          \"from google.colab import userdata\\n\"\n",
        "          \"userdata.set('OPENAI_API_KEY', 'sk-...')\\n\")\n",
        "    sys.exit(1)\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# 2) Try loading previously saved artifacts (from the “store” step)\n",
        "INDEX_PATH = \"index.faiss\"\n",
        "TEXTS_PATH = \"texts.jsonl\"\n",
        "EMB_MODEL = \"text-embedding-3-small\"    # must match the model used to build the index\n",
        "\n",
        "def load_texts_jsonl(path):\n",
        "    items, ids = [], []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            obj = json.loads(line)\n",
        "            ids.append(obj.get(\"id\"))\n",
        "            items.append(obj.get(\"text\"))\n",
        "    return ids, items\n",
        "\n",
        "need_rebuild = False\n",
        "if os.path.exists(INDEX_PATH) and os.path.exists(TEXTS_PATH):\n",
        "    try:\n",
        "        index = faiss.read_index(INDEX_PATH)\n",
        "        doc_ids, texts = load_texts_jsonl(TEXTS_PATH)\n",
        "        print(f\"✅ Loaded FAISS index (ntotal={index.ntotal}) and {len(texts)} texts.\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Could not load previous index/metadata. Will rebuild a small demo index.\\nReason:\", e)\n",
        "        need_rebuild = True\n",
        "else:\n",
        "    need_rebuild = True\n",
        "\n",
        "# 3) If needed, rebuild a small index (self-contained)\n",
        "if need_rebuild:\n",
        "    print(\"🔧 Rebuilding a small demo index locally...\")\n",
        "    texts = [\n",
        "        \"MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\",\n",
        "        \"SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\",\n",
        "        \"Amitabh Bachchan is nicknamed Big B in Indian cinema.\",\n",
        "        \"Shah Rukh Khan is popularly called King Khan.\",\n",
        "        \"Virat Kohli is an Indian cricketer known for batting consistency.\",\n",
        "        \"Rohit Sharma is known for explosive batting in limited-overs.\"\n",
        "    ]\n",
        "    emb = OpenAIEmbeddings(model=EMB_MODEL)\n",
        "    vecs = emb.embed_documents(texts)           # list[list[float]]\n",
        "    vec_arr = np.array(vecs, dtype=\"float32\")   # (N, dim)\n",
        "    dim = vec_arr.shape[1]\n",
        "\n",
        "    index = faiss.IndexFlatL2(dim)              # exact L2 index\n",
        "    index.add(vec_arr)\n",
        "\n",
        "    # save artifacts for later use\n",
        "    faiss.write_index(index, INDEX_PATH)\n",
        "    with open(TEXTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, t in enumerate(texts, start=1):\n",
        "            f.write(json.dumps({\"id\": f\"doc-{i}\", \"text\": t}, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"✅ Built and saved demo index. ntotal={index.ntotal}, dim={dim}\")\n",
        "\n",
        "# 4) Run a tiny search\n",
        "emb = OpenAIEmbeddings(model=EMB_MODEL)  # same model as used for the index\n",
        "query = input(\"\\nEnter your search query (e.g., 'Who is King Khan?'): \").strip()\n",
        "if not query:\n",
        "    query = \"Who is King Khan?\"\n",
        "    print(f\"(Using default) {query}\")\n",
        "\n",
        "q_vec = np.array([emb.embed_query(query)], dtype=\"float32\")  # shape (1, dim)\n",
        "\n",
        "k = 3  # top-k results\n",
        "distances, indices = index.search(q_vec, k)   # shapes: (1, k), (1, k)\n",
        "dist_list = distances[0].tolist()\n",
        "idx_list = indices[0].tolist()\n",
        "\n",
        "# load texts (if not already loaded)\n",
        "if 'texts' not in globals():\n",
        "    _, texts = load_texts_jsonl(TEXTS_PATH)\n",
        "\n",
        "print(f\"\\n🔎 Top-{k} results for: {query!r}  (Lower distance = closer match)\\n\")\n",
        "for rank, (idx, dist) in enumerate(zip(idx_list, dist_list), start=1):\n",
        "    if idx == -1:\n",
        "        continue  # FAISS returns -1 if not enough results\n",
        "    # idx corresponds to the position of the text used when building the index\n",
        "    print(f\"{rank}. [distance={dist:.4f}]  {texts[idx]}\")\n",
        "\n",
        "print(\"\\n✅ Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLjWzKgcAYnU",
        "outputId": "ca5b1e44-7bb4-4985-e01f-6efa9d3e0941"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded FAISS index (ntotal=4) and 4 texts.\n",
            "\n",
            "Enter your search query (e.g., 'Who is King Khan?'): Who is MSD\n",
            "\n",
            "🔎 Top-3 results for: 'Who is MSD'  (Lower distance = closer match)\n",
            "\n",
            "1. [distance=0.8261]  MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\n",
            "2. [distance=1.5741]  SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\n",
            "3. [distance=1.8090]  Amitabh Bachchan is nicknamed Big B in Indian cinema.\n",
            "\n",
            "✅ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4 - Cosine Similarity"
      ],
      "metadata": {
        "id": "vGWuw35TAbpy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Cosine similarity search with FAISS (normalized vectors)\n",
        "# ============================================================\n",
        "\n",
        "# 0) Install\n",
        "!pip install -q langchain-openai faiss-cpu numpy\n",
        "\n",
        "# 1) Imports & API key\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from google.colab import userdata\n",
        "import os, json, numpy as np, faiss, sys\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    print(\"❌ OPENAI_API_KEY not found. Set it with:\\n\"\n",
        "          \"from google.colab import userdata\\n\"\n",
        "          \"userdata.set('OPENAI_API_KEY', 'sk-...')\\n\")\n",
        "    sys.exit(1)\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "EMB_MODEL = \"text-embedding-3-small\"  # same model must be used for index & queries\n",
        "\n",
        "# 2) Data to index (you can replace with your own)\n",
        "texts = [\n",
        "    \"MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\",\n",
        "    \"SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\",\n",
        "    \"Amitabh Bachchan is nicknamed Big B in Indian cinema.\",\n",
        "    \"Shah Rukh Khan is popularly called King Khan.\",\n",
        "    \"Virat Kohli is an Indian cricketer known for batting consistency.\",\n",
        "    \"Rohit Sharma is known for explosive batting in limited-overs.\"\n",
        "]\n",
        "\n",
        "# 3) Embed and build a cosine index (normalize + IP)\n",
        "emb = OpenAIEmbeddings(model=EMB_MODEL)\n",
        "\n",
        "def l2_normalize_rows(mat: np.ndarray) -> np.ndarray:\n",
        "    norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-12\n",
        "    return mat / norms\n",
        "\n",
        "# Embed documents\n",
        "doc_vecs = emb.embed_documents(texts)            # list[list[float]]\n",
        "doc_arr = np.array(doc_vecs, dtype=\"float32\")    # (N, dim)\n",
        "doc_arr = l2_normalize_rows(doc_arr)             # normalize for cosine\n",
        "\n",
        "dim = doc_arr.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)                   # inner product ≈ cosine (after normalization)\n",
        "index.add(doc_arr)                               # add normalized vectors\n",
        "print(f\"✅ Cosine index ready. ntotal={index.ntotal}, dim={dim}\")\n",
        "\n",
        "# Save small metadata file so we can map results back to text\n",
        "with open(\"texts.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, t in enumerate(texts):\n",
        "        f.write(json.dumps({\"id\": i, \"text\": t}, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# 4) Query loop\n",
        "def load_texts(path=\"texts.jsonl\"):\n",
        "    items = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            items.append(json.loads(line)[\"text\"])\n",
        "    return items\n",
        "\n",
        "stored_texts = load_texts()\n",
        "\n",
        "print(\"\\n💬 Type your query (e.g., 'Who is King Khan?'). Type 'exit' to quit.\")\n",
        "while True:\n",
        "    q = input(\"Query: \").strip()\n",
        "    if q.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"👋 Goodbye!\")\n",
        "        break\n",
        "    if not q:\n",
        "        q = \"Who is King Khan?\"\n",
        "        print(f\"(Using default) {q}\")\n",
        "\n",
        "    # Embed and normalize the query\n",
        "    q_vec = np.array([emb.embed_query(q)], dtype=\"float32\")\n",
        "    q_vec = l2_normalize_rows(q_vec)\n",
        "\n",
        "    k = 3\n",
        "    sims, idxs = index.search(q_vec, k)   # sims in [0..1] after normalization\n",
        "    sims, idxs = sims[0], idxs[0]\n",
        "\n",
        "    print(f\"\\n🔎 Top-{k} cosine matches for: {q!r}\")\n",
        "    for rank, (i, s) in enumerate(zip(idxs, sims), 1):\n",
        "        if i == -1: continue\n",
        "        print(f\"{rank}. score={s:.4f}  |  {stored_texts[i]}\")\n",
        "    print(\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-7Q86ZUCyrb",
        "outputId": "9c415c52-3a95-4108-a359-c7402617f932"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cosine index ready. ntotal=6, dim=1536\n",
            "\n",
            "💬 Type your query (e.g., 'Who is King Khan?'). Type 'exit' to quit.\n",
            "Query: who is Virat Kohli\n",
            "\n",
            "🔎 Top-3 cosine matches for: 'who is Virat Kohli'\n",
            "1. score=0.6832  |  Virat Kohli is an Indian cricketer known for batting consistency.\n",
            "2. score=0.4041  |  Rohit Sharma is known for explosive batting in limited-overs.\n",
            "3. score=0.3921  |  MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\n",
            "\n",
            "Query: who is King Khan\n",
            "\n",
            "🔎 Top-3 cosine matches for: 'who is King Khan'\n",
            "1. score=0.6424  |  Shah Rukh Khan is popularly called King Khan.\n",
            "2. score=0.3208  |  Amitabh Bachchan is nicknamed Big B in Indian cinema.\n",
            "3. score=0.3035  |  Virat Kohli is an Indian cricketer known for batting consistency.\n",
            "\n",
            "Query: who is Rohit Sharma\n",
            "\n",
            "🔎 Top-3 cosine matches for: 'who is Rohit Sharma'\n",
            "1. score=0.6800  |  Rohit Sharma is known for explosive batting in limited-overs.\n",
            "2. score=0.4568  |  SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\n",
            "3. score=0.3934  |  Virat Kohli is an Indian cricketer known for batting consistency.\n",
            "\n",
            "Query: exit\n",
            "👋 Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"plug in a GPT model and do a tiny, clean RAG (Retrieval‑Augmented Generation): retrieve the most relevant snippets from your FAISS index and ask the model to answer using only that context.\n",
        "\n",
        "Below is a single Colab‑ready code block that:\n",
        "\n",
        "builds a small FAISS index (you can paste your own text),\n",
        "\n",
        "wraps it as a retriever,\n",
        "\n",
        "formats retrieved context,\n",
        "\n",
        "calls a GPT model to answer from that context,\n",
        "\n",
        "runs an interactive Q&A loop.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "What this gives us:\n",
        "\n",
        "Embeddings + FAISS turn your reference text into a searchable index.\n",
        "\n",
        "A retriever fetches the top-k relevant chunks for each question.\n",
        "\n",
        "A prompt instructs the model to answer only from the retrieved context, avoiding hallucinations.\n",
        "\n",
        "A tiny interactive loop to ask multiple questions.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "vZM_NHAcHYKX",
        "outputId": "d2acece6-5e06-43a2-8bb8-516d5c4d19aa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWhat this gives us: \\n\\nEmbeddings + FAISS turn your reference text into a searchable index.\\n\\nA retriever fetches the top-k relevant chunks for each question.\\n\\nA prompt instructs the model to answer only from the retrieved context, avoiding hallucinations.\\n\\nA tiny interactive loop to ask multiple questions.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5 - GPT Model Plug In"
      ],
      "metadata": {
        "id": "8hWNr9hKCzJN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Tiny RAG: GPT answers from FAISS-retrieved context (Colab)\n",
        "# ============================================================\n",
        "\n",
        "# 0) Install packages\n",
        "!pip install -q langchain-openai langchain-community faiss-cpu langchain-text-splitters\n",
        "\n",
        "# 1) Imports & API key\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise RuntimeError(\n",
        "        \"❌ OPENAI_API_KEY not found. Set once with:\\n\"\n",
        "        \"from google.colab import userdata\\n\"\n",
        "        \"userdata.set('OPENAI_API_KEY', 'sk-...')\"\n",
        "    )\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# 2) Paste your own source text (or press Enter to use defaults)\n",
        "raw = input(\n",
        "    \"Paste your reference text (or press Enter to use a default cricket sample):\\n\"\n",
        ").strip()\n",
        "\n",
        "if not raw:\n",
        "    raw = \"\"\"\n",
        "Mahendra Singh Dhoni (MSD) captained India and won the 2007 T20 World Cup and the 2011 ODI World Cup.\n",
        "Sachin Ramesh Tendulkar (SRT) is widely called the 'God of Cricket' for his batting records.\n",
        "Amitabh Bachchan, nicknamed Big B, is a legendary actor in Indian cinema.\n",
        "Shah Rukh Khan, often called King Khan, is one of the most popular Bollywood actors worldwide.\n",
        "Virat Kohli is renowned for batting consistency across formats.\n",
        "Rohit Sharma is known for explosive batting and captaincy in limited-overs cricket.\n",
        "\"\"\"\n",
        "\n",
        "# 3) Split into chunks (simple defaults work fine to start)\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=60)\n",
        "docs = splitter.create_documents([raw])\n",
        "\n",
        "# 4) Build vector store + retriever\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")   # fast & good\n",
        "vs = FAISS.from_documents(docs, embedding=emb)\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# 5) Define the RAG prompt\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a helpful assistant. Use ONLY the context to answer.\\n\\n\"\n",
        "    \"Context:\\n{context}\\n\\n\"\n",
        "    \"Question: {question}\\n\"\n",
        "    \"If the answer is not in the context, say you don't know.\\n\"\n",
        "    \"Answer:\"\n",
        ")\n",
        "\n",
        "# 6) Helper to format retrieved docs\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "# 7) LLM + chain\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0.2)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 8) Interactive Q&A\n",
        "print(\"\\n💬 Ask questions about your text. Type 'exit' to quit.\")\n",
        "while True:\n",
        "    q = input(\"Q: \").strip()\n",
        "    if q.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"👋 Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Optional: peek at retrieved chunks (uncomment to see)\n",
        "    # retrieved = retriever.get_relevant_documents(q)\n",
        "    # print(\"\\n[DEBUG] Retrieved context:\\n\", format_docs(retrieved), \"\\n\")\n",
        "\n",
        "    ans = rag_chain.invoke(q)\n",
        "    print(\"A:\", ans, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT4O7Qk7EGxX",
        "outputId": "6d670bce-19ee-4fbf-e46c-b9cc4a7791be"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your reference text (or press Enter to use a default cricket sample):\n",
            "\n",
            "\n",
            "💬 Ask questions about your text. Type 'exit' to quit.\n",
            "Q: who is SRT\n",
            "A: SRT is Sachin Ramesh Tendulkar. \n",
            "\n",
            "Q: who is MSD\n",
            "A: MSD is Mahendra Singh Dhoni. \n",
            "\n",
            "Q: who is King Khan\n",
            "A: Shah Rukh Khan is often called King Khan. \n",
            "\n",
            "Q: who is Sourav Ganguly\n",
            "A: I don't know. \n",
            "\n",
            "Q: who is virat Kohli\n",
            "A: Virat Kohli is renowned for batting consistency across formats. \n",
            "\n",
            "Q: who is Rohit Sharma\n",
            "A: Rohit Sharma is known for explosive batting and captaincy in limited-overs cricket. \n",
            "\n",
            "Q: who is Jasprit Bumrah\n",
            "A: The context does not provide information about Jasprit Bumrah. \n",
            "\n",
            "Q: exit\n",
            "👋 Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 6 - Chunking + Metadata + Citations"
      ],
      "metadata": {
        "id": "T_v-S8M3EHVP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install ---\n",
        "!pip install -q langchain-openai langchain-community langchain-text-splitters faiss-cpu\n",
        "\n",
        "# --- Imports & API key ---\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from google.colab import userdata\n",
        "from langchain.schema import Document\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# --- 1) Source text (replace with your own) ---\n",
        "raw_text = \"\"\"\n",
        "Mahendra Singh Dhoni (MSD) captained India and won the 2007 T20 World Cup and the 2011 ODI World Cup.\n",
        "Sachin Ramesh Tendulkar (SRT) is widely called the 'God of Cricket' for his batting records.\n",
        "Amitabh Bachchan, nicknamed Big B, is a legendary actor in Indian cinema.\n",
        "Shah Rukh Khan, often called King Khan, is one of the most popular Bollywood actors worldwide.\n",
        "Virat Kohli is renowned for batting consistency across formats.\n",
        "Rohit Sharma is known for explosive batting and captaincy in limited-overs cricket.\n",
        "\"\"\"\n",
        "\n",
        "# --- 2) Chunk with metadata (source + chunk_id) ---\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=280, chunk_overlap=80)\n",
        "chunks = splitter.split_text(raw_text)\n",
        "docs = [\n",
        "    Document(page_content=chunk, metadata={\"source\": \"notes.txt\", \"chunk_id\": i})\n",
        "    for i, chunk in enumerate(chunks)\n",
        "]\n",
        "\n",
        "# --- 3) Build vector store + retriever ---\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vs = FAISS.from_documents(docs, embedding=emb)\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# --- 4) Prompt that asks model to cite chunks used ---\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a helpful assistant. Use ONLY the context to answer.\\n\\n\"\n",
        "    \"Context:\\n{context}\\n\\n\"\n",
        "    \"Question: {question}\\n\\n\"\n",
        "    \"If the answer is not in the context, say you don't know.\\n\"\n",
        "    \"At the end, list citations as [source#chunk_id] for each chunk you used.\"\n",
        ")\n",
        "\n",
        "def format_docs(docs):\n",
        "    # Include metadata so the model can cite properly\n",
        "    lines = []\n",
        "    for d in docs:\n",
        "        src = d.metadata.get(\"source\", \"unknown\")\n",
        "        cid = d.metadata.get(\"chunk_id\", \"na\")\n",
        "        lines.append(f\"[{src}#{cid}] {d.page_content}\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"Ask questions (type 'exit' to quit):\")\n",
        "while True:\n",
        "    q = input(\"Q: \").strip()\n",
        "    if q.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"👋 Goodbye!\")\n",
        "        break\n",
        "    print(\"A:\", rag_chain.invoke(q), \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA-elhjGPWyP",
        "outputId": "9520cf95-8836-4e56-99a5-de770588955d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask questions (type 'exit' to quit):\n",
            "Q: who is Virat Kohli\n",
            "A: Virat Kohli is renowned for batting consistency across formats. [source#2] \n",
            "\n",
            "Q: who is Rohit Sharma\n",
            "A: Rohit Sharma is known for explosive batting and captaincy in limited-overs cricket. [source#2] \n",
            "\n",
            "Q: who is MSD\n",
            "A: MSD refers to Mahendra Singh Dhoni, who captained India and won the 2007 T20 World Cup and the 2011 ODI World Cup. \n",
            "\n",
            "Citations: [notes.txt#0] \n",
            "\n",
            "Q: who is SRT\n",
            "A: SRT refers to Sachin Ramesh Tendulkar, who is widely called the 'God of Cricket' for his batting records. [source#0] \n",
            "\n",
            "Q: who is Big B\n",
            "A: Big B is Amitabh Bachchan, a legendary actor in Indian cinema. \n",
            "\n",
            "Citations: [notes.txt#1] \n",
            "\n",
            "Q: Who is Rahul Dravid\n",
            "A: I don't know. \n",
            "\n",
            "Citations: None \n",
            "\n",
            "Q: exit\n",
            "👋 Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 7 - Persistence (Save/Load FAISS)"
      ],
      "metadata": {
        "id": "uiqVLLoHPixz"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# FAISS persistence\n",
        "# ================================\n",
        "\n",
        "# 0) Install\n",
        "!pip install -q langchain-openai langchain-community faiss-cpu\n",
        "\n",
        "# 1) Imports & API key\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from google.colab import userdata\n",
        "import os, sys\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    print(\"❌ OPENAI_API_KEY not found.\\n\"\n",
        "          \"Set it once with:\\n\"\n",
        "          \"from google.colab import userdata\\n\"\n",
        "          \"userdata.set('OPENAI_API_KEY', 'sk-...')\\n\")\n",
        "    sys.exit(1)\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# 2) Small corpus\n",
        "texts = [\n",
        "    \"MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\",\n",
        "    \"MSD won multiple world cups for India\"\n",
        "    \"MSD is the first captain ever to win a T20 world cup\"\n",
        "    \"SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\",\n",
        "    \"Amitabh Bachchan is nicknamed Big B in Indian cinema.\",\n",
        "    \"Shah Rukh Khan is popularly called King Khan.\",\n",
        "    \"Virat Kohli is known for batting consistency across formats.\",\n",
        "    \"Rohit Sharma is known for explosive batting in limited-overs.\"\n",
        "]\n",
        "\n",
        "# 3) Build vector store (embeddings + FAISS)\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vs = FAISS.from_texts(texts, embedding=emb)\n",
        "print(\"✅ Built FAISS index with\", len(texts), \"texts.\")\n",
        "\n",
        "# 4) Save to disk (persistence)\n",
        "save_dir = \"/content/my_faiss_plain\"\n",
        "vs.save_local(save_dir)\n",
        "print(\"💾 Saved index to:\", save_dir)\n",
        "\n",
        "# 5) Load it back (later or in a new runtime)\n",
        "vs2 = FAISS.load_local(save_dir, emb, allow_dangerous_deserialization=True)\n",
        "print(\"🔁 Reloaded index. Size =\", len(vs2.index_to_docstore_id))\n",
        "\n",
        "# 6) Plain similarity search (no MMR)\n",
        "retriever_plain = vs2.as_retriever(search_kwargs={\"k\": 3})  # default: top-k similarity\n",
        "\n",
        "def search(query: str, k: int = 3):\n",
        "    docs = retriever_plain.get_relevant_documents(query)\n",
        "    print(f\"\\n🔎 Top-{k} results for: {query!r}\")\n",
        "    for i, d in enumerate(docs[:k], 1):\n",
        "        print(f\"{i}. {d.page_content}\")\n",
        "\n",
        "# 7) Try a couple of queries\n",
        "search(\"Who is King Khan?\", k=3)\n",
        "search(\"Who is MSD?\", k=3)\n",
        "search(\"Who is called the God of Cricket?\", k=3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3fD7fPPRoCM",
        "outputId": "739d4748-eb28-48b6-8a3c-7db19570bffb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Built FAISS index with 6 texts.\n",
            "💾 Saved index to: /content/my_faiss_plain\n",
            "🔁 Reloaded index. Size = 6\n",
            "\n",
            "🔎 Top-3 results for: 'Who is King Khan?'\n",
            "1. Shah Rukh Khan is popularly called King Khan.\n",
            "2. Amitabh Bachchan is nicknamed Big B in Indian cinema.\n",
            "3. Virat Kohli is known for batting consistency across formats.\n",
            "\n",
            "🔎 Top-3 results for: 'Who is MSD?'\n",
            "1. MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\n",
            "2. MSD won multiple world cups for IndiaMSD is the first captain ever to win a T20 world cupSRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\n",
            "3. Rohit Sharma is known for explosive batting in limited-overs.\n",
            "\n",
            "🔎 Top-3 results for: 'Who is called the God of Cricket?'\n",
            "1. MSD won multiple world cups for IndiaMSD is the first captain ever to win a T20 world cupSRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\n",
            "2. MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\n",
            "3. Rohit Sharma is known for explosive batting in limited-overs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Incorrect answers as cosine similarity is not implemented."
      ],
      "metadata": {
        "id": "lqo-2NedRn_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install (if not already) ---\n",
        "!pip install -q langchain-openai langchain-community faiss-cpu\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Suppose you already have a FAISS `vs` from the previous cell.\n",
        "# If not, quickly rebuild from a tiny set:\n",
        "docs = [\n",
        "    \"MSD won the 2011 ODI World Cup as captain.\",\n",
        "    \"SRT is called the God of Cricket.\",\n",
        "    \"King Khan refers to Shah Rukh Khan.\"\n",
        "]\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vs = FAISS.from_texts(docs, embedding=emb)\n",
        "\n",
        "# --- Save ---\n",
        "save_dir = \"/content/my_faiss_rag\"\n",
        "vs.save_local(save_dir)\n",
        "print(\"💾 Saved to:\", save_dir)\n",
        "\n",
        "# --- Load in a new session (or just to test) ---\n",
        "vs2 = FAISS.load_local(save_dir, emb, allow_dangerous_deserialization=True)\n",
        "print(\"🔁 Reloaded. Size =\", len(vs2.index_to_docstore_id))\n",
        "\n",
        "# --- Turn on MMR for diverse top-k ---\n",
        "retriever_mmr = vs2.as_retriever(\n",
        "    search_kwargs={\n",
        "        \"k\": 3,        # results to return\n",
        "        \"fetch_k\": 20, # candidates to consider\n",
        "        \"lambda_mult\": 0.5  # 0→diversity, 1→similarity; try 0.2–0.7\n",
        "    },\n",
        "    search_type=\"mmr\"\n",
        ")\n",
        "\n",
        "# Quick test query\n",
        "ctx_docs = retriever_mmr.get_relevant_documents(\"Tell me about World Cups\")\n",
        "for i, d in enumerate(ctx_docs, 1):\n",
        "    print(f\"{i}.\", d.page_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxr9BXV3Qk4k",
        "outputId": "2d434f97-6a6a-43cd-d0e8-23421324ddcd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved to: /content/my_faiss_rag\n",
            "🔁 Reloaded. Size = 3\n",
            "1. MSD won the 2011 ODI World Cup as captain.\n",
            "2. King Khan refers to Shah Rukh Khan.\n",
            "3. SRT is called the God of Cricket.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4o2OgWAUBol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 8 - RAG Implementation - supporting PDF / URL / TXT with a simple switch.\n",
        "\n",
        "\"\"\"\n",
        "It:\n",
        "\n",
        "loads the document(s)\n",
        "\n",
        "chunks the text\n",
        "\n",
        "builds a FAISS index (cosine similarity)\n",
        "\n",
        "runs a small RAG loop with citations\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "E5hk8wrPQlR0",
        "outputId": "74dd2b02-09f6-4539-92d7-c74d13e5f594"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIt:\\n\\nloads the document(s)\\n\\nchunks the text\\n\\nbuilds a FAISS index (cosine similarity)\\n\\nruns a small RAG loop with citations \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# RAG over PDF / URL / TXT (Colab-ready)\n",
        "# =========================================\n",
        "\n",
        "# 0) Install\n",
        "!pip install -q langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf bs4\n",
        "\n",
        "# 1) Imports & API key\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.schema import Document\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# 2) === Choose your source here ===\n",
        "SOURCE_TYPE = \"txt\"   # \"pdf\" | \"url\" | \"txt\"\n",
        "\n",
        "# Provide ONE of these depending on SOURCE_TYPE\n",
        "PDF_PATH = \"/content/Databricks-Big-Book-Of-GenAI-FINAL.pdf\"                       # e.g., upload via Colab sidebar, then set path\n",
        "URLS = [\"https://docs.databricks.com/aws/en/machine-learning/\"]  # example URL list\n",
        "TXT_PATH = \"/content/notes.rtf\"\n",
        "                     # plain text file\n",
        "\n",
        "# 3) Load documents\n",
        "def load_docs():\n",
        "    if SOURCE_TYPE == \"pdf\":\n",
        "        loader = PyPDFLoader(PDF_PATH)\n",
        "        return loader.load()\n",
        "    elif SOURCE_TYPE == \"url\":\n",
        "        loader = WebBaseLoader(URLS)\n",
        "        return loader.load()\n",
        "    elif SOURCE_TYPE == \"txt\":\n",
        "        loader = TextLoader(TXT_PATH, encoding=\"utf-8\")\n",
        "        return loader.load()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported SOURCE_TYPE. Use 'pdf', 'url', or 'txt'.\")\n",
        "\n",
        "docs = load_docs()\n",
        "print(f\"Loaded {len(docs)} document chunks (pre-split).\")\n",
        "\n",
        "# 4) Split into chunks (tune if needed)\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
        "splits = splitter.split_documents(docs)\n",
        "print(f\"Split into {len(splits)} chunks.\")\n",
        "\n",
        "# 5) Build vector store with cosine similarity\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vs = FAISS.from_documents(\n",
        "    splits, embedding=emb, distance_strategy=DistanceStrategy.COSINE\n",
        ")\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
        "print(\"Vector store ready.\")\n",
        "\n",
        "# 6) Prompt with “use context only” + citations\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a helpful assistant. Use ONLY the context to answer.\\n\\n\"\n",
        "    \"Context:\\n{context}\\n\\n\"\n",
        "    \"Question: {question}\\n\\n\"\n",
        "    \"If the answer is not in the context, say you don't know.\\n\"\n",
        "    \"At the end, list citations as [source#page_or_id].\"\n",
        ")\n",
        "\n",
        "def format_docs(docs):\n",
        "    lines = []\n",
        "    for d in docs:\n",
        "        src = d.metadata.get(\"source\") or d.metadata.get(\"url\") or \"unknown\"\n",
        "        page = d.metadata.get(\"page\", d.metadata.get(\"chunk_id\", \"na\"))\n",
        "        lines.append(f\"[{src}#{page}] {d.page_content}\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "# 7) LLM + chain\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 8) (Optional) Save for reuse later\n",
        "SAVE_DIR = \"/content/my_faiss_index_docs\"\n",
        "vs.save_local(SAVE_DIR)\n",
        "print(\"Saved FAISS index to:\", SAVE_DIR)\n",
        "\n",
        "# 9) Q&A loop\n",
        "print(\"\\n💬 Ask about your document(s). Type 'exit' to quit.\")\n",
        "while True:\n",
        "    q = input(\"Q: \").strip()\n",
        "    if q.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"👋 Goodbye!\")\n",
        "        break\n",
        "    ans = rag_chain.invoke(q)\n",
        "    print(\"A:\", ans, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgGa1PryUrj8",
        "outputId": "092758a5-1411-4af8-8188-3db1ab7d59b1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1 document chunks (pre-split).\n",
            "Split into 59 chunks.\n",
            "Vector store ready.\n",
            "Saved FAISS index to: /content/my_faiss_index_docs\n",
            "\n",
            "💬 Ask about your document(s). Type 'exit' to quit.\n",
            "Q: what is gen ai\n",
            "A: Generative AI (GenAI) is defined as using models (often deep neural networks) to create new content—such as text, images, code, etc.—that closely mimics human-generated data. \n",
            "\n",
            "[source#na] \n",
            "\n",
            "Q: what is ai\n",
            "A: AI, or Artificial Intelligence, is defined as the overarching concept of machines that can perform tasks that typically require human intelligence, such as reasoning, decision-making, and perception. [source#na] \n",
            "\n",
            "Q: who is SRT\n",
            "A: I don't know. \n",
            "\n",
            "Citations: [14], [15]. \n",
            "\n",
            "Q: exit\n",
            "👋 Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# End of the Notebook"
      ],
      "metadata": {
        "id": "iD9IS52tcjQP"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build RAG Pipeline application via Gradio - UI."
      ],
      "metadata": {
        "id": "qDzMGB_QjatS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 9 - Gradio Implementation"
      ],
      "metadata": {
        "id": "sYgqReRvhV-X"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Gradio UI for a Tiny RAG over PDF / URL / TXT / Raw text\n",
        "# Colab-ready\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q gradio langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf bs4\n",
        "\n",
        "import os, json, tempfile, textwrap\n",
        "import gradio as gr\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader\n",
        "from langchain.schema import Document\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# API key (expects you've set it with userdata.set(...) once)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Helpers\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def _format_docs(docs):\n",
        "    \"\"\"Format retrieved docs with simple citations.\"\"\"\n",
        "    lines = []\n",
        "    for d in docs:\n",
        "        src = d.metadata.get(\"source\") or d.metadata.get(\"url\") or \"unknown\"\n",
        "        page = d.metadata.get(\"page\", d.metadata.get(\"chunk_id\", \"na\"))\n",
        "        lines.append(f\"[{src}#{page}] {d.page_content}\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "def _citations(docs):\n",
        "    \"\"\"Return a compact citations string like [source#page], deduped.\"\"\"\n",
        "    tags = []\n",
        "    seen = set()\n",
        "    for d in docs:\n",
        "        src = d.metadata.get(\"source\") or d.metadata.get(\"url\") or \"unknown\"\n",
        "        page = d.metadata.get(\"page\", d.metadata.get(\"chunk_id\", \"na\"))\n",
        "        tag = f\"[{src}#{page}]\"\n",
        "        if tag not in seen:\n",
        "            tags.append(tag)\n",
        "            seen.add(tag)\n",
        "    return \" \".join(tags) if tags else \"(no citations)\"\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Build index: load → chunk → embed → FAISS (cosine)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def build_index(\n",
        "    source_type,\n",
        "    files,        # list of temp files from gradio for \"Files\"\n",
        "    urls_text,    # str for \"URL(s)\" separated by newline/comma\n",
        "    raw_text,     # str for \"Raw Text\"\n",
        "    chunk_size,\n",
        "    chunk_overlap,\n",
        "    k,\n",
        "    model_name,\n",
        "    temperature\n",
        "):\n",
        "    try:\n",
        "        # 1) Load docs\n",
        "        docs = []\n",
        "        if source_type == \"Files (PDF/TXT)\":\n",
        "            if not files:\n",
        "                return gr.update(value=\"Please upload at least one file.\"), None, None, None\n",
        "            for f in files:\n",
        "                # Decide by extension\n",
        "                name = f.name if hasattr(f, \"name\") else str(f)\n",
        "                if name.lower().endswith(\".pdf\"):\n",
        "                    loader = PyPDFLoader(name)\n",
        "                    docs.extend(loader.load())\n",
        "                else:\n",
        "                    loader = TextLoader(name, encoding=\"utf-8\")\n",
        "                    docs.extend(loader.load())\n",
        "\n",
        "        elif source_type == \"URL(s)\":\n",
        "            if not urls_text.strip():\n",
        "                return gr.update(value=\"Please enter at least one URL.\"), None, None, None\n",
        "            # split on newline or comma\n",
        "            raw_urls = [u.strip() for u in urls_text.replace(\",\", \"\\n\").split(\"\\n\") if u.strip()]\n",
        "            loader = WebBaseLoader(raw_urls)\n",
        "            docs = loader.load()\n",
        "\n",
        "        elif source_type == \"Raw Text\":\n",
        "            if not raw_text.strip():\n",
        "                return gr.update(value=\"Please paste some text.\"), None, None, None\n",
        "            docs = [Document(page_content=raw_text, metadata={\"source\": \"raw.txt\"})]\n",
        "\n",
        "        else:\n",
        "            return gr.update(value=\"Unsupported source type.\"), None, None, None\n",
        "\n",
        "        if not docs:\n",
        "            return gr.update(value=\"No text found after loading.\"), None, None, None\n",
        "\n",
        "        # 2) Chunk\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=int(chunk_size), chunk_overlap=int(chunk_overlap)\n",
        "        )\n",
        "        splits = splitter.split_documents(docs)\n",
        "\n",
        "        # 3) Embeddings + FAISS (cosine)\n",
        "        emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "        vs = FAISS.from_documents(splits, embedding=emb, distance_strategy=DistanceStrategy.COSINE)\n",
        "        retriever = vs.as_retriever(search_kwargs={\"k\": int(k)})\n",
        "\n",
        "        # 4) LLM (GPT model)\n",
        "        llm = ChatOpenAI(model=model_name, temperature=float(temperature))\n",
        "\n",
        "        status = (\n",
        "            f\"✅ Index built.\\n\"\n",
        "            f\"- Chunks: {len(splits)}\\n\"\n",
        "            f\"- k: {k}\\n\"\n",
        "            f\"- LLM: {model_name} (temp={temperature})\\n\"\n",
        "            f\"- Similarity: COSINE (FAISS)\"\n",
        "        )\n",
        "        return status, vs, retriever, llm\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error while building index: {e}\", None, None, None\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Answer a question from the retriever context\n",
        "# ────────────────────────────────────────────────────────────\n",
        "def ask_question(chat_history, question, retriever, llm):\n",
        "    if retriever is None or llm is None:\n",
        "        chat_history = chat_history or []\n",
        "        chat_history.append((\"\",\n",
        "            \"Please build the index first (choose a source and click 'Build Index').\"\n",
        "        ))\n",
        "        return chat_history\n",
        "\n",
        "    try:\n",
        "        # Retrieve\n",
        "        ctx_docs = retriever.get_relevant_documents(question)\n",
        "        context = _format_docs(ctx_docs)\n",
        "\n",
        "        # Prompt (simple & explicit grounding)\n",
        "        messages = [\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a helpful assistant. Use ONLY the provided context to answer. \"\n",
        "                        \"If the answer is not present, say you don't know.\"},\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"}\n",
        "        ]\n",
        "        resp = llm.invoke(messages).content\n",
        "\n",
        "        # Append citations at the end\n",
        "        cites = _citations(ctx_docs)\n",
        "        final = f\"{resp}\\n\\n**Citations:** {cites}\"\n",
        "\n",
        "        chat_history = chat_history or []\n",
        "        chat_history.append((question, final))\n",
        "        return chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        chat_history = chat_history or []\n",
        "        chat_history.append((question, f\"❌ Error: {e}\"))\n",
        "        return chat_history\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# Gradio UI\n",
        "# ────────────────────────────────────────────────────────────\n",
        "with gr.Blocks(title=\"Tiny RAG UI (FAISS + OpenAI)\") as app:\n",
        "    gr.Markdown(\"## 🔎 Tiny RAG — Ask questions over your PDF/URL/TXT/Raw text\")\n",
        "\n",
        "    with gr.Accordion(\"1) Build Index\", open=True):\n",
        "        source_type = gr.Radio(\n",
        "            [\"Files (PDF/TXT)\", \"URL(s)\", \"Raw Text\"],\n",
        "            value=\"Files (PDF/TXT)\",\n",
        "            label=\"Source type\"\n",
        "        )\n",
        "        files = gr.File(label=\"Upload PDF/TXT files\", file_types=[\".pdf\", \".txt\"], file_count=\"multiple\", visible=True)\n",
        "        urls_text = gr.Textbox(label=\"Enter URL(s) (comma- or newline‑separated)\", visible=False, lines=3)\n",
        "        raw_text = gr.Textbox(label=\"Paste Raw Text\", visible=False, lines=6, placeholder=\"Paste your text here...\")\n",
        "\n",
        "        with gr.Row():\n",
        "            chunk_size = gr.Number(label=\"Chunk size\", value=800)\n",
        "            chunk_overlap = gr.Number(label=\"Chunk overlap\", value=120)\n",
        "            k = gr.Slider(1, 10, value=3, step=1, label=\"Top‑k retrieved\")\n",
        "\n",
        "        with gr.Row():\n",
        "            model_name = gr.Textbox(label=\"GPT model\", value=\"gpt-4o-mini\")\n",
        "            temperature = gr.Slider(0.0, 1.0, value=0.2, step=0.05, label=\"Temperature\")\n",
        "\n",
        "        build_btn = gr.Button(\"🔨 Build Index\")\n",
        "        status = gr.Markdown(\"Status will appear here.\")\n",
        "\n",
        "    with gr.Accordion(\"2) Ask Questions\", open=True):\n",
        "        chat = gr.Chatbot(height=320, label=\"Chat\")\n",
        "        question = gr.Textbox(label=\"Your question\")\n",
        "        ask_btn = gr.Button(\"Ask\")\n",
        "\n",
        "    # App state\n",
        "    vs_state = gr.State(None)\n",
        "    retriever_state = gr.State(None)\n",
        "    llm_state = gr.State(None)\n",
        "\n",
        "    # Source type toggling\n",
        "    def toggle_inputs(src):\n",
        "        return (\n",
        "            gr.update(visible=(src == \"Files (PDF/TXT)\")),\n",
        "            gr.update(visible=(src == \"URL(s)\")),\n",
        "            gr.update(visible=(src == \"Raw Text\")),\n",
        "        )\n",
        "\n",
        "    source_type.change(\n",
        "        toggle_inputs, inputs=[source_type], outputs=[files, urls_text, raw_text]\n",
        "    )\n",
        "\n",
        "    # Build index\n",
        "    build_btn.click(\n",
        "        build_index,\n",
        "        inputs=[source_type, files, urls_text, raw_text, chunk_size, chunk_overlap, k, model_name, temperature],\n",
        "        outputs=[status, vs_state, retriever_state, llm_state]\n",
        "    )\n",
        "\n",
        "    # Ask\n",
        "    ask_btn.click(\n",
        "        ask_question,\n",
        "        inputs=[chat, question, retriever_state, llm_state],\n",
        "        outputs=[chat]\n",
        "    )\n",
        "\n",
        "# Launch (share=True gives a public URL from Colab)\n",
        "app.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "tNrzEPEXiNg3",
        "outputId": "49b31958-46da-4716-b461-5f9f8fd15a33"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-53-3473171679.py:192: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chat = gr.Chatbot(height=320, label=\"Chat\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://71e4b9d1b852e5164b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://71e4b9d1b852e5164b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbQrTboMiOFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}