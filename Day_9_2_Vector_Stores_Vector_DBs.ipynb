{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1akQ2DCzx3w"
      },
      "outputs": [],
      "source": [
        "# Welcome to Langchain Deep Dive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectors Srore - FAISS, Vector DB - Chroma DB , PineCone etc"
      ],
      "metadata": {
        "id": "JwWK-8HG2DYj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"plug in a GPT model and do a tiny, clean RAG (Retrieval‚ÄëAugmented Generation): retrieve the most relevant snippets from your FAISS index and ask the model to answer using only that context.\n",
        "\n",
        "Below is a single Colab‚Äëready code block that:\n",
        "\n",
        "builds a small FAISS index (you can paste your own text),\n",
        "\n",
        "wraps it as a retriever,\n",
        "\n",
        "formats retrieved context,\n",
        "\n",
        "calls a GPT model to answer from that context,\n",
        "\n",
        "runs an interactive Q&A loop.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "What this gives us:\n",
        "\n",
        "Embeddings + FAISS turn your reference text into a searchable index.\n",
        "\n",
        "A retriever fetches the top-k relevant chunks for each question.\n",
        "\n",
        "A prompt instructs the model to answer only from the retrieved context, avoiding hallucinations.\n",
        "\n",
        "A tiny interactive loop to ask multiple questions.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "vZM_NHAcHYKX",
        "outputId": "463e70d5-c61a-40fc-dfa7-0fc6d1521737"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWhat this gives us:\\n\\nEmbeddings + FAISS turn your reference text into a searchable index.\\n\\nA retriever fetches the top-k relevant chunks for each question.\\n\\nA prompt instructs the model to answer only from the retrieved context, avoiding hallucinations.\\n\\nA tiny interactive loop to ask multiple questions.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Chunking + Metadata + Citations"
      ],
      "metadata": {
        "id": "T_v-S8M3EHVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install ---\n",
        "!pip install -q langchain-openai langchain-community langchain-text-splitters faiss-cpu\n",
        "\n",
        "# --- Imports & API key ---\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from google.colab import userdata\n",
        "from langchain.schema import Document\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# --- 1) Source text (replace with your own) ---\n",
        "raw_text = \"\"\"\n",
        "Mahendra Singh Dhoni (MSD) captained India and won the 2007 T20 World Cup and the 2011 ODI World Cup.\n",
        "Sachin Ramesh Tendulkar (SRT) is widely called the 'God of Cricket' for his batting records.\n",
        "Amitabh Bachchan, nicknamed Big B, is a legendary actor in Indian cinema.\n",
        "Shah Rukh Khan, often called King Khan, is one of the most popular Bollywood actors worldwide.\n",
        "Virat Kohli is renowned for batting consistency across formats.\n",
        "Rohit Sharma is known for explosive batting and captaincy in limited-overs cricket.\n",
        "\"\"\"\n",
        "\n",
        "# --- 2) Chunk with metadata (source + chunk_id) ---\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=280, chunk_overlap=80)\n",
        "chunks = splitter.split_text(raw_text)\n",
        "docs = [\n",
        "    Document(page_content=chunk, metadata={\"source\": \"notes.txt\", \"chunk_id\": i})\n",
        "    for i, chunk in enumerate(chunks)\n",
        "]\n",
        "\n",
        "# --- 3) Build vector store + retriever ---\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vs = FAISS.from_documents(docs, embedding=emb)\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# --- 4) Prompt that asks model to cite chunks used ---\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a helpful assistant. Use ONLY the context to answer.\\n\\n\"\n",
        "    \"Context:\\n{context}\\n\\n\"\n",
        "    \"Question: {question}\\n\\n\"\n",
        "    \"If the answer is not in the context, say you don't know.\\n\"\n",
        "    \"At the end, list citations as [source#chunk_id] for each chunk you used.\"\n",
        ")\n",
        "\n",
        "def format_docs(docs):\n",
        "    # Include metadata so the model can cite properly\n",
        "    lines = []\n",
        "    for d in docs:\n",
        "        src = d.metadata.get(\"source\", \"unknown\")\n",
        "        cid = d.metadata.get(\"chunk_id\", \"na\")\n",
        "        lines.append(f\"[{src}#{cid}] {d.page_content}\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"Ask questions (type 'exit' to quit):\")\n",
        "while True:\n",
        "    q = input(\"Q: \").strip()\n",
        "    if q.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"üëã Goodbye!\")\n",
        "        break\n",
        "    print(\"A:\", rag_chain.invoke(q), \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA-elhjGPWyP",
        "outputId": "db70e675-0baf-468e-d333-9dfba1f86d7c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask questions (type 'exit' to quit):\n",
            "Q: who is Virat Kohli\n",
            "A: Virat Kohli is renowned for batting consistency across formats. [source#2] \n",
            "\n",
            "Q: who is SRT\n",
            "A: SRT refers to Sachin Ramesh Tendulkar, who is widely called the 'God of Cricket' for his batting records. \n",
            "\n",
            "Citations: [notes.txt#0] \n",
            "\n",
            "Q: WHO IS srk\n",
            "A: SRK refers to Shah Rukh Khan, who is often called King Khan and is one of the most popular Bollywood actors worldwide. \n",
            "\n",
            "Citations: [notes.txt#1] \n",
            "\n",
            "Q: who is virendra sehwag\n",
            "A: I don't know. \n",
            "\n",
            "Citations: [notes.txt#2] \n",
            "\n",
            "Q: exit\n",
            "üëã Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Persistence (Save/Load FAISS)"
      ],
      "metadata": {
        "id": "uiqVLLoHPixz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# FAISS persistence\n",
        "# ================================\n",
        "\n",
        "# 0) Install\n",
        "!pip install -q langchain-openai langchain-community faiss-cpu\n",
        "\n",
        "# 1) Imports & API key\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from google.colab import userdata\n",
        "import os, sys\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    print(\"‚ùå OPENAI_API_KEY not found.\\n\"\n",
        "          \"Set it once with:\\n\"\n",
        "          \"from google.colab import userdata\\n\"\n",
        "          \"userdata.set('OPENAI_API_KEY', 'sk-...')\\n\")\n",
        "    sys.exit(1)\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# 2) Small corpus\n",
        "texts = [\n",
        "    \"MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\",\n",
        "    \"MSD won multiple world cups for India\"\n",
        "    \"MSD is the first captain ever to win a T20 world cup\"\n",
        "    \"SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\",\n",
        "    \"Amitabh Bachchan is nicknamed Big B in Indian cinema.\",\n",
        "    \"Shah Rukh Khan is popularly called King Khan.\",\n",
        "    \"Virat Kohli is known for batting consistency across formats.\",\n",
        "    \"Rohit Sharma is known for explosive batting in limited-overs.\"\n",
        "]\n",
        "\n",
        "# 3) Build vector store (embeddings + FAISS)\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vs = FAISS.from_texts(texts, embedding=emb)\n",
        "print(\"‚úÖ Built FAISS index with\", len(texts), \"texts.\")\n",
        "\n",
        "# 4) Save to disk (persistence)\n",
        "save_dir = \"/content/my_faiss_plain\"\n",
        "vs.save_local(save_dir)\n",
        "print(\"üíæ Saved index to:\", save_dir)\n",
        "\n",
        "# 5) Load it back (later or in a new runtime)\n",
        "vs2 = FAISS.load_local(save_dir, emb, allow_dangerous_deserialization=True)\n",
        "print(\"üîÅ Reloaded index. Size =\", len(vs2.index_to_docstore_id))\n",
        "\n",
        "# 6) Plain similarity search (no MMR)\n",
        "retriever_plain = vs2.as_retriever(search_kwargs={\"k\": 3})  # default: top-k similarity\n",
        "\n",
        "def search(query: str, k: int = 3):\n",
        "    docs = retriever_plain.get_relevant_documents(query)\n",
        "    print(f\"\\nüîé Top-{k} results for: {query!r}\")\n",
        "    for i, d in enumerate(docs[:k], 1):\n",
        "        print(f\"{i}. {d.page_content}\")\n",
        "\n",
        "# 7) Try a couple of queries\n",
        "search(\"Who is King Khan?\", k=3)\n",
        "search(\"Who is MSD?\", k=3)\n",
        "search(\"Who is called the God of Cricket?\", k=3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3fD7fPPRoCM",
        "outputId": "e01e7233-46bd-43a8-d0e9-3cb6e8ab8a19"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Built FAISS index with 6 texts.\n",
            "üíæ Saved index to: /content/my_faiss_plain\n",
            "üîÅ Reloaded index. Size = 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1382641590.py:53: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever_plain.get_relevant_documents(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîé Top-3 results for: 'Who is King Khan?'\n",
            "1. Shah Rukh Khan is popularly called King Khan.\n",
            "2. Amitabh Bachchan is nicknamed Big B in Indian cinema.\n",
            "3. Virat Kohli is known for batting consistency across formats.\n",
            "\n",
            "üîé Top-3 results for: 'Who is MSD?'\n",
            "1. MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\n",
            "2. MSD won multiple world cups for IndiaMSD is the first captain ever to win a T20 world cupSRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\n",
            "3. Rohit Sharma is known for explosive batting in limited-overs.\n",
            "\n",
            "üîé Top-3 results for: 'Who is called the God of Cricket?'\n",
            "1. MSD won multiple world cups for IndiaMSD is the first captain ever to win a T20 world cupSRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\n",
            "2. MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\n",
            "3. Virat Kohli is known for batting consistency across formats.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - RAG Implementation - supporting PDF / URL / TXT with a simple switch.\n",
        "\n",
        "\"\"\"\n",
        "It:\n",
        "\n",
        "loads the document(s)\n",
        "\n",
        "chunks the text\n",
        "\n",
        "builds a FAISS index (cosine similarity)\n",
        "\n",
        "runs a small RAG loop with citations\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "E5hk8wrPQlR0",
        "outputId": "5d535cef-ca6b-47b3-b51d-e9ca76ce53b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIt:\\n\\nloads the document(s)\\n\\nchunks the text\\n\\nbuilds a FAISS index (cosine similarity)\\n\\nruns a small RAG loop with citations\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# RAG over PDF / URL / TXT\n",
        "# =========================================\n",
        "\n",
        "# 0) Install\n",
        "!pip install -q langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf bs4\n",
        "\n",
        "# 1) Imports & API key\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.schema import Document\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# 2) === Choose your source here ===\n",
        "#SOURCE_TYPE = \"txt\"   # \"pdf\" | \"url\" | \"txt\"\n",
        "SOURCE_TYPE = \"url\"\n",
        "#SOURCE_TYPE = \"pdf\"\n",
        "\n",
        "# Provide ONE of these depending on SOURCE_TYPE\n",
        "PDF_PATH = \"/content/Databricks-Big-Book-Of-GenAI-FINAL.pdf\"                       # e.g., upload via Colab sidebar, then set path\n",
        "URLS = [\"https://docs.databricks.com/aws/en/machine-learning/\"]  # example URL list\n",
        "TXT_PATH = \"/content/notes.txt\"\n",
        "                     # plain text file\n",
        "\n",
        "# 3) Load documents\n",
        "def load_docs():\n",
        "    if SOURCE_TYPE == \"pdf\":\n",
        "        loader = PyPDFLoader(PDF_PATH)\n",
        "        return loader.load()\n",
        "    elif SOURCE_TYPE == \"url\":\n",
        "        loader = WebBaseLoader(URLS)\n",
        "        return loader.load()\n",
        "    elif SOURCE_TYPE == \"txt\":\n",
        "        loader = TextLoader(TXT_PATH, encoding=\"utf-8\")\n",
        "        return loader.load()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported SOURCE_TYPE. Use 'pdf', 'url', or 'txt'.\")\n",
        "\n",
        "docs = load_docs()\n",
        "print(f\"Loaded {len(docs)} document chunks (pre-split).\")\n",
        "\n",
        "# 4) Split into chunks (tune if needed)\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
        "splits = splitter.split_documents(docs)\n",
        "print(f\"Split into {len(splits)} chunks.\")\n",
        "\n",
        "# 5) Build vector store with cosine similarity\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vs = FAISS.from_documents(\n",
        "    splits, embedding=emb, distance_strategy=DistanceStrategy.COSINE\n",
        ")\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
        "print(\"Vector store ready.\")\n",
        "\n",
        "# 6) Prompt with ‚Äúuse context only‚Äù + citations\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a helpful assistant. Use ONLY the context to answer.\\n\\n\"\n",
        "    \"Context:\\n{context}\\n\\n\"\n",
        "    \"Question: {question}\\n\\n\"\n",
        "    \"If the answer is not in the context, say you don't know.\\n\"\n",
        "    \"At the end, list citations as [source#page_or_id].\"\n",
        ")\n",
        "\n",
        "def format_docs(docs):\n",
        "    lines = []\n",
        "    for d in docs:\n",
        "        src = d.metadata.get(\"source\") or d.metadata.get(\"url\") or \"unknown\"\n",
        "        page = d.metadata.get(\"page\", d.metadata.get(\"chunk_id\", \"na\"))\n",
        "        lines.append(f\"[{src}#{page}] {d.page_content}\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "# 7) LLM + chain\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 8) (Optional) Save for reuse later\n",
        "SAVE_DIR = \"/content/my_faiss_index_docs\"\n",
        "vs.save_local(SAVE_DIR)\n",
        "print(\"Saved FAISS index to:\", SAVE_DIR)\n",
        "\n",
        "# 9) Q&A loop\n",
        "print(\"\\nüí¨ Ask about your document(s). Type 'exit' to quit.\")\n",
        "while True:\n",
        "    q = input(\"Q: \").strip()\n",
        "    if q.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"üëã Goodbye!\")\n",
        "        break\n",
        "    ans = rag_chain.invoke(q)\n",
        "    print(\"A:\", ans, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgGa1PryUrj8",
        "outputId": "fbc4c979-fd99-428e-b7da-ae21c249e0fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 118 document chunks (pre-split).\n",
            "Split into 291 chunks.\n",
            "Vector store ready.\n",
            "Saved FAISS index to: /content/my_faiss_index_docs\n",
            "\n",
            "üí¨ Ask about your document(s). Type 'exit' to quit.\n",
            "Q: what is dbrx\n",
            "A: DBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token prediction. It employs a fine-grained mixture-of-experts (MoE) architecture with 132 billion total parameters, of which 36 billion parameters are active on any input. DBRX was pre-trained on 12 trillion tokens of text and code data. It features 16 experts and selects 4, providing 65 times more possible combinations of experts compared to other models like Mixtral and Grok-1. Additionally, DBRX utilizes rotary position encodings (RoPE), gated linear units (GLU), and grouped query attention (GQA) [source#6]. \n",
            "\n",
            "Q: what is databricks\n",
            "A: Databricks is the data and AI company that provides a platform for unifying and democratizing data, analytics, and AI. It is relied upon by more than 10,000 organizations worldwide, including notable companies like Comcast, Cond√© Nast, and Grammarly. Databricks was founded by the original creators of Lakehouse, Apache Spark‚Ñ¢, Delta Lake, and MLflow, and is headquartered in San Francisco with offices globally. The platform is used to build high-quality generative AI applications and offers features for governance and other enterprise needs [source#117]. \n",
            "\n",
            "Q: what is databricks and Gen AI and DBRX\n",
            "A: Databricks is a company that provides a unified platform for data analytics and machine learning. In the context of Generative AI (GenAI), Databricks focuses on enabling enterprises to control their data and leverage advanced AI capabilities. DBRX is a specific generative AI model developed by Databricks, trained on powerful hardware and designed for a wide range of applications. It represents a continuation of Databricks' efforts in building large language models (LLMs) and is part of their next generation of GenAI products.\n",
            "\n",
            "Citations: [source#16], [source#17] \n",
            "\n",
            "Q: exit\n",
            "üëã Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG over PDF/URL/TXT using Chroma DB (Vector Database)"
      ],
      "metadata": {
        "id": "f4BaU3xR4HeR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# RAG over PDF / URL / TXT using Chroma DB\n",
        "# =========================================\n",
        "\n",
        "# 0) Install dependencies\n",
        "!pip install -q langchain-openai langchain-community langchain-text-splitters chromadb pypdf bs4\n",
        "\n",
        "# 1) Imports & API Key\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.schema import Document\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# 2) === Choose your source here ===\n",
        "# SOURCE_TYPE = \"txt\"   # \"pdf\" | \"url\" | \"txt\"\n",
        "SOURCE_TYPE = \"url\"\n",
        "#SOURCE_TYPE = \"pdf\"\n",
        "\n",
        "# Provide ONE of these depending on SOURCE_TYPE\n",
        "PDF_PATH = \"/content/Databricks-Big-Book-Of-GenAI-FINAL.pdf\"\n",
        "URLS = [\"https://docs.databricks.com/aws/en/machine-learning/\"]\n",
        "TXT_PATH = \"/content/notes.txt\"\n",
        "\n",
        "# 3) Load documents\n",
        "def load_docs():\n",
        "    if SOURCE_TYPE == \"pdf\":\n",
        "        loader = PyPDFLoader(PDF_PATH)\n",
        "        return loader.load()\n",
        "    elif SOURCE_TYPE == \"url\":\n",
        "        loader = WebBaseLoader(URLS)\n",
        "        return loader.load()\n",
        "    elif SOURCE_TYPE == \"txt\":\n",
        "        loader = TextLoader(TXT_PATH, encoding=\"utf-8\")\n",
        "        return loader.load()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported SOURCE_TYPE. Use 'pdf', 'url', or 'txt'.\")\n",
        "\n",
        "docs = load_docs()\n",
        "print(f\"‚úÖ Loaded {len(docs)} document chunks (pre-split).\")\n",
        "\n",
        "# 4) Split into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
        "splits = splitter.split_documents(docs)\n",
        "print(f\"‚úÖ Split into {len(splits)} chunks.\")\n",
        "\n",
        "# 5) Create vector store with Chroma\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "SAVE_DIR = \"/content/my_chroma_index_docs\"\n",
        "\n",
        "vs = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=emb,\n",
        "    persist_directory=SAVE_DIR\n",
        ")\n",
        "vs.persist()\n",
        "\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
        "print(\"‚úÖ Chroma DB ready at:\", SAVE_DIR)\n",
        "\n",
        "# 6) Prompt with ‚Äúuse context only‚Äù + citations\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a helpful assistant. Use ONLY the context to answer.\\n\\n\"\n",
        "    \"Context:\\n{context}\\n\\n\"\n",
        "    \"Question: {question}\\n\\n\"\n",
        "    \"If the answer is not in the context, say you don't know.\\n\"\n",
        "    \"At the end, list citations as [source#page_or_id].\"\n",
        ")\n",
        "\n",
        "def format_docs(docs):\n",
        "    lines = []\n",
        "    for d in docs:\n",
        "        src = d.metadata.get(\"source\") or d.metadata.get(\"url\") or \"unknown\"\n",
        "        page = d.metadata.get(\"page\", d.metadata.get(\"chunk_id\", \"na\"))\n",
        "        lines.append(f\"[{src}#{page}] {d.page_content}\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "# 7) LLM + chain\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 8) Q&A loop\n",
        "print(\"\\nüí¨ Ask about your document(s). Type 'exit' to quit.\")\n",
        "while True:\n",
        "    q = input(\"Q: \").strip()\n",
        "    if q.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"üëã Goodbye!\")\n",
        "        break\n",
        "    ans = rag_chain.invoke(q)\n",
        "    print(\"A:\", ans, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_vt1gms4Hbs",
        "outputId": "d22f8e52-92e7-4501-9cf3-4d6bc2e7d8a3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 1 document chunks (pre-split).\n",
            "‚úÖ Split into 9 chunks.\n",
            "‚úÖ Chroma DB ready at: /content/my_chroma_index_docs\n",
            "\n",
            "üí¨ Ask about your document(s). Type 'exit' to quit.\n",
            "Q: what are foundation models\n",
            "A: Foundation models commonly refer to large language models that have been trained over extensive datasets to be generally good at some tasks, such as chat, instruction following, and code generation. They serve as the base for layers of increasingly complex techniques in the development of production-quality GenAI applications. Foundation models generally fall under two categories: proprietary models (like GPT-3.5 and Gemini) and open source models (like Llama2-70B and DBRX) [source#4]. \n",
            "\n",
            "Q: what is databricks\n",
            "A: Databricks is the data and AI company that provides a platform for unifying and democratizing data, analytics, and AI. It is relied upon by over 10,000 organizations worldwide, including major companies like Comcast, Cond√© Nast, and Grammarly, as well as over 50% of the Fortune 500. Databricks was founded by the original creators of Lakehouse, Apache Spark‚Ñ¢, Delta Lake, and MLflow, and is headquartered in San Francisco with offices globally. \n",
            "\n",
            "[source#117] \n",
            "\n",
            "Q: what is DBRX\n",
            "A: DBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token prediction. It utilizes a fine-grained mixture-of-experts (MoE) architecture with 132 billion total parameters, of which 36 billion parameters are active on any input. DBRX was pre-trained on 12 trillion tokens of text and code data and features 16 experts, choosing 4, which allows for 65 times more possible combinations of experts compared to other models like Mixtral and Grok-1. It employs rotary position encodings (RoPE), gated linear units (GLU), and grouped query attention (GQA) in its architecture [source#6]. \n",
            "\n",
            "Q: who is Sandip Nagpure\n",
            "A: I don't know. \n",
            "\n",
            "Citations: [source#23] \n",
            "\n",
            "Q: exit\n",
            "üëã Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG over PDF/URL/TXT using Pinecone DB (Cloud Hosted Managed Production Grade - Vector Database)"
      ],
      "metadata": {
        "id": "A9jSGfCJ4kut"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# End of the Notebook"
      ],
      "metadata": {
        "id": "iD9IS52tcjQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build RAG Pipeline application via Gradio - UI."
      ],
      "metadata": {
        "id": "qDzMGB_QjatS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example - Gradio Implementation"
      ],
      "metadata": {
        "id": "sYgqReRvhV-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Gradio UI for a Tiny RAG over PDF / URL / TXT / Raw text\n",
        "# Colab-ready\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q gradio langchain-openai langchain-community langchain-text-splitters faiss-cpu pypdf bs4\n",
        "\n",
        "import os, json, tempfile, textwrap\n",
        "import gradio as gr\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader\n",
        "from langchain.schema import Document\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# API key (expects you've set it with userdata.set(...) once)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Helpers\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def _format_docs(docs):\n",
        "    \"\"\"Format retrieved docs with simple citations.\"\"\"\n",
        "    lines = []\n",
        "    for d in docs:\n",
        "        src = d.metadata.get(\"source\") or d.metadata.get(\"url\") or \"unknown\"\n",
        "        page = d.metadata.get(\"page\", d.metadata.get(\"chunk_id\", \"na\"))\n",
        "        lines.append(f\"[{src}#{page}] {d.page_content}\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "def _citations(docs):\n",
        "    \"\"\"Return a compact citations string like [source#page], deduped.\"\"\"\n",
        "    tags = []\n",
        "    seen = set()\n",
        "    for d in docs:\n",
        "        src = d.metadata.get(\"source\") or d.metadata.get(\"url\") or \"unknown\"\n",
        "        page = d.metadata.get(\"page\", d.metadata.get(\"chunk_id\", \"na\"))\n",
        "        tag = f\"[{src}#{page}]\"\n",
        "        if tag not in seen:\n",
        "            tags.append(tag)\n",
        "            seen.add(tag)\n",
        "    return \" \".join(tags) if tags else \"(no citations)\"\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Build index: load ‚Üí chunk ‚Üí embed ‚Üí FAISS (cosine)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def build_index(\n",
        "    source_type,\n",
        "    files,        # list of temp files from gradio for \"Files\"\n",
        "    urls_text,    # str for \"URL(s)\" separated by newline/comma\n",
        "    raw_text,     # str for \"Raw Text\"\n",
        "    chunk_size,\n",
        "    chunk_overlap,\n",
        "    k,\n",
        "    model_name,\n",
        "    temperature\n",
        "):\n",
        "    try:\n",
        "        # 1) Load docs\n",
        "        docs = []\n",
        "        if source_type == \"Files (PDF/TXT)\":\n",
        "            if not files:\n",
        "                return gr.update(value=\"Please upload at least one file.\"), None, None, None\n",
        "            for f in files:\n",
        "                # Decide by extension\n",
        "                name = f.name if hasattr(f, \"name\") else str(f)\n",
        "                if name.lower().endswith(\".pdf\"):\n",
        "                    loader = PyPDFLoader(name)\n",
        "                    docs.extend(loader.load())\n",
        "                else:\n",
        "                    loader = TextLoader(name, encoding=\"utf-8\")\n",
        "                    docs.extend(loader.load())\n",
        "\n",
        "        elif source_type == \"URL(s)\":\n",
        "            if not urls_text.strip():\n",
        "                return gr.update(value=\"Please enter at least one URL.\"), None, None, None\n",
        "            # split on newline or comma\n",
        "            raw_urls = [u.strip() for u in urls_text.replace(\",\", \"\\n\").split(\"\\n\") if u.strip()]\n",
        "            loader = WebBaseLoader(raw_urls)\n",
        "            docs = loader.load()\n",
        "\n",
        "        elif source_type == \"Raw Text\":\n",
        "            if not raw_text.strip():\n",
        "                return gr.update(value=\"Please paste some text.\"), None, None, None\n",
        "            docs = [Document(page_content=raw_text, metadata={\"source\": \"raw.txt\"})]\n",
        "\n",
        "        else:\n",
        "            return gr.update(value=\"Unsupported source type.\"), None, None, None\n",
        "\n",
        "        if not docs:\n",
        "            return gr.update(value=\"No text found after loading.\"), None, None, None\n",
        "\n",
        "        # 2) Chunk\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=int(chunk_size), chunk_overlap=int(chunk_overlap)\n",
        "        )\n",
        "        splits = splitter.split_documents(docs)\n",
        "\n",
        "        # 3) Embeddings + FAISS (cosine)\n",
        "        emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "        vs = FAISS.from_documents(splits, embedding=emb, distance_strategy=DistanceStrategy.COSINE)\n",
        "        retriever = vs.as_retriever(search_kwargs={\"k\": int(k)})\n",
        "\n",
        "        # 4) LLM (GPT model)\n",
        "        llm = ChatOpenAI(model=model_name, temperature=float(temperature))\n",
        "\n",
        "        status = (\n",
        "            f\"‚úÖ Index built.\\n\"\n",
        "            f\"- Chunks: {len(splits)}\\n\"\n",
        "            f\"- k: {k}\\n\"\n",
        "            f\"- LLM: {model_name} (temp={temperature})\\n\"\n",
        "            f\"- Similarity: COSINE (FAISS)\"\n",
        "        )\n",
        "        return status, vs, retriever, llm\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error while building index: {e}\", None, None, None\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Answer a question from the retriever context\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def ask_question(chat_history, question, retriever, llm):\n",
        "    if retriever is None or llm is None:\n",
        "        chat_history = chat_history or []\n",
        "        chat_history.append((\"\",\n",
        "            \"Please build the index first (choose a source and click 'Build Index').\"\n",
        "        ))\n",
        "        return chat_history\n",
        "\n",
        "    try:\n",
        "        # Retrieve\n",
        "        ctx_docs = retriever.get_relevant_documents(question)\n",
        "        context = _format_docs(ctx_docs)\n",
        "\n",
        "        # Prompt (simple & explicit grounding)\n",
        "        messages = [\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a helpful assistant. Use ONLY the provided context to answer. \"\n",
        "                        \"If the answer is not present, say you don't know.\"},\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"}\n",
        "        ]\n",
        "        resp = llm.invoke(messages).content\n",
        "\n",
        "        # Append citations at the end\n",
        "        cites = _citations(ctx_docs)\n",
        "        final = f\"{resp}\\n\\n**Citations:** {cites}\"\n",
        "\n",
        "        chat_history = chat_history or []\n",
        "        chat_history.append((question, final))\n",
        "        return chat_history\n",
        "\n",
        "    except Exception as e:\n",
        "        chat_history = chat_history or []\n",
        "        chat_history.append((question, f\"‚ùå Error: {e}\"))\n",
        "        return chat_history\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Gradio UI\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "with gr.Blocks(title=\"Tiny RAG UI (FAISS + OpenAI)\") as app:\n",
        "    gr.Markdown(\"## üîé Tiny RAG ‚Äî Ask questions over your PDF/URL/TXT/Raw text\")\n",
        "\n",
        "    with gr.Accordion(\"1) Build Index\", open=True):\n",
        "        source_type = gr.Radio(\n",
        "            [\"Files (PDF/TXT)\", \"URL(s)\", \"Raw Text\"],\n",
        "            value=\"Files (PDF/TXT)\",\n",
        "            label=\"Source type\"\n",
        "        )\n",
        "        files = gr.File(label=\"Upload PDF/TXT files\", file_types=[\".pdf\", \".txt\"], file_count=\"multiple\", visible=True)\n",
        "        urls_text = gr.Textbox(label=\"Enter URL(s) (comma- or newline‚Äëseparated)\", visible=False, lines=3)\n",
        "        raw_text = gr.Textbox(label=\"Paste Raw Text\", visible=False, lines=6, placeholder=\"Paste your text here...\")\n",
        "\n",
        "        with gr.Row():\n",
        "            chunk_size = gr.Number(label=\"Chunk size\", value=800)\n",
        "            chunk_overlap = gr.Number(label=\"Chunk overlap\", value=120)\n",
        "            k = gr.Slider(1, 10, value=3, step=1, label=\"Top‚Äëk retrieved\")\n",
        "\n",
        "        with gr.Row():\n",
        "            model_name = gr.Textbox(label=\"GPT model\", value=\"gpt-4o-mini\")\n",
        "            temperature = gr.Slider(0.0, 1.0, value=0.2, step=0.05, label=\"Temperature\")\n",
        "\n",
        "        build_btn = gr.Button(\"üî® Build Index\")\n",
        "        status = gr.Markdown(\"Status will appear here.\")\n",
        "\n",
        "    with gr.Accordion(\"2) Ask Questions\", open=True):\n",
        "        chat = gr.Chatbot(height=320, label=\"Chat\")\n",
        "        question = gr.Textbox(label=\"Your question\")\n",
        "        ask_btn = gr.Button(\"Ask\")\n",
        "\n",
        "    # App state\n",
        "    vs_state = gr.State(None)\n",
        "    retriever_state = gr.State(None)\n",
        "    llm_state = gr.State(None)\n",
        "\n",
        "    # Source type toggling\n",
        "    def toggle_inputs(src):\n",
        "        return (\n",
        "            gr.update(visible=(src == \"Files (PDF/TXT)\")),\n",
        "            gr.update(visible=(src == \"URL(s)\")),\n",
        "            gr.update(visible=(src == \"Raw Text\")),\n",
        "        )\n",
        "\n",
        "    source_type.change(\n",
        "        toggle_inputs, inputs=[source_type], outputs=[files, urls_text, raw_text]\n",
        "    )\n",
        "\n",
        "    # Build index\n",
        "    build_btn.click(\n",
        "        build_index,\n",
        "        inputs=[source_type, files, urls_text, raw_text, chunk_size, chunk_overlap, k, model_name, temperature],\n",
        "        outputs=[status, vs_state, retriever_state, llm_state]\n",
        "    )\n",
        "\n",
        "    # Ask\n",
        "    ask_btn.click(\n",
        "        ask_question,\n",
        "        inputs=[chat, question, retriever_state, llm_state],\n",
        "        outputs=[chat]\n",
        "    )\n",
        "\n",
        "# Launch (share=True gives a public URL from Colab)\n",
        "app.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "tNrzEPEXiNg3",
        "outputId": "f48b9395-b0ae-4d8e-ac80-93bb0eded099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "/tmp/ipython-input-2-3473171679.py:192: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chat = gr.Chatbot(height=320, label=\"Chat\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a6c40ec6302667a6da.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a6c40ec6302667a6da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# End of the notebook"
      ],
      "metadata": {
        "id": "nbQrTboMiOFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VCR39iTZkTl5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}