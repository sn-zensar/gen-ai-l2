{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Basic RAG Based Application"
      ],
      "metadata": {
        "id": "D2V9oiacA-K-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# End to End RAG Based Application -\n",
        "    # Input Data (External Knowledge Base)\n",
        "    # Splitting - Text Splitters\n",
        "    # Vector Stores / Vector DBs\n",
        "    # Integrate LLMs\n",
        "    # Q&A System\n",
        "    # UI"
      ],
      "metadata": {
        "id": "I3kMTIOvBDUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Generate External Knowledge (as challenges in uploading pdf/docs etc)"
      ],
      "metadata": {
        "id": "gMOPCZdeB0ca"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt to LLM =>\n",
        "# Lets build an end to end RAG based application which will start from Text splitters and will end on building Gradio UI. So for starting to build this application - generate some sample text (external knowledge) - 100 - lines on Gen-AI and Langchain topic and store it in a variable.\n"
      ],
      "metadata": {
        "id": "OwWKqVCtEWbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sample text about Gen-AI and Langchain\n",
        "sample_text = \"\"\"\n",
        "Generative AI, or Gen-AI, is a type of artificial intelligence that can create new content, such as text, images, audio, and video. Unlike traditional AI that focuses on analysis and prediction, Gen-AI is about creation and innovation. Large Language Models (LLMs) are a prominent example of Gen-AI, trained on vast amounts of text data to understand and generate human-like language.\n",
        "\n",
        "Langchain is a framework designed to simplify the development of applications using large language models. It provides a structured way to chain together different components, such as LLMs, data sources, and other tools, to build more complex and powerful applications. Langchain's modular design allows developers to easily swap out components and experiment with different configurations.\n",
        "\n",
        "One of the core concepts in Langchain is the \"chain,\" which represents a sequence of operations performed by different components. For example, a simple chain might involve taking user input, passing it to an LLM to generate a response, and then formatting the response before presenting it to the user. More complex chains can involve retrieving information from external knowledge bases, performing calculations, or interacting with other APIs.\n",
        "\n",
        "Retrieval Augmented Generation (RAG) is a technique that combines the power of LLMs with the ability to retrieve relevant information from external sources. In a RAG system, the LLM's generation process is augmented by retrieved information, leading to more accurate, factual, and contextually relevant responses. Langchain provides built-in support for building RAG applications, making it easier to integrate external knowledge into LLM-powered systems.\n",
        "\n",
        "Text splitting is a crucial step in preparing large documents or datasets for use with LLMs and RAG systems. Since LLMs have limitations on the amount of text they can process at once, large texts need to be split into smaller chunks. Various text splitting strategies exist, such as splitting by characters, words, or sentences, and different strategies may be more suitable for different types of data and applications.\n",
        "\n",
        "Vector stores, also known as vector databases, are specialized databases designed to store and search for vector representations of data. In the context of LLMs and RAG, text chunks are often converted into numerical vectors (embeddings) using embedding models. These vectors capture the semantic meaning of the text, allowing for efficient similarity search. When a user query is received, it is also converted into a vector, and the vector store is queried to find the most similar text chunks from the external knowledge base.\n",
        "\n",
        "Integrating LLMs into applications involves selecting an appropriate LLM, configuring its parameters, and handling the input and output. Langchain provides connectors for various LLMs, including popular models from OpenAI, Google, and others. It also offers tools for managing prompts, parsing model outputs, and handling conversational flows.\n",
        "\n",
        "Building a Question Answering (Q&A) system using LLMs and RAG involves several steps:\n",
        "1. Loading the external knowledge base.\n",
        "2. Splitting the knowledge base into smaller chunks.\n",
        "3. Generating embeddings for the text chunks and storing them in a vector store.\n",
        "4. Receiving a user query.\n",
        "5. Generating an embedding for the user query.\n",
        "6. Searching the vector store for relevant text chunks based on the query embedding.\n",
        "7. Passing the retrieved text chunks and the user query to an LLM to generate an answer.\n",
        "8. Presenting the answer to the user.\n",
        "\n",
        "User interfaces (UIs) are essential for interacting with RAG-based applications. Gradio is a popular Python library for building simple and interactive UIs for machine learning models and demos. It allows developers to quickly create web interfaces with input and output components, such as text boxes, image displays, and audio players. Integrating Gradio with Langchain enables the creation of user-friendly Q&A interfaces that allow users to ask questions and receive answers generated by the RAG system.\n",
        "\n",
        "Langchain's expressiveness allows for the creation of complex chains involving multiple steps and components. For instance, a chain might involve retrieving information, summarizing it with an LLM, and then using another LLM to generate a response based on the summary. This flexibility makes Langchain suitable for a wide range of natural language processing tasks.\n",
        "\n",
        "The choice of text splitter can significantly impact the performance of a RAG system. Different splitters have different parameters, such as chunk size and overlap, which can be tuned to optimize the retrieval process. Experimenting with different splitters and parameters is often necessary to find the best approach for a given dataset and application.\n",
        "\n",
        "Embedding models play a crucial role in converting text into meaningful vector representations. The quality of the embeddings directly affects the accuracy of the similarity search in the vector store. Various embedding models are available, each with its strengths and weaknesses. Selecting an appropriate embedding model is an important consideration when building a RAG system.\n",
        "\n",
        "Vector stores provide efficient mechanisms for storing and searching high-dimensional vectors. Different vector stores offer different features and performance characteristics. Some popular vector stores include FAISS, Annoy, Pinecone, and Weaviate. The choice of vector store depends on factors such as the size of the knowledge base, the required search performance, and the desired scalability.\n",
        "\n",
        "Integrating LLMs with external APIs allows for the creation of applications that can interact with real-world services. For example, an LLM-powered application could use an API to retrieve weather information, make a reservation, or send an email. Langchain provides tools for integrating with various APIs, enabling the creation of more powerful and versatile applications.\n",
        "\n",
        "Handling conversational history is important for building engaging and contextually aware Q&A systems. Langchain provides mechanisms for managing conversational memory, allowing the LLM to remember previous turns in the conversation and generate responses that are consistent with the ongoing dialogue.\n",
        "\n",
        "Error handling and robustness are important considerations when building RAG-based applications. Potential issues include errors during text loading, splitting, embedding, retrieval, or LLM generation. Implementing proper error handling mechanisms is essential to ensure the application's stability and reliability.\n",
        "\n",
        "Evaluating the performance of a RAG system involves measuring its ability to retrieve relevant information and generate accurate answers. Various evaluation metrics can be used, such as precision, recall, F1-score, and ROUGE. Evaluating the system's performance on a representative dataset is crucial for identifying areas for improvement.\n",
        "\n",
        "Fine-tuning LLMs on specific datasets can further improve their performance on particular tasks. While pre-trained LLMs are powerful, fine-tuning them on a domain-specific dataset can lead to more accurate and relevant responses in that domain. Langchain can be used to integrate fine-tuned LLMs into RAG systems.\n",
        "\n",
        "Deploying RAG-based applications involves packaging the application components and making them accessible to users. Various deployment options are available, such as deploying the application as a web service, a desktop application, or a mobile application.\n",
        "\n",
        "Security and privacy are important considerations when building RAG-based applications, especially when dealing with sensitive data. Protecting the external knowledge base and ensuring the privacy of user queries are crucial.\n",
        "\n",
        "The field of Gen-AI and LLMs is constantly evolving, with new models and techniques being developed regularly. Staying updated with the latest advancements is important for building cutting-edge RAG-based applications.\n",
        "\n",
        "Langchain's community and documentation are valuable resources for developers building RAG-based applications. The community provides support, shares examples, and contributes to the framework's development. The documentation provides detailed information on Langchain's components and features.\n",
        "\n",
        "Building a successful RAG-based application requires a combination of technical skills, domain expertise, and careful consideration of the application's requirements and constraints.\n",
        "\n",
        "Langchain simplifies the process of building complex LLM applications by providing a structured and modular framework. Its focus on chains and components makes it easier to reason about and manage the different parts of an application.\n",
        "\n",
        "The ability to integrate external knowledge is a key advantage of RAG systems. By leveraging external data sources, RAG systems can overcome the limitations of the LLM's internal knowledge and provide more accurate and up-to-date information.\n",
        "\n",
        "Text splitting strategies can be tailored to the specific characteristics of the text data. For example, splitting by sentences might be suitable for conversational data, while splitting by paragraphs might be more appropriate for documents.\n",
        "\n",
        "Embedding models are trained on different types of data and have varying levels of performance on different tasks. Choosing an embedding model that is well-suited to the domain of the external knowledge base can improve the retrieval accuracy.\n",
        "\n",
        "Vector stores offer different indexing techniques and search algorithms, which can impact the search performance. Selecting a vector store that provides efficient search for the desired scale and dimensionality of the vectors is important.\n",
        "\n",
        "Integrating external APIs can extend the capabilities of RAG-based applications and enable them to interact with the real world. This allows for the creation of applications that can perform actions based on the information retrieved from the external knowledge base.\n",
        "\n",
        "Managing conversational memory is essential for building engaging and natural-sounding Q&A systems. By remembering the conversational history, the LLM can maintain context and generate responses that are relevant to the ongoing dialogue.\n",
        "\n",
        "Implementing proper error handling mechanisms is crucial for ensuring the stability and reliability of RAG-based applications. This involves anticipating potential errors and implementing strategies to handle them gracefully.\n",
        "\n",
        "Evaluating the performance of a RAG system is an iterative process that involves refining the system's components and parameters based on the evaluation results.\n",
        "\n",
        "Fine-tuning LLMs on specific datasets can improve their performance on particular tasks and domains. This can be especially beneficial when dealing with specialized or technical knowledge bases.\n",
        "\n",
        "Deploying RAG-based applications requires careful planning and consideration of the deployment environment and infrastructure.\n",
        "\n",
        "Security and privacy should be considered throughout the development process of RAG-based applications. Implementing appropriate security measures is essential to protect sensitive data and ensure user privacy.\n",
        "\n",
        "Staying updated with the latest advancements in Gen-AI and LLMs is important for building cutting-edge RAG-based applications. The field is rapidly evolving, with new models and techniques being developed regularly.\n",
        "\n",
        "Langchain's community and documentation are valuable resources for developers seeking support and information on building RAG-based applications.\n",
        "\n",
        "Building a successful RAG-based application requires a combination of technical skills, domain expertise, and careful consideration of the application's requirements and constraints.\n",
        "\n",
        "Langchain simplifies the process of building complex LLM applications by providing a structured and modular framework. Its focus on chains and components makes it easier to reason about and manage the different parts of an application.\n",
        "\n",
        "The ability to integrate external knowledge is a key advantage of RAG systems. By leveraging external data sources, RAG systems can overcome the limitations of the LLM's internal knowledge and provide more accurate and up-to-date information.\n",
        "\n",
        "Text splitting strategies can be tailored to the specific characteristics of the text data. For example, splitting by sentences might be suitable for conversational data, while splitting by paragraphs might be more appropriate for documents.\n",
        "\n",
        "Embedding models are trained on different types of data and have varying levels of performance on different tasks. Choosing an embedding model that is well-suited to the domain of the external knowledge base can improve the retrieval accuracy.\n",
        "\n",
        "Vector stores offer different indexing techniques and search algorithms, which can impact the search performance. Selecting a vector store that provides efficient search for the desired scale and dimensionality of the vectors is important.\n",
        "\n",
        "Integrating external APIs can extend the capabilities of RAG-based applications and enable them to interact with the real world. This allows for the creation of applications that can perform actions based on the information retrieved from the external knowledge base.\n",
        "\n",
        "Managing conversational memory is essential for building engaging and natural-sounding Q&A systems. By remembering the conversational history, the LLM can maintain context and generate responses that are relevant to the ongoing dialogue.\n",
        "\n",
        "Implementing proper error handling mechanisms is crucial for ensuring the stability and reliability of RAG-based applications. This involves anticipating potential errors and implementing strategies to handle them gracefully.\n",
        "\n",
        "Evaluating the performance of a RAG system is an iterative process that involves refining the system's components and parameters based on the evaluation results.\n",
        "\n",
        "Fine-tuning LLMs on specific datasets can improve their performance on particular tasks and domains. This can be especially beneficial when dealing with specialized or technical knowledge bases.\n",
        "\n",
        "Deploying RAG-based applications requires careful planning and consideration of the deployment environment and infrastructure.\n",
        "\n",
        "Security and privacy should be considered throughout the development process of RAG-based applications. Implementing appropriate security measures is essential to protect sensitive data and ensure user privacy.\n",
        "\n",
        "Staying updated with the latest advancements in Gen-AI and LLMs is important for building cutting-edge RAG-based applications. The field is rapidly evolving, with new models and techniques being developed regularly.\n",
        "\n",
        "Langchain's community and documentation are valuable resources for developers seeking support and information on building RAG-based applications.\n",
        "\n",
        "Building a successful RAG-based application requires a combination of technical skills, domain expertise, and careful consideration of the application's requirements and constraints.\n",
        "\n",
        "Langchain simplifies the process of building complex LLM applications by providing a structured and modular framework. Its focus on chains and components makes it easier to reason about and manage the different parts of an application.\n",
        "\n",
        "The ability to integrate external knowledge is a key advantage of RAG systems. By leveraging external data sources, RAG systems can overcome the limitations of the LLM's internal knowledge and provide more accurate and up-to-date information.\n",
        "\n",
        "Text splitting strategies can be tailored to the specific characteristics of the text data. For example, splitting by sentences might be suitable for conversational data, while splitting by paragraphs might be more appropriate for documents.\n",
        "\n",
        "Embedding models are trained on different types of data and have varying levels of performance on different tasks. Choosing an embedding model that is well-suited to the domain of the external knowledge base can improve the retrieval accuracy.\n",
        "\n",
        "Vector stores offer different indexing techniques and search algorithms, which can impact the search performance. Selecting a vector store that provides efficient search for the desired scale and dimensionality of the vectors is important.\n",
        "\n",
        "Integrating external APIs can extend the capabilities of RAG-based applications and enable them to interact with the real world. This allows for the creation of applications that can perform actions based on the information retrieved from the external knowledge base.\n",
        "\n",
        "Managing conversational memory is essential for building engaging and natural-sounding Q&A systems. By remembering the conversational history, the LLM can maintain context and generate responses that are relevant to the ongoing dialogue.\n",
        "\n",
        "Implementing proper error handling mechanisms is crucial for ensuring the stability and reliability of RAG-based applications. This involves anticipating potential errors and implementing strategies to handle them gracefully.\n",
        "\n",
        "Evaluating the performance of a RAG system is an iterative process that involves refining the system's components and parameters based on the evaluation results.\n",
        "\n",
        "Fine-tuning LLMs on specific datasets can improve their performance on particular tasks and domains. This can be especially beneficial when dealing with specialized or technical knowledge bases.\n",
        "\n",
        "Deploying RAG-based applications requires careful planning and consideration of the deployment environment and infrastructure.\n",
        "\n",
        "Security and privacy should be considered throughout the development process of RAG-based applications. Implementing appropriate security measures is essential to protect sensitive data and ensure user privacy.\n",
        "\n",
        "Staying updated with the latest advancements in Gen-AI and LLMs is important for building cutting-edge RAG-based applications. The field is rapidly evolving, with new models and techniques being developed regularly.\n",
        "\n",
        "Langchain's community and documentation are valuable resources for developers seeking support and information on building RAG-based applications.\n",
        "\n",
        "Building a successful RAG-based application requires a combination of technical skills, domain expertise, and careful consideration of the application's requirements and constraints.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3FocX0eeDhuj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sample text about Gen-AI and Langchain with topic headers and chapter names\n",
        "sample_text_with_chapters = \"\"\"\n",
        "# Chapter 1: Introduction to Generative AI\n",
        "## What is Gen-AI?\n",
        "Generative AI, or Gen-AI, is a type of artificial intelligence that can create new content, such as text, images, audio, and video. Unlike traditional AI that focuses on analysis and prediction, Gen-AI is about creation and innovation.\n",
        "\n",
        "## Large Language Models (LLMs)\n",
        "Large Language Models (LLMs) are a prominent example of Gen-AI, trained on vast amounts of text data to understand and generate human-like language. They are the backbone of many modern AI applications.\n",
        "\n",
        "# Chapter 2: Understanding Langchain\n",
        "## What is Langchain?\n",
        "Langchain is a framework designed to simplify the development of applications using large language models. It provides a structured way to chain together different components, such as LLMs, data sources, and other tools.\n",
        "\n",
        "## The Concept of Chains\n",
        "One of the core concepts in Langchain is the \"chain,\" which represents a sequence of operations performed by different components. This allows for building complex workflows.\n",
        "\n",
        "# Chapter 3: Retrieval Augmented Generation (RAG)\n",
        "## RAG Explained\n",
        "Retrieval Augmented Generation (RAG) is a technique that combines the power of LLMs with the ability to retrieve relevant information from external sources. This enhances the accuracy and relevance of generated responses.\n",
        "\n",
        "## RAG in Langchain\n",
        "Langchain provides built-in support for building RAG applications, making it easier to integrate external knowledge into LLM-powered systems.\n",
        "\n",
        "# Chapter 4: Data Preparation for RAG\n",
        "## Text Splitting\n",
        "Text splitting is a crucial step in preparing large documents or datasets for use with LLMs and RAG systems. Large texts need to be split into smaller chunks due to LLM limitations.\n",
        "\n",
        "## Text Splitting Strategies\n",
        "Various text splitting strategies exist, such as splitting by characters, words, or sentences, and different strategies may be more suitable for different types of data.\n",
        "\n",
        "# Chapter 5: Vector Stores and Embeddings\n",
        "## Vector Stores\n",
        "Vector stores, also known as vector databases, are specialized databases designed to store and search for vector representations of data.\n",
        "\n",
        "## Embeddings\n",
        "Text chunks are often converted into numerical vectors (embeddings) using embedding models. These vectors capture the semantic meaning of the text, allowing for efficient similarity search.\n",
        "\n",
        "# Chapter 6: Integrating LLMs\n",
        "## Selecting and Configuring LLMs\n",
        "Integrating LLMs into applications involves selecting an appropriate LLM, configuring its parameters, and handling the input and output.\n",
        "\n",
        "## Langchain Connectors\n",
        "Langchain provides connectors for various LLMs, including popular models from OpenAI, Google, and others, simplifying the integration process.\n",
        "\n",
        "# Chapter 7: Building a Q&A System\n",
        "## Steps in Building a Q&A System\n",
        "Building a Question Answering (Q&A) system using LLMs and RAG involves several steps, from loading data to generating answers.\n",
        "\n",
        "## Q&A Workflow\n",
        "The workflow typically includes loading data, splitting text, generating embeddings, storing in a vector store, receiving queries, retrieving relevant chunks, and generating answers with an LLM.\n",
        "\n",
        "# Chapter 8: User Interfaces with Gradio\n",
        "## Importance of UIs\n",
        "User interfaces (UIs) are essential for interacting with RAG-based applications, providing a way for users to input queries and receive responses.\n",
        "\n",
        "## Using Gradio\n",
        "Gradio is a popular Python library for building simple and interactive UIs for machine learning models and demos. It allows quickly creating web interfaces.\n",
        "\n",
        "## Gradio and Langchain\n",
        "Integrating Gradio with Langchain enables the creation of user-friendly Q&A interfaces that allow users to ask questions and receive answers.\n",
        "\n",
        "# Chapter 9: Advanced Langchain Concepts\n",
        "## Expressiveness of Chains\n",
        "Langchain's expressiveness allows for the creation of complex chains involving multiple steps and components, suitable for a wide range of NLP tasks.\n",
        "\n",
        "## Customizing Splitters\n",
        "The choice of text splitter can significantly impact RAG performance. Different splitters have parameters that can be tuned for optimization.\n",
        "\n",
        "# Chapter 10: Embedding Models in Detail\n",
        "## Role of Embedding Models\n",
        "Embedding models play a crucial role in converting text into meaningful vector representations. The quality of embeddings affects similarity search accuracy.\n",
        "\n",
        "## Selecting Embedding Models\n",
        "Various embedding models are available, each with strengths and weaknesses. Selecting an appropriate model for the domain is important.\n",
        "\n",
        "# Chapter 11: Exploring Vector Stores\n",
        "## Vector Store Options\n",
        "Different vector stores offer varying features and performance. Popular options include FAISS, Annoy, Pinecone, and Weaviate.\n",
        "\n",
        "## Choosing a Vector Store\n",
        "The choice of vector store depends on factors like data size, required search performance, and desired scalability.\n",
        "\n",
        "# Chapter 12: Integrating External APIs\n",
        "## Extending Capabilities\n",
        "Integrating LLMs with external APIs allows for creating applications that interact with real-world services, extending their capabilities.\n",
        "\n",
        "## Langchain API Tools\n",
        "Langchain provides tools for integrating with various APIs, enabling the creation of more powerful and versatile applications.\n",
        "\n",
        "# Chapter 13: Conversational Memory\n",
        "## Handling Conversation History\n",
        "Handling conversational history is important for building engaging and contextually aware Q&A systems.\n",
        "\n",
        "## Langchain Memory Mechanisms\n",
        "Langchain provides mechanisms for managing conversational memory, allowing the LLM to remember previous turns and generate consistent responses.\n",
        "\n",
        "# Chapter 14: Error Handling and Robustness\n",
        "## Importance of Error Handling\n",
        "Error handling and robustness are important considerations when building RAG applications to ensure stability and reliability.\n",
        "\n",
        "## Implementing Error Handling\n",
        "Implementing proper error handling mechanisms is essential to anticipate potential errors during different stages of the RAG process.\n",
        "\n",
        "# Chapter 15: Evaluating RAG Systems\n",
        "## Evaluating Performance\n",
        "Evaluating the performance of a RAG system involves measuring its ability to retrieve relevant information and generate accurate answers.\n",
        "\n",
        "## Evaluation Metrics\n",
        "Various evaluation metrics can be used, such as precision, recall, F1-score, and ROUGE, to assess system performance.\n",
        "\n",
        "# Chapter 16: Fine-tuning LLMs\n",
        "## Improving Performance\n",
        "Fine-tuning LLMs on specific datasets can further improve their performance on particular tasks and domains.\n",
        "\n",
        "## Fine-tuning with Langchain\n",
        "Langchain can be used to integrate fine-tuned LLMs into RAG systems, leveraging their specialized knowledge.\n",
        "\n",
        "# Chapter 17: Deploying RAG Applications\n",
        "## Deployment Options\n",
        "Deploying RAG-based applications involves packaging components and making them accessible to users through various options like web services or desktop applications.\n",
        "\n",
        "## Planning Deployment\n",
        "Deploying requires careful planning and consideration of the deployment environment and infrastructure.\n",
        "\n",
        "# Chapter 18: Security and Privacy\n",
        "## Protecting Sensitive Data\n",
        "Security and privacy are important considerations, especially when dealing with sensitive data in RAG applications.\n",
        "\n",
        "## Implementing Security Measures\n",
        "Implementing appropriate security measures is essential to protect the external knowledge base and ensure user privacy.\n",
        "\n",
        "# Chapter 19: Staying Updated\n",
        "## Evolving Field\n",
        "The field of Gen-AI and LLMs is constantly evolving, with new models and techniques regularly developed.\n",
        "\n",
        "## Importance of Staying Updated\n",
        "Staying updated with the latest advancements is important for building cutting-edge RAG applications.\n",
        "\n",
        "# Chapter 20: Resources and Community\n",
        "## Langchain Resources\n",
        "Langchain's community and documentation are valuable resources for developers seeking support and information.\n",
        "\n",
        "## Building Successful Applications\n",
        "Building a successful RAG application requires technical skills, domain expertise, and careful consideration of requirements.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DeJdO2MxFIn9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4-VDsGjF2Rn",
        "outputId": "a97929d1-2c71-497b-94db-7ad42655a990"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generative AI, or Gen-AI, is a type of artificial intelligence that can create new content, such as text, images, audio, and video. Unlike traditional AI that focuses on analysis and prediction, Gen-AI is about creation and innovation. Large Language Models (LLMs) are a prominent example of Gen-AI, trained on vast amounts of text data to understand and generate human-like language.\n",
            "\n",
            "Langchain is a framework designed to simplify the development of applications using large language models. It provides a structured way to chain together different components, such as LLMs, data sources, and other tools, to build more complex and powerful applications. Langchain's modular design allows developers to easily swap out components and experiment with different configurations.\n",
            "\n",
            "One of the core concepts in Langchain is the \"chain,\" which represents a sequence of operations performed by different components. For example, a simple chain might involve taking user input, passing it to an LLM to generate a response, and then formatting the response before presenting it to the user. More complex chains can involve retrieving information from external knowledge bases, performing calculations, or interacting with other APIs.\n",
            "\n",
            "Retrieval Augmented Generation (RAG) is a technique that combines the power of LLMs with the ability to retrieve relevant information from external sources. In a RAG system, the LLM's generation process is augmented by retrieved information, leading to more accurate, factual, and contextually relevant responses. Langchain provides built-in support for building RAG applications, making it easier to integrate external knowledge into LLM-powered systems.\n",
            "\n",
            "Text splitting is a crucial step in preparing large documents or datasets for use with LLMs and RAG systems. Since LLMs have limitations on the amount of text they can process at once, large texts need to be split into smaller chunks. Various text splitting strategies exist, such as splitting by characters, words, or sentences, and different strategies may be more suitable for different types of data and applications.\n",
            "\n",
            "Vector stores, also known as vector databases, are specialized databases designed to store and search for vector representations of data. In the context of LLMs and RAG, text chunks are often converted into numerical vectors (embeddings) using embedding models. These vectors capture the semantic meaning of the text, allowing for efficient similarity search. When a user query is received, it is also converted into a vector, and the vector store is queried to find the most similar text chunks from the external knowledge base.\n",
            "\n",
            "Integrating LLMs into applications involves selecting an appropriate LLM, configuring its parameters, and handling the input and output. Langchain provides connectors for various LLMs, including popular models from OpenAI, Google, and others. It also offers tools for managing prompts, parsing model outputs, and handling conversational flows.\n",
            "\n",
            "Building a Question Answering (Q&A) system using LLMs and RAG involves several steps:\n",
            "1. Loading the external knowledge base.\n",
            "2. Splitting the knowledge base into smaller chunks.\n",
            "3. Generating embeddings for the text chunks and storing them in a vector store.\n",
            "4. Receiving a user query.\n",
            "5. Generating an embedding for the user query.\n",
            "6. Searching the vector store for relevant text chunks based on the query embedding.\n",
            "7. Passing the retrieved text chunks and the user query to an LLM to generate an answer.\n",
            "8. Presenting the answer to the user.\n",
            "\n",
            "User interfaces (UIs) are essential for interacting with RAG-based applications. Gradio is a popular Python library for building simple and interactive UIs for machine learning models and demos. It allows developers to quickly create web interfaces with input and output components, such as text boxes, image displays, and audio players. Integrating Gradio with Langchain enables the creation of user-friendly Q&A interfaces that allow users to ask questions and receive answers generated by the RAG system.\n",
            "\n",
            "Langchain's expressiveness allows for the creation of complex chains involving multiple steps and components. For instance, a chain might involve retrieving information, summarizing it with an LLM, and then using another LLM to generate a response based on the summary. This flexibility makes Langchain suitable for a wide range of natural language processing tasks.\n",
            "\n",
            "The choice of text splitter can significantly impact the performance of a RAG system. Different splitters have different parameters, such as chunk size and overlap, which can be tuned to optimize the retrieval process. Experimenting with different splitters and parameters is often necessary to find the best approach for a given dataset and application.\n",
            "\n",
            "Embedding models play a crucial role in converting text into meaningful vector representations. The quality of the embeddings directly affects the accuracy of the similarity search in the vector store. Various embedding models are available, each with its strengths and weaknesses. Selecting an appropriate embedding model is an important consideration when building a RAG system.\n",
            "\n",
            "Vector stores provide efficient mechanisms for storing and searching high-dimensional vectors. Different vector stores offer different features and performance characteristics. Some popular vector stores include FAISS, Annoy, Pinecone, and Weaviate. The choice of vector store depends on factors such as the size of the knowledge base, the required search performance, and the desired scalability.\n",
            "\n",
            "Integrating LLMs with external APIs allows for the creation of applications that can interact with real-world services. For example, an LLM-powered application could use an API to retrieve weather information, make a reservation, or send an email. Langchain provides tools for integrating with various APIs, enabling the creation of more powerful and versatile applications.\n",
            "\n",
            "Handling conversational history is important for building engaging and contextually aware Q&A systems. Langchain provides mechanisms for managing conversational memory, allowing the LLM to remember previous turns in the conversation and generate responses that are consistent with the ongoing dialogue.\n",
            "\n",
            "Error handling and robustness are important considerations when building RAG-based applications. Potential issues include errors during text loading, splitting, embedding, retrieval, or LLM generation. Implementing proper error handling mechanisms is essential to ensure the application's stability and reliability.\n",
            "\n",
            "Evaluating the performance of a RAG system involves measuring its ability to retrieve relevant information and generate accurate answers. Various evaluation metrics can be used, such as precision, recall, F1-score, and ROUGE. Evaluating the system's performance on a representative dataset is crucial for identifying areas for improvement.\n",
            "\n",
            "Fine-tuning LLMs on specific datasets can further improve their performance on particular tasks. While pre-trained LLMs are powerful, fine-tuning them on a domain-specific dataset can lead to more accurate and relevant responses in that domain. Langchain can be used to integrate fine-tuned LLMs into RAG systems.\n",
            "\n",
            "Deploying RAG-based applications involves packaging the application components and making them accessible to users. Various deployment options are available, such as deploying the application as a web service, a desktop application, or a mobile application.\n",
            "\n",
            "Security and privacy are important considerations when building RAG-based applications, especially when dealing with sensitive data. Protecting the external knowledge base and ensuring the privacy of user queries are crucial.\n",
            "\n",
            "The field of Gen-AI and LLMs is constantly evolving, with new models and techniques being developed regularly. Staying updated with the latest advancements is important for building cutting-edge RAG-based applications.\n",
            "\n",
            "Langchain's community and documentation are valuable resources for developers building RAG-based applications. The community provides support, shares examples, and contributes to the framework's development. The documentation provides detailed information on Langchain's components and features.\n",
            "\n",
            "Building a successful RAG-based application requires a combination of technical skills, domain expertise, and careful consideration of the application's requirements and constraints.\n",
            "\n",
            "Langchain simplifies the process of building complex LLM applications by providing a structured and modular framework. Its focus on chains and components makes it easier to reason about and manage the different parts of an application.\n",
            "\n",
            "The ability to integrate external knowledge is a key advantage of RAG systems. By leveraging external data sources, RAG systems can overcome the limitations of the LLM's internal knowledge and provide more accurate and up-to-date information.\n",
            "\n",
            "Text splitting strategies can be tailored to the specific characteristics of the text data. For example, splitting by sentences might be suitable for conversational data, while splitting by paragraphs might be more appropriate for documents.\n",
            "\n",
            "Embedding models are trained on different types of data and have varying levels of performance on different tasks. Choosing an embedding model that is well-suited to the domain of the external knowledge base can improve the retrieval accuracy.\n",
            "\n",
            "Vector stores offer different indexing techniques and search algorithms, which can impact the search performance. Selecting a vector store that provides efficient search for the desired scale and dimensionality of the vectors is important.\n",
            "\n",
            "Integrating external APIs can extend the capabilities of RAG-based applications and enable them to interact with the real world. This allows for the creation of applications that can perform actions based on the information retrieved from the external knowledge base.\n",
            "\n",
            "Managing conversational memory is essential for building engaging and natural-sounding Q&A systems. By remembering the conversational history, the LLM can maintain context and generate responses that are relevant to the ongoing dialogue.\n",
            "\n",
            "Implementing proper error handling mechanisms is crucial for ensuring the stability and reliability of RAG-based applications. This involves anticipating potential errors and implementing strategies to handle them gracefully.\n",
            "\n",
            "Evaluating the performance of a RAG system is an iterative process that involves refining the system's components and parameters based on the evaluation results.\n",
            "\n",
            "Fine-tuning LLMs on specific datasets can improve their performance on particular tasks and domains. This can be especially beneficial when dealing with specialized or technical knowledge bases.\n",
            "\n",
            "Deploying RAG-based applications requires careful planning and consideration of the deployment environment and infrastructure.\n",
            "\n",
            "Security and privacy should be considered throughout the development process of RAG-based applications. Implementing appropriate security measures is essential to protect sensitive data and ensure user privacy.\n",
            "\n",
            "Staying updated with the latest advancements in Gen-AI and LLMs is important for building cutting-edge RAG-based applications. The field is rapidly evolving, with new models and techniques being developed regularly.\n",
            "\n",
            "Langchain's community and documentation are valuable resources for developers seeking support and information on building RAG-based applications.\n",
            "\n",
            "Building a successful RAG-based application requires a combination of technical skills, domain expertise, and careful consideration of the application's requirements and constraints.\n",
            "\n",
            "Langchain simplifies the process of building complex LLM applications by providing a structured and modular framework. Its focus on chains and components makes it easier to reason about and manage the different parts of an application.\n",
            "\n",
            "The ability to integrate external knowledge is a key advantage of RAG systems. By leveraging external data sources, RAG systems can overcome the limitations of the LLM's internal knowledge and provide more accurate and up-to-date information.\n",
            "\n",
            "Text splitting strategies can be tailored to the specific characteristics of the text data. For example, splitting by sentences might be suitable for conversational data, while splitting by paragraphs might be more appropriate for documents.\n",
            "\n",
            "Embedding models are trained on different types of data and have varying levels of performance on different tasks. Choosing an embedding model that is well-suited to the domain of the external knowledge base can improve the retrieval accuracy.\n",
            "\n",
            "Vector stores offer different indexing techniques and search algorithms, which can impact the search performance. Selecting a vector store that provides efficient search for the desired scale and dimensionality of the vectors is important.\n",
            "\n",
            "Integrating external APIs can extend the capabilities of RAG-based applications and enable them to interact with the real world. This allows for the creation of applications that can perform actions based on the information retrieved from the external knowledge base.\n",
            "\n",
            "Managing conversational memory is essential for building engaging and natural-sounding Q&A systems. By remembering the conversational history, the LLM can maintain context and generate responses that are relevant to the ongoing dialogue.\n",
            "\n",
            "Implementing proper error handling mechanisms is crucial for ensuring the stability and reliability of RAG-based applications. This involves anticipating potential errors and implementing strategies to handle them gracefully.\n",
            "\n",
            "Evaluating the performance of a RAG system is an iterative process that involves refining the system's components and parameters based on the evaluation results.\n",
            "\n",
            "Fine-tuning LLMs on specific datasets can improve their performance on particular tasks and domains. This can be especially beneficial when dealing with specialized or technical knowledge bases.\n",
            "\n",
            "Deploying RAG-based applications requires careful planning and consideration of the deployment environment and infrastructure.\n",
            "\n",
            "Security and privacy should be considered throughout the development process of RAG-based applications. Implementing appropriate security measures is essential to protect sensitive data and ensure user privacy.\n",
            "\n",
            "Staying updated with the latest advancements in Gen-AI and LLMs is important for building cutting-edge RAG-based applications. The field is rapidly evolving, with new models and techniques being developed regularly.\n",
            "\n",
            "Langchain's community and documentation are valuable resources for developers seeking support and information on building RAG-based applications.\n",
            "\n",
            "Building a successful RAG-based application requires a combination of technical skills, domain expertise, and careful consideration of the application's requirements and constraints.\n",
            "\n",
            "Langchain simplifies the process of building complex LLM applications by providing a structured and modular framework. Its focus on chains and components makes it easier to reason about and manage the different parts of an application.\n",
            "\n",
            "The ability to integrate external knowledge is a key advantage of RAG systems. By leveraging external data sources, RAG systems can overcome the limitations of the LLM's internal knowledge and provide more accurate and up-to-date information.\n",
            "\n",
            "Text splitting strategies can be tailored to the specific characteristics of the text data. For example, splitting by sentences might be suitable for conversational data, while splitting by paragraphs might be more appropriate for documents.\n",
            "\n",
            "Embedding models are trained on different types of data and have varying levels of performance on different tasks. Choosing an embedding model that is well-suited to the domain of the external knowledge base can improve the retrieval accuracy.\n",
            "\n",
            "Vector stores offer different indexing techniques and search algorithms, which can impact the search performance. Selecting a vector store that provides efficient search for the desired scale and dimensionality of the vectors is important.\n",
            "\n",
            "Integrating external APIs can extend the capabilities of RAG-based applications and enable them to interact with the real world. This allows for the creation of applications that can perform actions based on the information retrieved from the external knowledge base.\n",
            "\n",
            "Managing conversational memory is essential for building engaging and natural-sounding Q&A systems. By remembering the conversational history, the LLM can maintain context and generate responses that are relevant to the ongoing dialogue.\n",
            "\n",
            "Implementing proper error handling mechanisms is crucial for ensuring the stability and reliability of RAG-based applications. This involves anticipating potential errors and implementing strategies to handle them gracefully.\n",
            "\n",
            "Evaluating the performance of a RAG system is an iterative process that involves refining the system's components and parameters based on the evaluation results.\n",
            "\n",
            "Fine-tuning LLMs on specific datasets can improve their performance on particular tasks and domains. This can be especially beneficial when dealing with specialized or technical knowledge bases.\n",
            "\n",
            "Deploying RAG-based applications requires careful planning and consideration of the deployment environment and infrastructure.\n",
            "\n",
            "Security and privacy should be considered throughout the development process of RAG-based applications. Implementing appropriate security measures is essential to protect sensitive data and ensure user privacy.\n",
            "\n",
            "Staying updated with the latest advancements in Gen-AI and LLMs is important for building cutting-edge RAG-based applications. The field is rapidly evolving, with new models and techniques being developed regularly.\n",
            "\n",
            "Langchain's community and documentation are valuable resources for developers seeking support and information on building RAG-based applications.\n",
            "\n",
            "Building a successful RAG-based application requires a combination of technical skills, domain expertise, and careful consideration of the application's requirements and constraints.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 => Create Chunks of External Knowledge by using Text Splitters\n",
        "\n",
        "# Prompt => Split the above 'sample_text' using a basic character splitter from text splitter class in Langchain and display few chunks."
      ],
      "metadata": {
        "id": "3CZto_1rGpA9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Initialize the CharacterTextSplitter\n",
        "# chunk_size and chunk_overlap can be adjusted based on your needs\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "# Split the sample_text\n",
        "texts = text_splitter.create_documents([sample_text])\n",
        "\n",
        "# Display a few chunks\n",
        "print(type(sample_text))\n",
        "print(type(text_splitter))\n",
        "print(type(texts))\n",
        "\n",
        "print(f\"Number of chunks: {len(texts)}\")\n",
        "print(\"First 5 chunks:\")\n",
        "for i, text in enumerate(texts[:5]):\n",
        "    print(f\"--- Chunk {i+1} ---\")\n",
        "    print(text.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJvkqcVhGAmQ",
        "outputId": "5b5ea8e5-2e1b-414b-f91d-b191c9231749"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "<class 'langchain_text_splitters.character.CharacterTextSplitter'>\n",
            "<class 'list'>\n",
            "Number of chunks: 21\n",
            "First 5 chunks:\n",
            "--- Chunk 1 ---\n",
            "Generative AI, or Gen-AI, is a type of artificial intelligence that can create new content, such as text, images, audio, and video. Unlike traditional AI that focuses on analysis and prediction, Gen-AI is about creation and innovation. Large Language Models (LLMs) are a prominent example of Gen-AI, trained on vast amounts of text data to understand and generate human-like language.\n",
            "\n",
            "Langchain is a framework designed to simplify the development of applications using large language models. It provides a structured way to chain together different components, such as LLMs, data sources, and other tools, to build more complex and powerful applications. Langchain's modular design allows developers to easily swap out components and experiment with different configurations.\n",
            "--- Chunk 2 ---\n",
            "One of the core concepts in Langchain is the \"chain,\" which represents a sequence of operations performed by different components. For example, a simple chain might involve taking user input, passing it to an LLM to generate a response, and then formatting the response before presenting it to the user. More complex chains can involve retrieving information from external knowledge bases, performing calculations, or interacting with other APIs.\n",
            "\n",
            "Retrieval Augmented Generation (RAG) is a technique that combines the power of LLMs with the ability to retrieve relevant information from external sources. In a RAG system, the LLM's generation process is augmented by retrieved information, leading to more accurate, factual, and contextually relevant responses. Langchain provides built-in support for building RAG applications, making it easier to integrate external knowledge into LLM-powered systems.\n",
            "--- Chunk 3 ---\n",
            "Text splitting is a crucial step in preparing large documents or datasets for use with LLMs and RAG systems. Since LLMs have limitations on the amount of text they can process at once, large texts need to be split into smaller chunks. Various text splitting strategies exist, such as splitting by characters, words, or sentences, and different strategies may be more suitable for different types of data and applications.\n",
            "\n",
            "Vector stores, also known as vector databases, are specialized databases designed to store and search for vector representations of data. In the context of LLMs and RAG, text chunks are often converted into numerical vectors (embeddings) using embedding models. These vectors capture the semantic meaning of the text, allowing for efficient similarity search. When a user query is received, it is also converted into a vector, and the vector store is queried to find the most similar text chunks from the external knowledge base.\n",
            "--- Chunk 4 ---\n",
            "Integrating LLMs into applications involves selecting an appropriate LLM, configuring its parameters, and handling the input and output. Langchain provides connectors for various LLMs, including popular models from OpenAI, Google, and others. It also offers tools for managing prompts, parsing model outputs, and handling conversational flows.\n",
            "\n",
            "Building a Question Answering (Q&A) system using LLMs and RAG involves several steps:\n",
            "1. Loading the external knowledge base.\n",
            "2. Splitting the knowledge base into smaller chunks.\n",
            "3. Generating embeddings for the text chunks and storing them in a vector store.\n",
            "4. Receiving a user query.\n",
            "5. Generating an embedding for the user query.\n",
            "6. Searching the vector store for relevant text chunks based on the query embedding.\n",
            "7. Passing the retrieved text chunks and the user query to an LLM to generate an answer.\n",
            "8. Presenting the answer to the user.\n",
            "--- Chunk 5 ---\n",
            "User interfaces (UIs) are essential for interacting with RAG-based applications. Gradio is a popular Python library for building simple and interactive UIs for machine learning models and demos. It allows developers to quickly create web interfaces with input and output components, such as text boxes, image displays, and audio players. Integrating Gradio with Langchain enables the creation of user-friendly Q&A interfaces that allow users to ask questions and receive answers generated by the RAG system.\n",
            "\n",
            "Langchain's expressiveness allows for the creation of complex chains involving multiple steps and components. For instance, a chain might involve retrieving information, summarizing it with an LLM, and then using another LLM to generate a response based on the summary. This flexibility makes Langchain suitable for a wide range of natural language processing tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_text_with_chapters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk7BSJlnHlw_",
        "outputId": "40337176-1063-4fac-8763-ee7d73b5b241"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# Chapter 1: Introduction to Generative AI\n",
            "## What is Gen-AI?\n",
            "Generative AI, or Gen-AI, is a type of artificial intelligence that can create new content, such as text, images, audio, and video. Unlike traditional AI that focuses on analysis and prediction, Gen-AI is about creation and innovation.\n",
            "\n",
            "## Large Language Models (LLMs)\n",
            "Large Language Models (LLMs) are a prominent example of Gen-AI, trained on vast amounts of text data to understand and generate human-like language. They are the backbone of many modern AI applications.\n",
            "\n",
            "# Chapter 2: Understanding Langchain\n",
            "## What is Langchain?\n",
            "Langchain is a framework designed to simplify the development of applications using large language models. It provides a structured way to chain together different components, such as LLMs, data sources, and other tools.\n",
            "\n",
            "## The Concept of Chains\n",
            "One of the core concepts in Langchain is the \"chain,\" which represents a sequence of operations performed by different components. This allows for building complex workflows.\n",
            "\n",
            "# Chapter 3: Retrieval Augmented Generation (RAG)\n",
            "## RAG Explained\n",
            "Retrieval Augmented Generation (RAG) is a technique that combines the power of LLMs with the ability to retrieve relevant information from external sources. This enhances the accuracy and relevance of generated responses.\n",
            "\n",
            "## RAG in Langchain\n",
            "Langchain provides built-in support for building RAG applications, making it easier to integrate external knowledge into LLM-powered systems.\n",
            "\n",
            "# Chapter 4: Data Preparation for RAG\n",
            "## Text Splitting\n",
            "Text splitting is a crucial step in preparing large documents or datasets for use with LLMs and RAG systems. Large texts need to be split into smaller chunks due to LLM limitations.\n",
            "\n",
            "## Text Splitting Strategies\n",
            "Various text splitting strategies exist, such as splitting by characters, words, or sentences, and different strategies may be more suitable for different types of data.\n",
            "\n",
            "# Chapter 5: Vector Stores and Embeddings\n",
            "## Vector Stores\n",
            "Vector stores, also known as vector databases, are specialized databases designed to store and search for vector representations of data.\n",
            "\n",
            "## Embeddings\n",
            "Text chunks are often converted into numerical vectors (embeddings) using embedding models. These vectors capture the semantic meaning of the text, allowing for efficient similarity search.\n",
            "\n",
            "# Chapter 6: Integrating LLMs\n",
            "## Selecting and Configuring LLMs\n",
            "Integrating LLMs into applications involves selecting an appropriate LLM, configuring its parameters, and handling the input and output.\n",
            "\n",
            "## Langchain Connectors\n",
            "Langchain provides connectors for various LLMs, including popular models from OpenAI, Google, and others, simplifying the integration process.\n",
            "\n",
            "# Chapter 7: Building a Q&A System\n",
            "## Steps in Building a Q&A System\n",
            "Building a Question Answering (Q&A) system using LLMs and RAG involves several steps, from loading data to generating answers.\n",
            "\n",
            "## Q&A Workflow\n",
            "The workflow typically includes loading data, splitting text, generating embeddings, storing in a vector store, receiving queries, retrieving relevant chunks, and generating answers with an LLM.\n",
            "\n",
            "# Chapter 8: User Interfaces with Gradio\n",
            "## Importance of UIs\n",
            "User interfaces (UIs) are essential for interacting with RAG-based applications, providing a way for users to input queries and receive responses.\n",
            "\n",
            "## Using Gradio\n",
            "Gradio is a popular Python library for building simple and interactive UIs for machine learning models and demos. It allows quickly creating web interfaces.\n",
            "\n",
            "## Gradio and Langchain\n",
            "Integrating Gradio with Langchain enables the creation of user-friendly Q&A interfaces that allow users to ask questions and receive answers.\n",
            "\n",
            "# Chapter 9: Advanced Langchain Concepts\n",
            "## Expressiveness of Chains\n",
            "Langchain's expressiveness allows for the creation of complex chains involving multiple steps and components, suitable for a wide range of NLP tasks.\n",
            "\n",
            "## Customizing Splitters\n",
            "The choice of text splitter can significantly impact RAG performance. Different splitters have parameters that can be tuned for optimization.\n",
            "\n",
            "# Chapter 10: Embedding Models in Detail\n",
            "## Role of Embedding Models\n",
            "Embedding models play a crucial role in converting text into meaningful vector representations. The quality of embeddings affects similarity search accuracy.\n",
            "\n",
            "## Selecting Embedding Models\n",
            "Various embedding models are available, each with strengths and weaknesses. Selecting an appropriate model for the domain is important.\n",
            "\n",
            "# Chapter 11: Exploring Vector Stores\n",
            "## Vector Store Options\n",
            "Different vector stores offer varying features and performance. Popular options include FAISS, Annoy, Pinecone, and Weaviate.\n",
            "\n",
            "## Choosing a Vector Store\n",
            "The choice of vector store depends on factors like data size, required search performance, and desired scalability.\n",
            "\n",
            "# Chapter 12: Integrating External APIs\n",
            "## Extending Capabilities\n",
            "Integrating LLMs with external APIs allows for creating applications that interact with real-world services, extending their capabilities.\n",
            "\n",
            "## Langchain API Tools\n",
            "Langchain provides tools for integrating with various APIs, enabling the creation of more powerful and versatile applications.\n",
            "\n",
            "# Chapter 13: Conversational Memory\n",
            "## Handling Conversation History\n",
            "Handling conversational history is important for building engaging and contextually aware Q&A systems.\n",
            "\n",
            "## Langchain Memory Mechanisms\n",
            "Langchain provides mechanisms for managing conversational memory, allowing the LLM to remember previous turns and generate consistent responses.\n",
            "\n",
            "# Chapter 14: Error Handling and Robustness\n",
            "## Importance of Error Handling\n",
            "Error handling and robustness are important considerations when building RAG applications to ensure stability and reliability.\n",
            "\n",
            "## Implementing Error Handling\n",
            "Implementing proper error handling mechanisms is essential to anticipate potential errors during different stages of the RAG process.\n",
            "\n",
            "# Chapter 15: Evaluating RAG Systems\n",
            "## Evaluating Performance\n",
            "Evaluating the performance of a RAG system involves measuring its ability to retrieve relevant information and generate accurate answers.\n",
            "\n",
            "## Evaluation Metrics\n",
            "Various evaluation metrics can be used, such as precision, recall, F1-score, and ROUGE, to assess system performance.\n",
            "\n",
            "# Chapter 16: Fine-tuning LLMs\n",
            "## Improving Performance\n",
            "Fine-tuning LLMs on specific datasets can further improve their performance on particular tasks and domains.\n",
            "\n",
            "## Fine-tuning with Langchain\n",
            "Langchain can be used to integrate fine-tuned LLMs into RAG systems, leveraging their specialized knowledge.\n",
            "\n",
            "# Chapter 17: Deploying RAG Applications\n",
            "## Deployment Options\n",
            "Deploying RAG-based applications involves packaging components and making them accessible to users through various options like web services or desktop applications.\n",
            "\n",
            "## Planning Deployment\n",
            "Deploying requires careful planning and consideration of the deployment environment and infrastructure.\n",
            "\n",
            "# Chapter 18: Security and Privacy\n",
            "## Protecting Sensitive Data\n",
            "Security and privacy are important considerations, especially when dealing with sensitive data in RAG applications.\n",
            "\n",
            "## Implementing Security Measures\n",
            "Implementing appropriate security measures is essential to protect the external knowledge base and ensure user privacy.\n",
            "\n",
            "# Chapter 19: Staying Updated\n",
            "## Evolving Field\n",
            "The field of Gen-AI and LLMs is constantly evolving, with new models and techniques regularly developed.\n",
            "\n",
            "## Importance of Staying Updated\n",
            "Staying updated with the latest advancements is important for building cutting-edge RAG applications.\n",
            "\n",
            "# Chapter 20: Resources and Community\n",
            "## Langchain Resources\n",
            "Langchain's community and documentation are valuable resources for developers seeking support and information.\n",
            "\n",
            "## Building Successful Applications\n",
            "Building a successful RAG application requires technical skills, domain expertise, and careful consideration of requirements.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt => From the above 'sample_text_with_chapters' split the input text using Text plitters in langchain - the chunks/splits should be based on each 'chapter name'."
      ],
      "metadata": {
        "id": "lvP2ijCbIOj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "# Define the headers to split on\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Chapter\"),\n",
        "    (\"##\", \"Topic\"),\n",
        "]\n",
        "\n",
        "# Initialize the MarkdownHeaderTextSplitter\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "# Split the sample_text_with_chapters\n",
        "md_header_splits = markdown_splitter.split_text(sample_text_with_chapters)\n",
        "\n",
        "# Display a few chunks\n",
        "print(type(headers_to_split_on))\n",
        "print(type(markdown_splitter))\n",
        "print(type(md_header_splits))\n",
        "\n",
        "\n",
        "print(f\"Number of chunks: {len(md_header_splits)}\")\n",
        "print(\"First 5 chunks:\")\n",
        "for i, chunk in enumerate(md_header_splits[:5]):\n",
        "    print(f\"--- Chunk {i+1} ---\")\n",
        "    print(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmmHTdD0IRX0",
        "outputId": "76771d68-8b86-41a6-e033-8336d650af54"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain_text_splitters.markdown.MarkdownHeaderTextSplitter'>\n",
            "<class 'list'>\n",
            "Number of chunks: 41\n",
            "First 5 chunks:\n",
            "--- Chunk 1 ---\n",
            "page_content='Generative AI, or Gen-AI, is a type of artificial intelligence that can create new content, such as text, images, audio, and video. Unlike traditional AI that focuses on analysis and prediction, Gen-AI is about creation and innovation.' metadata={'Chapter': 'Chapter 1: Introduction to Generative AI', 'Topic': 'What is Gen-AI?'}\n",
            "--- Chunk 2 ---\n",
            "page_content='Large Language Models (LLMs) are a prominent example of Gen-AI, trained on vast amounts of text data to understand and generate human-like language. They are the backbone of many modern AI applications.' metadata={'Chapter': 'Chapter 1: Introduction to Generative AI', 'Topic': 'Large Language Models (LLMs)'}\n",
            "--- Chunk 3 ---\n",
            "page_content='Langchain is a framework designed to simplify the development of applications using large language models. It provides a structured way to chain together different components, such as LLMs, data sources, and other tools.' metadata={'Chapter': 'Chapter 2: Understanding Langchain', 'Topic': 'What is Langchain?'}\n",
            "--- Chunk 4 ---\n",
            "page_content='One of the core concepts in Langchain is the \"chain,\" which represents a sequence of operations performed by different components. This allows for building complex workflows.' metadata={'Chapter': 'Chapter 2: Understanding Langchain', 'Topic': 'The Concept of Chains'}\n",
            "--- Chunk 5 ---\n",
            "page_content='Retrieval Augmented Generation (RAG) is a technique that combines the power of LLMs with the ability to retrieve relevant information from external sources. This enhances the accuracy and relevance of generated responses.' metadata={'Chapter': 'Chapter 3: Retrieval Augmented Generation (RAG)', 'Topic': 'RAG Explained'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt => for the above splits - create embeddings and display few embeddings too. Use openai embedding models to build embeddings. Store the embeddings later in vector store / db, do not write the code to storing, stick to create and display embeddings only."
      ],
      "metadata": {
        "id": "Ktgkx1qcJR4j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e79adfc",
        "outputId": "4caa13ac-594c-4100-a1f2-4eef4382ed82"
      },
      "source": [
        "%pip install --upgrade --quiet  openai"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.0/812.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12e183c3",
        "outputId": "c7150627-49b0-423b-906b-1ac98aa4a9d8"
      },
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your OpenAI API key\n",
        "# In Colab, add the key to the secrets manager under the \"🔑\" in the left panel.\n",
        "# Give it the name `OPENAI_API_KEY`.\n",
        "# Then pass the key to the SDK:\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = []\n",
        "for chunk in md_header_splits:\n",
        "    response = client.embeddings.create(\n",
        "        input=chunk.page_content,\n",
        "        model=\"text-embedding-ada-002\"  # You can choose a different embedding model if needed\n",
        "        #model = \"text-embedding-3-small\"\n",
        "      )\n",
        "    embeddings.append(response.data[0].embedding)\n",
        "\n",
        "# Display a few embeddings\n",
        "print(f\"Number of embeddings generated: {len(embeddings)}\")\n",
        "print(\"First 3 embeddings:\")\n",
        "for i, embedding in enumerate(embeddings[:3]):\n",
        "    print(f\"--- Embedding {i+1} (first 10 dimensions) ---\")\n",
        "    print(embedding[:10])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of embeddings generated: 41\n",
            "First 3 embeddings:\n",
            "--- Embedding 1 (first 10 dimensions) ---\n",
            "[-0.025043487548828125, -0.012731093913316727, -0.016826502978801727, 0.007274910807609558, 0.012358189560472965, 0.020529380068182945, -0.029125811532139778, 0.015177871100604534, -0.01033665332943201, -0.04071856662631035]\n",
            "--- Embedding 2 (first 10 dimensions) ---\n",
            "[-0.023705128580331802, 0.007175141014158726, 0.0011254636337980628, -0.010037247091531754, -0.00905671063810587, 0.013416122645139694, -0.007380523718893528, 0.02825004793703556, -0.01583433710038662, -0.0336032472550869]\n",
            "--- Embedding 3 (first 10 dimensions) ---\n",
            "[-0.007045831996947527, 0.008539576083421707, -0.015674076974391937, -0.04586270451545715, 0.009446735493838787, 0.0065854317508637905, -0.0030352326575666666, 0.011138280853629112, 0.012529713101685047, -0.026696404442191124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note => Use a different embedding model below"
      ],
      "metadata": {
        "id": "uVsGdgyCMOMv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your OpenAI API key\n",
        "# In Colab, add the key to the secrets manager under the \"🔑\" in the left panel.\n",
        "# Give it the name `OPENAI_API_KEY`.\n",
        "# Then pass the key to the SDK:\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI()\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = []\n",
        "for chunk in md_header_splits:\n",
        "    response = client.embeddings.create(\n",
        "        input=chunk.page_content,\n",
        "        #model=\"text-embedding-ada-002\"  # You can choose a different embedding model if needed\n",
        "        model = \"text-embedding-3-small\"\n",
        "      )\n",
        "    embeddings.append(response.data[0].embedding)\n",
        "\n",
        "# Display a few embeddings\n",
        "print(f\"Number of embeddings generated: {len(embeddings)}\")\n",
        "print(\"First 3 embeddings:\")\n",
        "for i, embedding in enumerate(embeddings[:3]):\n",
        "    print(f\"--- Embedding {i+1} (first 10 dimensions) ---\")\n",
        "    print(embedding[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXa37b2fL3R4",
        "outputId": "324453ea-5382-4cad-f0d0-81d87f8da091"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of embeddings generated: 41\n",
            "First 3 embeddings:\n",
            "--- Embedding 1 (first 10 dimensions) ---\n",
            "[0.014702312648296356, 0.009112956933677197, 0.02872917428612709, -0.007750798016786575, 0.010638350620865822, -0.08983495831489563, 0.011617753654718399, 0.005397977773100138, 0.022233588621020317, -0.02839144878089428]\n",
            "--- Embedding 2 (first 10 dimensions) ---\n",
            "[0.0038948608562350273, -0.000697912706527859, 0.053160130977630615, -0.008831404149532318, 0.011985068209469318, -0.04526166245341301, -0.019322631880640984, -0.012088091112673283, -0.013358714990317822, 0.051008082926273346]\n",
            "--- Embedding 3 (first 10 dimensions) ---\n",
            "[-0.03487681597471237, -0.04441509768366814, 0.04697801172733307, -0.0356694720685482, 0.016064472496509552, -0.012523947283625603, -0.0033852970227599144, -0.003979788161814213, -0.049223870038986206, 0.0212299395352602]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4 Build a Vector Store & Vector DB"
      ],
      "metadata": {
        "id": "nOCjqcccMU-K"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt 1 => cretae a FAISS vector store and write/store the embeddings for 'sample_text_with_chapters'"
      ],
      "metadata": {
        "id": "RIqgHJ22Mw56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISGCuV6VNHLO",
        "outputId": "871ec757-a048-48ab-8d85-e25c45d7d0fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.75)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.16)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt => cretae a FAISS vector store and write/store the embeddings for 'sample_text_with_chapters' - print a success message once the embeddings are stored."
      ],
      "metadata": {
        "id": "sF5OgF1YNPO6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ba51ee3",
        "outputId": "6341d9aa-67d5-4aa3-fae2-c8347e205ca9"
      },
      "source": [
        "%pip install --upgrade --quiet  faiss-cpu"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain-openai"
      ],
      "metadata": {
        "id": "bXyk5cEwO322"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c24344d8",
        "outputId": "3b8ca99d-5ffa-4fea-937d-c0dbd8661b5f"
      },
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your OpenAI API key\n",
        "# In Colab, add the key to the secrets manager under the \"🔑\" in the left panel.\n",
        "# Give it the name `OPENAI_API_KEY`.\n",
        "# Then pass the key to the SDK:\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI Embedding model\n",
        "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\") # Using the same model as before\n",
        "\n",
        "# Create a FAISS vector store from the text chunks and embeddings\n",
        "vectorstore = FAISS.from_documents(md_header_splits, embeddings_model)\n",
        "\n",
        "print(type(vectorstore))\n",
        "print(\"Embeddings successfully stored in FAISS vector store.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_community.vectorstores.faiss.FAISS'>\n",
            "Embeddings successfully stored in FAISS vector store.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt => Display few embeddings from the above FAISS Vector Store."
      ],
      "metadata": {
        "id": "x_zNlGWhPUN_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a similarity search to demonstrate the vector store\n",
        "query = \"What is RAG?\"\n",
        "docs = vectorstore.similarity_search(query)\n",
        "\n",
        "print(f\"Number of relevant documents found: {len(docs)}\")\n",
        "print(\"Relevant documents:\")\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"--- Document {i+1} ---\")\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print(f\"Metadata: {doc.metadata}\") # Display metadata which includes chapter/topic info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PybaMnH2OGlp",
        "outputId": "37ec58eb-2668-43c7-dd93-b23582e5a9a9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of relevant documents found: 4\n",
            "Relevant documents:\n",
            "--- Document 1 ---\n",
            "Content: Retrieval Augmented Generation (RAG) is a technique that combines the power of LLMs with the ability to retrieve relevant information from external sources. This enhances the accuracy and relevance of generated responses.\n",
            "Metadata: {'Chapter': 'Chapter 3: Retrieval Augmented Generation (RAG)', 'Topic': 'RAG Explained'}\n",
            "--- Document 2 ---\n",
            "Content: Building a successful RAG application requires technical skills, domain expertise, and careful consideration of requirements.\n",
            "Metadata: {'Chapter': 'Chapter 20: Resources and Community', 'Topic': 'Building Successful Applications'}\n",
            "--- Document 3 ---\n",
            "Content: Evaluating the performance of a RAG system involves measuring its ability to retrieve relevant information and generate accurate answers.\n",
            "Metadata: {'Chapter': 'Chapter 15: Evaluating RAG Systems', 'Topic': 'Evaluating Performance'}\n",
            "--- Document 4 ---\n",
            "Content: Staying updated with the latest advancements is important for building cutting-edge RAG applications.\n",
            "Metadata: {'Chapter': 'Chapter 19: Staying Updated', 'Topic': 'Importance of Staying Updated'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3N46YX4Oz6r",
        "outputId": "5b375090-b1cc-4cca-e389-5896bf63a817"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/67.3 kB\u001b[0m \u001b[31m528.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/67.3 kB\u001b[0m \u001b[31m528.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m493.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI Embedding model\n",
        "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\") # Using the same model as before\n",
        "\n",
        "# Define the directory to store ChromaDB data\n",
        "persist_directory = \"/content/chroma_db\"\n",
        "\n",
        "# Create a Chroma vector store from the text chunks and embeddings, and persist it to disk\n",
        "vectorstore_chroma = Chroma.from_documents(\n",
        "    md_header_splits,\n",
        "    embeddings_model,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "print(type(vectorstore_chroma))\n",
        "print(f\"Embeddings successfully stored in ChromaDB and persisted to {persist_directory}.\")\n",
        "\n",
        "# Perform a similarity search to demonstrate the vector store\n",
        "query = \"What is RAG?\"\n",
        "docs_chroma = vectorstore_chroma.similarity_search(query)\n",
        "\n",
        "print(f\"\\nNumber of relevant documents found in ChromaDB: {len(docs_chroma)}\")\n",
        "print(\"Relevant documents from ChromaDB:\")\n",
        "for i, doc in enumerate(docs_chroma):\n",
        "    print(f\"--- Document {i+1} ---\")\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print(f\"Metadata: {doc.metadata}\") # Display metadata which includes chapter/topic info\n",
        "\n",
        "# To confirm data is on disk, you can list the directory contents\n",
        "print(f\"\\nContents of the persistence directory ({persist_directory}):\")\n",
        "print(os.listdir(persist_directory))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuGxK11QQQJK",
        "outputId": "865fb4f1-7ce4-4f10-b7d1-bee00ede6dd9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_community.vectorstores.chroma.Chroma'>\n",
            "Embeddings successfully stored in ChromaDB and persisted to /content/chroma_db.\n",
            "\n",
            "Number of relevant documents found in ChromaDB: 4\n",
            "Relevant documents from ChromaDB:\n",
            "--- Document 1 ---\n",
            "Content: Retrieval Augmented Generation (RAG) is a technique that combines the power of LLMs with the ability to retrieve relevant information from external sources. This enhances the accuracy and relevance of generated responses.\n",
            "Metadata: {'Chapter': 'Chapter 3: Retrieval Augmented Generation (RAG)', 'Topic': 'RAG Explained'}\n",
            "--- Document 2 ---\n",
            "Content: Retrieval Augmented Generation (RAG) is a technique that combines the power of LLMs with the ability to retrieve relevant information from external sources. This enhances the accuracy and relevance of generated responses.\n",
            "Metadata: {'Chapter': 'Chapter 3: Retrieval Augmented Generation (RAG)', 'Topic': 'RAG Explained'}\n",
            "--- Document 3 ---\n",
            "Content: Retrieval Augmented Generation (RAG) is a technique that combines the power of LLMs with the ability to retrieve relevant information from external sources. This enhances the accuracy and relevance of generated responses.\n",
            "Metadata: {'Topic': 'RAG Explained', 'Chapter': 'Chapter 3: Retrieval Augmented Generation (RAG)'}\n",
            "--- Document 4 ---\n",
            "Content: Retrieval Augmented Generation (RAG) is a technique that combines the power of LLMs with the ability to retrieve relevant information from external sources. This enhances the accuracy and relevance of generated responses.\n",
            "Metadata: {'Chapter': 'Chapter 3: Retrieval Augmented Generation (RAG)', 'Topic': 'RAG Explained'}\n",
            "\n",
            "Contents of the persistence directory (/content/chroma_db):\n",
            "['chroma.sqlite3', '39fe71a6-6398-483d-8726-c86b60eeb967']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt => Integrate openai LLM for the above embeddings store in chroma vector db. Only integrate LLM and do not build Retrieval, we will build it in the next step."
      ],
      "metadata": {
        "id": "1Hxj-Xu_RNNO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "# Using a different model that might be better at leveraging retrieved context\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "print(\"OpenAI LLM integrated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iG_7SN4Sbz2",
        "outputId": "4e08a893-e10d-4fa6-f4cb-7f6901f20763"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI LLM integrated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Y_axQ6VSzFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Create a retriever from the Chroma vector store\n",
        "# The vectorstore_chroma object was created in a previous step (cell LuGxK11QQQJK)\n",
        "retriever = vectorstore_chroma.as_retriever()\n",
        "\n",
        "# Create a RetrievalQA chain\n",
        "# The llm object was created in a previous step (cell 0iG_7SN4Sbz2)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    chain_type=\"stuff\", # Other chain types include \"map_reduce\", \"refine\", \"map_rerank\"\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True # Optional: return the retrieved documents\n",
        ")\n",
        "\n",
        "print(\"RetrievalQA chain created and connected to the LLM and ChromaDB vector store.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1lMTAFaTL4d",
        "outputId": "2e420413-e7e4-4710-8d19-104e7579de33"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RetrievalQA chain created and connected to the LLM and ChromaDB vector store.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7 -> Build a CLI chatbot"
      ],
      "metadata": {
        "id": "EKIh6B1FUxUu"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt 1 => Now lets build a CLI chatbot on the above embeddings."
      ],
      "metadata": {
        "id": "4C1U4LG3TcCT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Define a custom prompt template\n",
        "custom_prompt_template = \"\"\"Use the following pieces of context to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "--------------------\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "CUSTOM_PROMPT = PromptTemplate(\n",
        "    template=custom_prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Create a RetrievalQA chain with the custom prompt\n",
        "# The llm object and vectorstore_chroma object were created in previous steps\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    chain_type=\"stuff\", # Other chain types include \"map_reduce\", \"refine\", \"map_rerank\"\n",
        "    retriever=vectorstore_chroma.as_retriever(),\n",
        "    return_source_documents=True, # Optional: return the retrieved documents\n",
        "    chain_type_kwargs={\"prompt\": CUSTOM_PROMPT} # Pass the custom prompt here\n",
        ")\n",
        "\n",
        "\n",
        "# Simple CLI interaction for the chatbot\n",
        "print(\"Chatbot ready! Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Use the created RetrievalQA chain to get the answer\n",
        "    response = qa_chain({\"query\": query})\n",
        "    answer = response[\"result\"]\n",
        "    source_documents = response[\"source_documents\"]\n",
        "\n",
        "    # Print the response\n",
        "    print(f\"Bot: {answer}\")\n",
        "    #print(\"\\nSource Documents:\")\n",
        "\n",
        "    \"\"\"for i, doc in enumerate(source_documents):\n",
        "        #print(f\"--- Document {i+1} ---\")\n",
        "        #print(f\"Content: {doc.page_content}\")\n",
        "        #print(f\"Metadata: {doc.metadata}\") # Display metadata which includes chapter/topic info\n",
        "    \"\"\"\n",
        "    print(\"-\" * 20) # Separator for clarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3zBX9m6T719",
        "outputId": "d47dbe2d-4f4d-45c9-887b-ad389673c6e6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot ready! Type 'exit' to quit.\n",
            "You: what is Gen-AI\n",
            "Bot: Generative AI, or Gen-AI, is a type of artificial intelligence that can create new content, such as text, images, audio, and video. Unlike traditional AI that focuses on analysis and prediction, Gen-AI is about creation and innovation.\n",
            "--------------------\n",
            "You: WHAT IS cRICKET\n",
            "Bot: I don't know.\n",
            "--------------------\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8 - Build a UI and deploy the chatbot."
      ],
      "metadata": {
        "id": "gRy-NvQWU07v"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  gradio\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def chatbot_response(message, history):\n",
        "    # Use the created RetrievalQA chain to get the answer\n",
        "    response = qa_chain({\"query\": message})\n",
        "    answer = response[\"result\"]\n",
        "    source_documents = response[\"source_documents\"]\n",
        "\n",
        "    # Format the response to include the answer and source documents (optional)\n",
        "    formatted_response = f\"{answer}\" # You can add source documents here if desired\n",
        "\n",
        "    return formatted_response\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.ChatInterface(\n",
        "    fn=chatbot_response,\n",
        "    title=\"RAG Chatbot\",\n",
        "    description=\"Ask questions about Gen-AI and Langchain based on the provided text.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "kx3t4HyKXj8p",
        "outputId": "c45edeb1-6243-4073-eabf-1b7595ea9640"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7007a35ba947d4096e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7007a35ba947d4096e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insurance documents - RAG Based"
      ],
      "metadata": {
        "id": "pk7jAtcBXwkJ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG\n",
        "    # 1 Private Data as External Knowledge\n",
        "    # 2 Chat with LLM - on Private Data\n",
        "    # 3 Reduce Hallucinations\n",
        ""
      ],
      "metadata": {
        "id": "QsRhdiGibdHa"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trading application - It can read charts, can implement diff strategies and make decisions to place and order or not - Buy/Sell."
      ],
      "metadata": {
        "id": "URcIJJleb1eq"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hands-on Assignemnts -\n",
        "# QnA Chatbot - > YouTube Video Transcripts.\n",
        "\n",
        "#Build a RAG Based Gen-AI Application (chat-bot) - Q&A over YouTube Videos.\n",
        "\n",
        "#Steps:\n",
        "\n",
        "# 1 Select Topics (e.g. Healthcare, Data Engineering, Gen-AI, or any Podcast videos.\n",
        "\n",
        "# 2 Get the transcript of the YouTube Video / Videos (Hint - Use YouTube API and Video-IDs to get the transcript)\n",
        "\n",
        "# 3 Use Text-Splitter to divide huge transcripts into chunks.\n",
        "\n",
        "# 4 Generate Embeddings for the chunks and store in Vector-DB (eg - Chroma / PineCone)\n",
        "\n",
        "# 5 Integrate LLM\n",
        "\n",
        "# 6 Send prompts and generate responses from the external knowledge (transcripts)\n",
        "\n",
        "# 7 Try to make it dynamic - to chat in a loop untill 'exit' is typed, the bot shall run.\n",
        "\n",
        "# 8 Build UI using 'Gradio'\n",
        "\n",
        "# 9 Optional - Deploy the application on Azure Cloud."
      ],
      "metadata": {
        "id": "rKF7ZhdUcsbb"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# End of the Notebook"
      ],
      "metadata": {
        "id": "anI5YNhPdZa_"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zdV60tQldcaW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}