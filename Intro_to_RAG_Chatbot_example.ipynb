{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Welcoem to Langchain"
      ],
      "metadata": {
        "id": "NSAV7onpk3kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Simple Embeddings\n",
        "# Install\n",
        "!pip install -q langchain-openai\n",
        "\n",
        "# Imports\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# 1) Put your key once in Colab (in a separate cell):\n",
        "# from google.colab import userdata\n",
        "# userdata.set(\"OPENAI_API_KEY\", \"sk-...\")\n",
        "\n",
        "# 2) Load key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# 3) Create the embeddings model\n",
        "#   - \"text-embedding-3-small\": fast, cheap, 1536-dim\n",
        "#   - \"text-embedding-3-large\": higher quality, 3072-dim\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# 4) Take input and embed\n",
        "text = input(\"Enter text to embed: \").strip()\n",
        "vec = emb.embed_query(text)  # list[float]\n",
        "\n",
        "# 5) Show results\n",
        "print(f\"\\nText: {text!r}\")\n",
        "print(f\"Vector length (dimension): {len(vec)}\")\n",
        "print(\"First 8 numbers:\", [round(x, 6) for x in vec[:8]])\n"
      ],
      "metadata": {
        "id": "W-pKYs-4kyZz",
        "outputId": "1c7923dd-6524-4822-f620-fca1305c4a78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text to embed:  who is Sachin\n",
            "\n",
            "Text: 'who is Sachin'\n",
            "Vector length (dimension): 1536\n",
            "First 8 numbers: [0.057726, -0.000847, 0.011904, -0.00289, 0.005939, -0.045332, 0.051492, 0.049921]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exaple 2 - to create / build and save embeddings"
      ],
      "metadata": {
        "id": "G5-b6fUoky5L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Basic: Create + STORE embeddings (no search)\n",
        "# ============================================\n",
        "\n",
        "\n",
        "# Example 2 - Create + STORE embeddings (no search)\n",
        "\n",
        "# 0) Install deps\n",
        "!pip install -q langchain-openai faiss-cpu numpy pandas\n",
        "\n",
        "# 1) Imports & API key\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from google.colab import userdata\n",
        "import os, json, numpy as np, pandas as pd\n",
        "import faiss  # direct FAISS usage\n",
        "\n",
        "# Load key\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise RuntimeError(\n",
        "        \"‚ùå OPENAI_API_KEY not found in Colab userdata.\\n\"\n",
        "        \"Set it once with:\\n\"\n",
        "        \"from google.colab import userdata\\n\"\n",
        "        \"userdata.set('OPENAI_API_KEY', 'sk-...')\"\n",
        "    )\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# 2) Collect texts (user input or defaults)\n",
        "raw = input(\n",
        "    \"Enter texts separated by '||' (or press Enter to use defaults):\\n\"\n",
        ").strip()\n",
        "\n",
        "if raw:\n",
        "    texts = [t.strip() for t in raw.split(\"||\") if t.strip()]\n",
        "else:\n",
        "    texts = [\n",
        "        \"MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\",\n",
        "        \"SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\",\n",
        "        \"Amitabh Bachchan is nicknamed Big B in Indian cinema.\",\n",
        "        \"Shah Rukh Khan is popularly called King Khan.\",\n",
        "    ]\n",
        "\n",
        "print(f\"\\nüìù {len(texts)} texts to embed.\\n\")\n",
        "\n",
        "# 3) Create embeddings (vectors)\n",
        "emb_model = \"text-embedding-3-small\"  # 1536-dim; switch to -large for 3072-dim\n",
        "emb = OpenAIEmbeddings(model=emb_model)\n",
        "\n",
        "# embed_documents -> list of vectors (one per text)\n",
        "vectors = emb.embed_documents(texts)  # list[list[float]]\n",
        "dim = len(vectors[0]) if vectors else 0\n",
        "\n",
        "print(f\"‚úÖ Created {len(vectors)} embeddings with dimension = {dim} (model: {emb_model}).\\n\")\n",
        "\n",
        "# 4) SHOW what we stored (table preview)\n",
        "rows = []\n",
        "for i, (t, v) in enumerate(zip(texts, vectors), start=1):\n",
        "    rows.append({\n",
        "        \"id\": f\"doc-{i}\",\n",
        "        \"text\": t,\n",
        "        \"dim\": len(v),\n",
        "        \"preview(first_8_dims)\": [round(x, 6) for x in v[:8]]\n",
        "    })\n",
        "df = pd.DataFrame(rows)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# 5) STORE embeddings plainly (files you can inspect)\n",
        "#    - vectors.npy: the raw numeric vectors (float32)\n",
        "#    - texts.jsonl: metadata (id + original text)\n",
        "vec_arr = np.array(vectors, dtype=\"float32\")\n",
        "np.save(\"vectors.npy\", vec_arr)\n",
        "with open(\"texts.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, t in enumerate(texts, start=1):\n",
        "        f.write(json.dumps({\"id\": f\"doc-{i}\", \"text\": t}, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"\\nüíæ Saved raw vectors to ./vectors.npy and text metadata to ./texts.jsonl\")\n",
        "\n",
        "# 6) STORE embeddings in a FAISS index (no search yet)\n",
        "#    Build an L2 index and add our vectors.\n",
        "if len(vec_arr) > 0:\n",
        "    index = faiss.IndexFlatL2(dim)  # exact L2 index\n",
        "    index.add(vec_arr)              # add all vectors\n",
        "    print(f\"üì¶ FAISS index created. ntotal = {index.ntotal} vectors.\")\n",
        "\n",
        "    # (Optional) Save FAISS index to disk\n",
        "    faiss.write_index(index, \"index.faiss\")\n",
        "    print(\"üíæ Saved FAISS index to ./index.faiss\")\n",
        "\n",
        "    # (Optional) Show how to reload it later\n",
        "    reloaded = faiss.read_index(\"index.faiss\")\n",
        "    print(f\"üîÅ Reloaded FAISS index. ntotal = {reloaded.ntotal} vectors.\")\n",
        "else:\n",
        "    print(\"No vectors created; FAISS index not built.\")\n"
      ],
      "metadata": {
        "id": "dH293lJ5ky21",
        "outputId": "3ef8d614-dc2d-48bd-93e4-2f63cbac064b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter texts separated by '||' (or press Enter to use defaults):\n",
            "\n",
            "\n",
            "üìù 4 texts to embed.\n",
            "\n",
            "‚úÖ Created 4 embeddings with dimension = 1536 (model: text-embedding-3-small).\n",
            "\n",
            "   id                                                             text  dim                                                               preview(first_8_dims)\n",
            "doc-1 MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain. 1536    [0.108245, -0.016881, 0.041814, 0.037209, -0.008451, 0.011741, 0.0371, 0.007595]\n",
            "doc-2     SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'. 1536    [0.047046, 0.011721, 0.035615, 0.005706, -0.037799, -0.0189, 0.030132, 0.020781]\n",
            "doc-3            Amitabh Bachchan is nicknamed Big B in Indian cinema. 1536   [0.0534, -0.022114, 0.005836, 0.058191, -0.063351, -0.003442, 0.025553, 0.047135]\n",
            "doc-4                    Shah Rukh Khan is popularly called King Khan. 1536 [0.016193, -0.093645, -0.018306, 0.03755, 0.015275, -0.009726, -0.006596, 0.054999]\n",
            "\n",
            "üíæ Saved raw vectors to ./vectors.npy and text metadata to ./texts.jsonl\n",
            "üì¶ FAISS index created. ntotal = 4 vectors.\n",
            "üíæ Saved FAISS index to ./index.faiss\n",
            "üîÅ Reloaded FAISS index. ntotal = 4 vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3"
      ],
      "metadata": {
        "id": "3ZHo7LYtk_zq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3 - Tiny Search\n",
        "\n",
        "# ============================================\n",
        "# Tiny similarity search with FAISS (Colab)\n",
        "# ============================================\n",
        "\n",
        "# 0) Install deps (if not already)\n",
        "!pip install -q langchain-openai faiss-cpu numpy\n",
        "\n",
        "# 1) Imports & API key\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from google.colab import userdata\n",
        "import os, json, numpy as np, faiss, sys\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    print(\"‚ùå OPENAI_API_KEY not found. Set it with:\\n\"\n",
        "          \"from google.colab import userdata\\n\"\n",
        "          \"userdata.set('OPENAI_API_KEY', 'sk-...')\\n\")\n",
        "    sys.exit(1)\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# 2) Try loading previously saved artifacts (from the ‚Äústore‚Äù step)\n",
        "INDEX_PATH = \"index.faiss\"\n",
        "TEXTS_PATH = \"texts.jsonl\"\n",
        "EMB_MODEL = \"text-embedding-3-small\"    # must match the model used to build the index\n",
        "\n",
        "def load_texts_jsonl(path):\n",
        "    items, ids = [], []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            obj = json.loads(line)\n",
        "            ids.append(obj.get(\"id\"))\n",
        "            items.append(obj.get(\"text\"))\n",
        "    return ids, items\n",
        "\n",
        "need_rebuild = False\n",
        "if os.path.exists(INDEX_PATH) and os.path.exists(TEXTS_PATH):\n",
        "    try:\n",
        "        index = faiss.read_index(INDEX_PATH)\n",
        "        doc_ids, texts = load_texts_jsonl(TEXTS_PATH)\n",
        "        print(f\"‚úÖ Loaded FAISS index (ntotal={index.ntotal}) and {len(texts)} texts.\")\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Could not load previous index/metadata. Will rebuild a small demo index.\\nReason:\", e)\n",
        "        need_rebuild = True\n",
        "else:\n",
        "    need_rebuild = True\n",
        "\n",
        "# 3) If needed, rebuild a small index (self-contained)\n",
        "if need_rebuild:\n",
        "    print(\"üîß Rebuilding a small demo index locally...\")\n",
        "    texts = [\n",
        "        \"MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\",\n",
        "        \"SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\",\n",
        "        \"Amitabh Bachchan is nicknamed Big B in Indian cinema.\",\n",
        "        \"Shah Rukh Khan is popularly called King Khan.\",\n",
        "        \"Virat Kohli is an Indian cricketer known for batting consistency.\",\n",
        "        \"Rohit Sharma is known for explosive batting in limited-overs.\"\n",
        "    ]\n",
        "    emb = OpenAIEmbeddings(model=EMB_MODEL)\n",
        "    vecs = emb.embed_documents(texts)           # list[list[float]]\n",
        "    vec_arr = np.array(vecs, dtype=\"float32\")   # (N, dim)\n",
        "    dim = vec_arr.shape[1]\n",
        "\n",
        "    index = faiss.IndexFlatL2(dim)              # exact L2 index\n",
        "    index.add(vec_arr)\n",
        "\n",
        "    # save artifacts for later use\n",
        "    faiss.write_index(index, INDEX_PATH)\n",
        "    with open(TEXTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, t in enumerate(texts, start=1):\n",
        "            f.write(json.dumps({\"id\": f\"doc-{i}\", \"text\": t}, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Built and saved demo index. ntotal={index.ntotal}, dim={dim}\")\n",
        "\n",
        "# 4) Run a tiny search\n",
        "emb = OpenAIEmbeddings(model=EMB_MODEL)  # same model as used for the index\n",
        "query = input(\"\\nEnter your search query (e.g., 'Who is King Khan?'): \").strip()\n",
        "if not query:\n",
        "    query = \"Who is King Khan?\"\n",
        "    print(f\"(Using default) {query}\")\n",
        "\n",
        "q_vec = np.array([emb.embed_query(query)], dtype=\"float32\")  # shape (1, dim)\n",
        "\n",
        "k = 3  # top-k results\n",
        "distances, indices = index.search(q_vec, k)   # shapes: (1, k), (1, k)\n",
        "dist_list = distances[0].tolist()\n",
        "idx_list = indices[0].tolist()\n",
        "\n",
        "# load texts (if not already loaded)\n",
        "if 'texts' not in globals():\n",
        "    _, texts = load_texts_jsonl(TEXTS_PATH)\n",
        "\n",
        "print(f\"\\nüîé Top-{k} results for: {query!r}  (Lower distance = closer match)\\n\")\n",
        "for rank, (idx, dist) in enumerate(zip(idx_list, dist_list), start=1):\n",
        "    if idx == -1:\n",
        "        continue  # FAISS returns -1 if not enough results\n",
        "    # idx corresponds to the position of the text used when building the index\n",
        "    print(f\"{rank}. [distance={dist:.4f}]  {texts[idx]}\")\n",
        "\n",
        "print(\"\\n‚úÖ Done.\")\n"
      ],
      "metadata": {
        "id": "bANj-kgQmqSz",
        "outputId": "5f4d0d8f-b936-461e-99f3-dce6573eba17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded FAISS index (ntotal=4) and 4 texts.\n",
            "\n",
            "Enter your search query (e.g., 'Who is King Khan?'): who is MSD\n",
            "\n",
            "üîé Top-3 results for: 'who is MSD'  (Lower distance = closer match)\n",
            "\n",
            "1. [distance=0.8382]  MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\n",
            "2. [distance=1.5777]  SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\n",
            "3. [distance=1.8358]  Amitabh Bachchan is nicknamed Big B in Indian cinema.\n",
            "\n",
            "‚úÖ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4 - Cosine Similarity"
      ],
      "metadata": {
        "id": "I7murIrknKmC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4 - Cosine Similarity\n",
        "# ============================================================\n",
        "# Cosine similarity search with FAISS (normalized vectors)\n",
        "# ============================================================\n",
        "\n",
        "# 0) Install\n",
        "!pip install -q langchain-openai faiss-cpu numpy\n",
        "\n",
        "# 1) Imports & API key\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from google.colab import userdata\n",
        "import os, json, numpy as np, faiss, sys\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    print(\"‚ùå OPENAI_API_KEY not found. Set it with:\\n\"\n",
        "          \"from google.colab import userdata\\n\"\n",
        "          \"userdata.set('OPENAI_API_KEY', 'sk-...')\\n\")\n",
        "    sys.exit(1)\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "EMB_MODEL = \"text-embedding-3-small\"  # same model must be used for index & queries\n",
        "\n",
        "# 2) Data to index (you can replace with your own)\n",
        "texts = [\n",
        "    \"MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\",\n",
        "    \"SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\",\n",
        "    \"Amitabh Bachchan is nicknamed Big B in Indian cinema.\",\n",
        "    \"Shah Rukh Khan is popularly called King Khan.\",\n",
        "    \"Virat Kohli is an Indian cricketer known for batting consistency.\",\n",
        "    \"Rohit Sharma is known for explosive batting in limited-overs.\"\n",
        "]\n",
        "\n",
        "# 3) Embed and build a cosine index (normalize + IP)\n",
        "emb = OpenAIEmbeddings(model=EMB_MODEL)\n",
        "\n",
        "def l2_normalize_rows(mat: np.ndarray) -> np.ndarray:\n",
        "    norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-12\n",
        "    return mat / norms\n",
        "\n",
        "# Embed documents\n",
        "doc_vecs = emb.embed_documents(texts)            # list[list[float]]\n",
        "doc_arr = np.array(doc_vecs, dtype=\"float32\")    # (N, dim)\n",
        "doc_arr = l2_normalize_rows(doc_arr)             # normalize for cosine\n",
        "\n",
        "dim = doc_arr.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)                   # inner product ‚âà cosine (after normalization)\n",
        "index.add(doc_arr)                               # add normalized vectors\n",
        "print(f\"‚úÖ Cosine index ready. ntotal={index.ntotal}, dim={dim}\")\n",
        "\n",
        "# Save small metadata file so we can map results back to text\n",
        "with open(\"texts.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, t in enumerate(texts):\n",
        "        f.write(json.dumps({\"id\": i, \"text\": t}, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# 4) Query loop\n",
        "def load_texts(path=\"texts.jsonl\"):\n",
        "    items = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            items.append(json.loads(line)[\"text\"])\n",
        "    return items\n",
        "\n",
        "stored_texts = load_texts()\n",
        "\n",
        "print(\"\\nüí¨ Type your query (e.g., 'Who is King Khan?'). Type 'exit' to quit.\")\n",
        "while True:\n",
        "    q = input(\"Query: \").strip()\n",
        "    if q.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"üëã Goodbye!\")\n",
        "        break\n",
        "    if not q:\n",
        "        q = \"Who is King Khan?\"\n",
        "        print(f\"(Using default) {q}\")\n",
        "\n",
        "    # Embed and normalize the query\n",
        "    q_vec = np.array([emb.embed_query(q)], dtype=\"float32\")\n",
        "    q_vec = l2_normalize_rows(q_vec)\n",
        "\n",
        "    k = 3\n",
        "    sims, idxs = index.search(q_vec, k)   # sims in [0..1] after normalization\n",
        "    sims, idxs = sims[0], idxs[0]\n",
        "\n",
        "    print(f\"\\nüîé Top-{k} cosine matches for: {q!r}\")\n",
        "    for rank, (i, s) in enumerate(zip(idxs, sims), 1):\n",
        "        if i == -1: continue\n",
        "        print(f\"{rank}. score={s:.4f}  |  {stored_texts[i]}\")\n",
        "    print(\"\")\n"
      ],
      "metadata": {
        "id": "BzZNrSS0pQy1",
        "outputId": "059c06b9-54b0-4406-b1e2-20f110e49ed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cosine index ready. ntotal=6, dim=1536\n",
            "\n",
            "üí¨ Type your query (e.g., 'Who is King Khan?'). Type 'exit' to quit.\n",
            "Query: who is ABC\n",
            "\n",
            "üîé Top-3 cosine matches for: 'who is ABC'\n",
            "1. score=0.2215  |  Amitabh Bachchan is nicknamed Big B in Indian cinema.\n",
            "2. score=0.1518  |  MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\n",
            "3. score=0.1147  |  SRT refers to Sachin Ramesh Tendulkar, the 'God of Cricket'.\n",
            "\n",
            "Query: who is XYZ\n",
            "\n",
            "üîé Top-3 cosine matches for: 'who is XYZ'\n",
            "1. score=0.2052  |  MSD is Mahendra Singh Dhoni, a legendary Indian cricket captain.\n",
            "2. score=0.1615  |  Amitabh Bachchan is nicknamed Big B in Indian cinema.\n",
            "3. score=0.1603  |  Virat Kohli is an Indian cricketer known for batting consistency.\n",
            "\n",
            "Query: exit\n",
            "üëã Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding Models + LLM => Retrival"
      ],
      "metadata": {
        "id": "NYh84PYBpiDK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plug in LLM => 1. Input (Text/data) 2. Prompts 3. Output (Response) Retrieval\n",
        "\n",
        "# Input - Text -> Build Index (FAISS) --> Faster Retrieval\n",
        "\n",
        "# Select (statement/query) on RDBMS - RETRIEVES (Index)\n",
        "\n",
        "# EMBEDDINGS + FAISS --> Index\n",
        "# Retrive - fetch top -k relevant chunks (splits)\n",
        "# Prompt"
      ],
      "metadata": {
        "id": "vH9OASrpq37m"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example - basic - RAG implementation"
      ],
      "metadata": {
        "id": "DMnsk_5YyW0_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5 - GPT Model Plug In\n",
        "\n",
        "# ============================================================\n",
        "# Tiny RAG: GPT answers from FAISS-retrieved context (Colab)\n",
        "# ============================================================\n",
        "\n",
        "# 0) Install packages\n",
        "!pip install -q langchain-openai langchain-community faiss-cpu langchain-text-splitters\n",
        "\n",
        "# 1) Imports & API key\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise RuntimeError(\n",
        "        \"‚ùå OPENAI_API_KEY not found. Set once with:\\n\"\n",
        "        \"from google.colab import userdata\\n\"\n",
        "        \"userdata.set('OPENAI_API_KEY', 'sk-...')\"\n",
        "    )\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# 2) Paste your own source text (or press Enter to use defaults)\n",
        "raw = input(\n",
        "    \"Paste your reference text (or press Enter to use a default cricket sample):\\n\"\n",
        ").strip()\n",
        "\n",
        "if not raw:\n",
        "    raw = \"\"\"\n",
        "Mahendra Singh Dhoni (MSD) captained India and won the 2007 T20 World Cup and the 2011 ODI World Cup.\n",
        "Sachin Ramesh Tendulkar (SRT) is widely called the 'God of Cricket' for his batting records.\n",
        "Amitabh Bachchan, nicknamed Big B, is a legendary actor in Indian cinema.\n",
        "Shah Rukh Khan, often called King Khan, is one of the most popular Bollywood actors worldwide.\n",
        "Virat Kohli is renowned for batting consistency across formats.\n",
        "Rohit Sharma is known for explosive batting and captaincy in limited-overs cricket.\n",
        "\"\"\"\n",
        "\n",
        "# 3) Split into chunks (simple defaults work fine to start)\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=60)\n",
        "docs = splitter.create_documents([raw])\n",
        "\n",
        "# 4) Build vector store + retriever\n",
        "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")   # fast & good\n",
        "vs = FAISS.from_documents(docs, embedding=emb)\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# 5) Define the RAG prompt\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a helpful assistant. Use ONLY the context to answer.\\n\\n\"\n",
        "    \"Context:\\n{context}\\n\\n\"\n",
        "    \"Question: {question}\\n\"\n",
        "    \"If the answer is not in the context, say you don't know.\\n\"\n",
        "    \"Answer:\"\n",
        ")\n",
        "\n",
        "# 6) Helper to format retrieved docs\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "# 7) LLM + chain\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0.2)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 8) Interactive Q&A\n",
        "print(\"\\nüí¨ Ask questions about your text. Type 'exit' to quit.\")\n",
        "while True:\n",
        "    q = input(\"Q: \").strip()\n",
        "    if q.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"üëã Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Optional: peek at retrieved chunks (uncomment to see)\n",
        "    # retrieved = retriever.get_relevant_documents(q)\n",
        "    # print(\"\\n[DEBUG] Retrieved context:\\n\", format_docs(retrieved), \"\\n\")\n",
        "\n",
        "    ans = rag_chain.invoke(q)\n",
        "    print(\"A:\", ans, \"\\n\")\n"
      ],
      "metadata": {
        "id": "MinUllFFykeP",
        "outputId": "8b9d713e-16af-4506-f80b-318c4319a60f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your reference text (or press Enter to use a default cricket sample):\n",
            "Data engineering with Databricks Databricks provides Lakeflow, an end-to-end data engineering solution that empowers data engineers, software developers, SQL developers, analysts, and data scientists to deliver high-quality data for downstream analytics, AI, and operational applications. Lakeflow is a unified solution for ingestion, transformation, and orchestration of your data, and includes Lakeflow Connect, Lakeflow Declarative Pipelines, and Lakeflow Jobs.  Lakeflow Connect Lakeflow Connect simplifies data ingestion with connectors to popular enterprise applications, databases, cloud storage, message buses, and local files. See Lakeflow Connect.  Feature  Description  Managed connectors  Managed connectors provide a simple UI and a configuration-based ingestion service with minimum operational overhead, without requiring you to use the underlying Lakeflow Declarative Pipelines APIs and infrastructure.  Standard connectors  Standard connectors provide the ability to access data from a wider range of data sources from within your Lakeflow Declarative Pipelines or other queries.  Lakeflow Declarative Pipelines Lakeflow Declarative Pipelines is a declarative framework that lowers the complexity of building and managing efficient batch and streaming data pipelines. Lakeflow Declarative Pipelines runs on the performance-optimized Databricks Runtime. In addition, Lakeflow Declarative Pipelines automatically orchestrates the execution of flows, sinks, streaming tables, and materialized views by encapsulating and running them as a pipeline. See Lakeflow Declarative Pipelines.  Feature  Description  Flows  Flows process data in Lakeflow Declarative Pipelines. The flows API uses the same DataFrame API as Apache Spark and Structured Streaming. A flow can write into streaming tables and sinks, such as a Kafka topic, using streaming semantics, or it can write to a materialized view using batch semantics.  Streaming tables  A streaming table is a Delta table with additional support for streaming or incremental data processing. It acts as a target for one or more flows in Lakeflow Declarative Pipelines.  Materialized views  A materialized view is a view with cached results for faster access. A materialized view acts as a target for Lakeflow Declarative Pipelines.  Sinks  Lakeflow Declarative Pipelines support external data sinks as targets. These sinks can include event streaming services, like Apache Kafka or Azure Event Hubs, as well as external tables managed by Unity Catalog.  Lakeflow Jobs Lakeflow Jobs provide reliable orchestration and production monitoring for any data and AI workload. A job can consist of one or more tasks that run notebooks, pipelines, managed connectors, SQL queries, machine learning training, and model deployment and inference. Jobs also support custom control flow logic, such as branching with if / else statements, and looping with for each statements. See Lakeflow Jobs.  Feature  Description  Jobs  Jobs are the primary resource for orchestration. They represent a process that you want to perform on a scheduled basis.  Tasks  A specific unit of work within a job. There are a variety of task types that give you a range of options that can be performed within a job.  Control flow in jobs  Control flow tasks allow you to control whether to run other tasks, or the order of tasks to run.  Databricks Runtime for Apache Spark The Databricks Runtime is a reliable and performance-optimized compute environment for running Spark workloads, including batch and streaming. Databricks Runtime provides Photon, a high-performance Databricks-native vectorized query engine, and various infrastructure optimizations like autoscaling. You can run your Spark and Structured Streaming workloads on the Databricks Runtime by building your Spark programs as notebooks, JARs, or Python wheels. See Databricks Runtime for Apache Spark.  Feature  Description  Apache Spark on Databricks  Spark is at the heart of the Databricks Data Intelligence Platform.  Structured Streaming  Structured Streaming is the Spark near real-time processing engine for streaming data.  What happened to Delta Live Tables (DLT)? The product formerly known as Delta Live Tables (DLT) is now Lakeflow Declarative Pipelines. There is no migration required to use Lakeflow Declarative Pipelines.  note There are still some references to the DLT name in Databricks. The classic SKUs for Lakeflow Declarative Pipelines still begin with DLT, and APIs with DLT in the name have not changed.  Additional resources Data engineering concepts describes data engineering concepts in Databricks. Delta Lake is the optimized storage layer that provides the foundation for tables in a lakehouse in Databricks. Data engineering best practices teaches you about best practices for data engineering in Databricks. Databricks notebooks are a popular tool for collaboration and development. Databricks SQL describes using SQL queries and BI tools in Databricks. Databricks Mosaic AI describes architecting machine learning solutions.\n",
            "\n",
            "üí¨ Ask questions about your text. Type 'exit' to quit.\n",
            "Q: what are DLT\n",
            "A: DLT refers to Delta Live Tables, a product that is now called Lakeflow Declarative Pipelines. \n",
            "\n",
            "Q: what are Lakeflow Jobs\n",
            "A: Lakeflow Jobs are the primary resource for orchestration. They represent a process that you want to perform on a scheduled basis. \n",
            "\n",
            "Q: who is MSD\n",
            "A: I don't know. \n",
            "\n",
            "Q: exit\n",
            "üëã Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Document Loaders 2. Splitters 3. Embeddings & Stored into Vector Store/Vector Databses (Chroma DB) 4. LLM"
      ],
      "metadata": {
        "id": "Ha71QprR5SYe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Text File / PDF / CSV / Web Based Application"
      ],
      "metadata": {
        "id": "Vz65SlX1zM2S"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Text Splitter (Huge Text)\n",
        "\n",
        "  # 1. Length Based 2. Text Strutured Based 3. Doc Structure Bases 4. Semantic Meaning Based"
      ],
      "metadata": {
        "id": "gUMizt8C5Q1o"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# End of the Notebook"
      ],
      "metadata": {
        "id": "rwoQeZX97f2n"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jPLyLtod8z69"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}