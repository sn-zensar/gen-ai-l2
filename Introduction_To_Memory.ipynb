{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Memory"
      ],
      "metadata": {
        "id": "ZG9HIVeAY5WH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "# Initialize model and prompt\n",
        "llm = ChatOllama(model=\"llama3\", temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant that answers questions about {topic}.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "# Session-level memory\n",
        "conversation_topic = \"\"\n",
        "conversation_history = []\n",
        "\n",
        "def format_history(history):\n",
        "    if not history:\n",
        "        return \"No previous Q&A.\"\n",
        "    formatted = \"\"\n",
        "    for i, (q, a) in enumerate(history[-5:], start=1):\n",
        "        formatted += f\"Q{i}: {q}\\nA{i}: {a}\\n\"\n",
        "    return formatted.strip()\n",
        "\n",
        "def chat_interface(topic, question, history_json):\n",
        "    global conversation_topic, conversation_history\n",
        "\n",
        "    if topic != conversation_topic:\n",
        "        conversation_topic = topic\n",
        "        conversation_history = []\n",
        "\n",
        "    history_text = format_history(conversation_history)\n",
        "\n",
        "    # Compose full prompt chain\n",
        "    response = chain.invoke({\n",
        "        \"topic\": topic,\n",
        "        \"question\": question\n",
        "    })\n",
        "\n",
        "    answer = response.content\n",
        "    conversation_history.append((question, answer))\n",
        "    return answer, conversation_history\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"#Ask LLaMA3 Anything\")\n",
        "\n",
        "    topic_input = gr.Textbox(label=\"Topic\", placeholder=\"e.g. Space, History, Python\")\n",
        "    question_input = gr.Textbox(label=\"Your Question\", placeholder=\"Ask your question here...\")\n",
        "    chat_output = gr.Textbox(label=\"Answer\")\n",
        "    state = gr.State([])\n",
        "\n",
        "    submit_btn = gr.Button(\"Get Answer\")\n",
        "\n",
        "    submit_btn.click(\n",
        "        chat_interface,\n",
        "        inputs=[topic_input, question_input, state],\n",
        "        outputs=[chat_output, state]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "9exTvOL4GmxW",
        "outputId": "48a2affa-8fb2-445d-8624-8cfaab8b9bed"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cfc4d7be69c49c454e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cfc4d7be69c49c454e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI # Use ChatOpenAI from langchain_openai\n",
        "\n",
        "# Initialize model and prompt\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) # Use an appropriate OpenAI model\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant that answers questions about {topic}.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "# Session-level memory\n",
        "conversation_topic = \"\"\n",
        "conversation_history = []\n",
        "\n",
        "def format_history(history):\n",
        "    if not history:\n",
        "        return \"No previous Q&A.\"\n",
        "    formatted = \"\"\n",
        "    for i, (q, a) in enumerate(history[-5:], start=1):\n",
        "        formatted += f\"Q{i}: {q}\\nA{i}: {a}\\n\"\n",
        "    return formatted.strip()\n",
        "\n",
        "def chat_interface(topic, question, history_json):\n",
        "    global conversation_topic, conversation_history\n",
        "\n",
        "    if topic != conversation_topic:\n",
        "        conversation_topic = topic\n",
        "        conversation_history = []\n",
        "\n",
        "    history_text = format_history(conversation_history)\n",
        "\n",
        "    # Compose full prompt chain\n",
        "    response = chain.invoke({\n",
        "        \"topic\": topic,\n",
        "        \"question\": question\n",
        "    })\n",
        "\n",
        "    answer = response.content\n",
        "    conversation_history.append((question, answer))\n",
        "    return answer, conversation_history\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"#Ask OpenAI Anything\") # Update title to reflect OpenAI\n",
        "\n",
        "    topic_input = gr.Textbox(label=\"Topic\", placeholder=\"e.g. Space, History, Python\")\n",
        "    question_input = gr.Textbox(label=\"Your Question\", placeholder=\"Ask your question here...\")\n",
        "    chat_output = gr.Textbox(label=\"Answer\")\n",
        "    state = gr.State([])\n",
        "\n",
        "    submit_btn = gr.Button(\"Get Answer\")\n",
        "\n",
        "    submit_btn.click(\n",
        "        chat_interface,\n",
        "        inputs=[topic_input, question_input, state],\n",
        "        outputs=[chat_output, state]\n",
        "    )\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "TUS9X-DALxQh",
        "outputId": "cb5eae20-ea99-42b6-ce3c-4618b30d0b00"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://24153b4dd612b5bb33.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://24153b4dd612b5bb33.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfhIhDg1MHCj",
        "outputId": "b10654ac-18a7-4652-dedf-cf5b952e8582"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('What are types of  functions in Python', 'In Python, functions can be categorized into several types based on their characteristics and usage. Here are the main types of functions:\\n\\n1. **Built-in Functions**: These are functions that are provided by Python itself. Examples include `print()`, `len()`, `type()`, `max()`, `min()`, and many others.\\n\\n2. **User-defined Functions**: These are functions that you define yourself using the `def` keyword. They allow you to encapsulate code for reuse. For example:\\n   ```python\\n   def my_function():\\n       print(\"Hello, World!\")\\n   ```\\n\\n3. **Lambda Functions**: Also known as anonymous functions, these are small, unnamed functions defined using the `lambda` keyword. They can take any number of arguments but can only have one expression. For example:\\n   ```python\\n   square = lambda x: x * x\\n   ```\\n\\n4. **Recursive Functions**: These are functions that call themselves in order to solve a problem. They must have a base case to prevent infinite recursion. For example:\\n   ```python\\n   def factorial(n):\\n       if n == 0:\\n           return 1\\n       else:\\n           return n * factorial(n - 1)\\n   ```\\n\\n5. **Higher-order Functions**: These are functions that can take other functions as arguments or return them as results. Examples include `map()`, `filter()`, and `reduce()`.\\n\\n6. **Generator Functions**: These are functions that use the `yield` keyword to return an iterator. They allow you to iterate over a sequence of values without storing them all in memory at once. For example:\\n   ```python\\n   def count_up_to(n):\\n       count = 1\\n       while count <= n:\\n           yield count\\n           count += 1\\n   ```\\n\\n7. **Coroutine Functions**: These are a more advanced type of generator that can pause and resume execution, allowing for asynchronous programming. They are defined using the `async def` syntax and use `await` to yield control.\\n\\n8. **Static Methods**: These are methods defined within a class that do not require an instance of the class to be called. They are defined using the `@staticmethod` decorator.\\n\\n9. **Class Methods**: These are methods that receive the class as the first argument instead of an instance. They are defined using the `@classmethod` decorator.\\n\\n10. **Instance Methods**: These are the most common type of methods in classes, which operate on an instance of the class and have access to its attributes. They take `self` as the first parameter.\\n\\nEach of these function types serves different purposes and can be used in various contexts to achieve specific programming goals.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatPromptTemplates + Chains + Memory (To store)"
      ],
      "metadata": {
        "id": "w4IQkQrKOD6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disks, RAM, Databases (RDBMS / NoSQL DBs)\n"
      ],
      "metadata": {
        "id": "NYTSsozYPsKk"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat_History - Store in Memory\n",
        "    # list, vector store, vector databases  => Memory"
      ],
      "metadata": {
        "id": "QbysVwDEP_PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory Components\n",
        "\n",
        "# 1 Buffer Memory\n",
        "# Complete Chat History, Partial Chat History"
      ],
      "metadata": {
        "id": "YuzZ4Lf-SKzF"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1 - Memory"
      ],
      "metadata": {
        "id": "uUKglkToSlKE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI # Import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1) # Use ChatOpenAI with an appropriate model\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "conversation = ConversationChain(llm=llm, memory=memory)\n",
        "\n",
        "print(conversation.invoke(\"Hello, I'm Sandip.\"))\n",
        "print(conversation.invoke(\"What did I just say?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHw6IYYmStWP",
        "outputId": "dde27c2b-4a45-4e14-edd0-170cccd30b74"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': \"Hello, I'm Sandip.\", 'history': '', 'response': \"Hello, Sandip! It's great to meet you. How's your day going so far?\"}\n",
            "{'input': 'What did I just say?', 'history': \"Human: Hello, I'm Sandip.\\nAI: Hello, Sandip! It's great to meet you. How's your day going so far?\", 'response': \"You just introduced yourself as Sandip! It's nice to know your name. Is there anything specific you'd like to talk about today?\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create a ConversationChain with the LLM and memory\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "\n",
        "print(\"Start chatting with the AI. Type 'exit' or 'quit' to end the conversation.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in ['exit', 'quit']:\n",
        "        break\n",
        "    try:\n",
        "        response = conversation.invoke(user_input)\n",
        "        print(\"Bot:\", response['response'])\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"Conversation ended.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIp4v0e9UEyS",
        "outputId": "8d48d0bd-d7e5-407a-b489-dea8843732fc"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start chatting with the AI. Type 'exit' or 'quit' to end the conversation.\n",
            "You: Hello I am Sandip\n",
            "Bot: Hello Sandip! It's great to meet you! How are you doing today? If there's anything specific you'd like to chat about or any questions you have, feel free to let me know!\n",
            "You: I am from Pune Maharashtra\n",
            "Bot: That's wonderful, Sandip! Pune is such a vibrant city with a rich cultural heritage and a fantastic blend of tradition and modernity. Did you know that it’s often referred to as the \"Oxford of the East\" because of its numerous educational institutions? There are also some beautiful historical sites like the Aga Khan Palace and Shaniwar Wada. What do you love most about living in Pune?\n",
            "You: I am a technology evangalist\n",
            "Bot: That's fantastic, Sandip! As a technology evangelist, you must be deeply involved in exploring and promoting the latest trends and innovations in the tech world. Whether it's artificial intelligence, blockchain, cloud computing, or the Internet of Things, there's always something exciting happening. Do you focus on any particular area of technology, or do you explore a wide range of topics? Also, I'd love to hear about any recent projects or initiatives you've been involved in!\n",
            "You: I want to know brief about gen-ai\n",
            "Bot: Generative AI, often abbreviated as Gen-AI, refers to a category of artificial intelligence that is designed to create new content based on the data it has been trained on. This can include generating text, images, music, and even videos. One of the most notable examples of generative AI in recent years is the development of large language models, such as OpenAI's GPT series, which can produce human-like text and engage in conversations.\n",
            "\n",
            "Generative AI operates using various techniques, including neural networks, which are inspired by the human brain's structure. These models learn patterns from vast amounts of data, allowing them to generate coherent and contextually relevant outputs.\n",
            "\n",
            "In addition to text generation, generative AI has applications in fields such as art creation, game development, and even drug discovery. For instance, AI can create unique artwork or assist in designing characters for video games. The technology is rapidly evolving, leading to exciting possibilities, but it also brings ethical considerations and challenges, such as misinformation and copyright issues.\n",
            "\n",
            "Is there a specific aspect of generative AI you’re interested in exploring further, or any questions you have about its applications?\n",
            "You: who is MS Dhoni\n",
            "Bot: Mahendra Singh Dhoni, commonly known as MS Dhoni, is one of the most celebrated cricketers in the history of Indian cricket. Born on July 7, 1981, in Ranchi, Jharkhand, Dhoni is renowned for his exceptional leadership skills, calm demeanor, and remarkable wicketkeeping abilities. \n",
            "\n",
            "He captained the Indian national team to significant victories, including the ICC T20 World Cup in 2007 and the ICC Cricket World Cup in 2011, making him the first captain to win all three major ICC trophies (T20 World Cup, ODI World Cup, and ICC Champions Trophy). Dhoni is also known for his explosive batting style, particularly in limited-overs formats, where he has played numerous match-winning innings.\n",
            "\n",
            "In the Indian Premier League (IPL), Dhoni is the captain of the Chennai Super Kings (CSK) and has led the team to multiple championships, further solidifying his legacy in the cricketing world. Off the field, he is known for his humble personality and philanthropic efforts. Are you a fan of cricket, or do you follow MS Dhoni's career closely?\n",
            "You: Who is SR Tendulkar\n",
            "Bot: Sachin Ramesh Tendulkar, commonly known as Sachin Tendulkar, is widely regarded as one of the greatest cricketers of all time. Born on April 24, 1973, in Mumbai, India, he made his debut for the Indian national team at the tender age of 16, becoming the youngest player to play in Test cricket at that time. Over his illustrious career, which spanned 24 years, Tendulkar set numerous records, many of which still stand today.\n",
            "\n",
            "He is the highest run-scorer in the history of both Test and One Day International (ODI) cricket, with over 34,000 runs across formats and 100 international centuries to his name. His technique, consistency, and ability to perform under pressure have earned him a massive fan following around the world.\n",
            "\n",
            "Tendulkar was also a key figure in India's victories, including the ICC Cricket World Cup in 2011, where he played a pivotal role in the team's success. He is known for his humility and sportsmanship, both on and off the field. After retiring from international cricket in 2013, he has continued to inspire young cricketers and remains active in various philanthropic efforts.\n",
            "\n",
            "Are you a fan of Sachin Tendulkar or cricket in general? What do you admire most about his career?\n",
            "You: What all did I ask you before\n",
            "Bot: You asked me about generative AI, MS Dhoni, and Sachin Tendulkar. We discussed what generative AI is, along with its applications, and then we talked about MS Dhoni's achievements in cricket, followed by an overview of Sachin Tendulkar's legendary career and records. If there's anything specific you'd like to revisit or explore further, just let me know!\n",
            "You: share some interesting places to visit in my city\n",
            "Bot: Sure, Sandip! Pune is filled with interesting places to explore. Here are some highlights:\n",
            "\n",
            "1. **Aga Khan Palace**: This historical landmark, built in 1892, is not only beautiful but also significant in Indian history. It served as a prison for Mahatma Gandhi and his followers during the freedom struggle. The palace is surrounded by lush gardens, making it a peaceful spot to visit.\n",
            "\n",
            "2. **Shaniwar Wada**: A fortification built in the 18th century, Shaniwar Wada was the seat of the Peshwas of the Maratha Empire. Its impressive architecture and rich history, including tales of intrigue and valor, make it a must-visit. The sound and light show in the evening is particularly captivating!\n",
            "\n",
            "3. **Sinhagad Fort**: Located about 30 kilometers from the city, this fort is steeped in history and offers stunning views of the surrounding landscape. It's a popular trekking destination and is also known for its delicious local snacks, especially the famous \"kanda bhaji\" (onion fritters).\n",
            "\n",
            "4. **Osho Ashram**: Located in Koregaon Park, this meditation resort offers a serene environment for those looking to relax and rejuvenate. It's associated with the spiritual leader Osho and features beautiful gardens and various wellness programs.\n",
            "\n",
            "5. **Raja Dinkar Kelkar Museum**: This museum showcases a vast collection of artifacts, including traditional Indian art, crafts, and everyday items from the past. It provides a fascinating glimpse into Indian culture and history.\n",
            "\n",
            "6. **Pune Okayama Friendship Garden**: Inspired by the Korakuen Garden in Japan, this is a beautiful Japanese-style garden, perfect for a peaceful stroll. The serene environment and lush greenery make it a lovely spot for relaxation.\n",
            "\n",
            "7. **Khadakwasla Dam**: A great place for picnics, especially during the monsoon season when the dam is full. The scenic views and the surrounding hills make it a popular outing spot for locals.\n",
            "\n",
            "8. **Fergusson College**: An iconic educational institution, it's known for its beautiful campus and rich heritage. Even if you're not a student, wandering around the campus can be quite enjoyable.\n",
            "\n",
            "These are just a few of the many wonderful places in Pune! Do any of these catch your interest, or is there a particular type of place you're looking for?\n",
            "You: exit\n",
            "Conversation ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Initialize the ChatOpenAI model and memory\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "\n",
        "def chat_interface(user_input):\n",
        "    \"\"\"Handles user input and returns the AI's response.\"\"\"\n",
        "    try:\n",
        "        response = conversation.invoke(user_input)\n",
        "        return response['response']\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Chat with Memory\")\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(label=\"Your Message\")\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        bot_message = chat_interface(message)\n",
        "        chat_history.append((message, bot_message))\n",
        "        return \"\", chat_history\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "IGCSjHx4UXn6",
        "outputId": "2d879140-af44-403c-b85f-c07b9aaeedd8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1680475894.py:24: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ed75b72029d651afaf.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ed75b72029d651afaf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory # Import ConversationBufferWindowMemory\n",
        "\n",
        "# Initialize the ChatOpenAI model and memory\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "memory = ConversationBufferWindowMemory(k=3) # Use ConversationBufferWindowMemory with k=2\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "\n",
        "def chat_interface(user_input):\n",
        "    \"\"\"Handles user input and returns the AI's response.\"\"\"\n",
        "    try:\n",
        "        response = conversation.invoke(user_input)\n",
        "        return response['response']\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Chat with Memory (Last 2 Chats)\") # Update title\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(label=\"Your Message\")\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        bot_message = chat_interface(message)\n",
        "        chat_history.append((message, bot_message))\n",
        "        return \"\", chat_history\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "sCzrturCV1FQ",
        "outputId": "8a6d8e6c-b1e6-4674-f0ae-91825f7cf044"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2592228887.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=3) # Use ConversationBufferWindowMemory with k=2\n",
            "/tmp/ipython-input-2592228887.py:24: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://060373c4cd2410b718.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://060373c4cd2410b718.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# End of the Notebook"
      ],
      "metadata": {
        "id": "HGH0L8dzX11p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}