{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Langchain Components:\n",
        "    # 1 Models\n",
        "    # 2 Prompts\n",
        "    # 3 Chains\n",
        "    # 4 Memory\n",
        "              # 1 Conversation Buffer Memory (Full/Entire Conversation History)\n",
        "              # 2 Conversation Buffer Window Memory (Store last k interactions with llm)\n",
        "              # 3 Conversation Summary Memory (Entire conversation Summarization)\n",
        "              # 4 Conversation Token Buffer Memory\n",
        "              # 5 VectorStore Retriever Memory (Vector Databases) (Vector Stores/ Text Splitters/ Document Loaders)\n",
        "\n",
        "    # 5 Vectors\n",
        "    # 6 RAG"
      ],
      "metadata": {
        "id": "Ex0Jhx4984s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Conversation Buffer Memory"
      ],
      "metadata": {
        "id": "ECd9sHxg-xvB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== SETUP (run first in a fresh Colab) =====\n",
        "!pip -q install langchain langchain-openai langchain-community grandalf\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.runnables import (\n",
        "    RunnableLambda, RunnableSequence, RunnableParallel\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "print(\"Setup complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGlPhU-PASh1",
        "outputId": "8115043e-3f32-4a8a-f8e2-2efb79b1a6b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/2.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSetup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2n4TtHc_4IS",
        "outputId": "813b596f-a655-454b-c1c1-35d884d6a1aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.3.74)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.100.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain-openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.31-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create a ConversationChain with the LLM and memory\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "\n",
        "print(\"Start chatting with the AI. Type 'exit' or 'quit' to end the conversation.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in ['exit', 'quit']:\n",
        "        break\n",
        "    try:\n",
        "        response = conversation.invoke(user_input)\n",
        "        print(\"Bot:\", response['response'])\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"Conversation ended.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1ofiDSV_zEN",
        "outputId": "f330f6e9-621b-4757-bdf7-1b8360cbbb34"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start chatting with the AI. Type 'exit' or 'quit' to end the conversation.\n",
            "You: where is he from\n",
            "Bot: AI: Could you provide a bit more context? Are you asking about a specific person or a character from a story? If you give me a name or some details, I might be able to help you out!\n",
            "You: where is Sachin Tendular from\n",
            "Bot: Sachin Tendulkar is from India! He was born on April 24, 1973, in Mumbai, which is one of the largest and most vibrant cities in the country. Tendulkar is widely regarded as one of the greatest cricketers of all time, having represented India in international cricket for over two decades. He made his debut at just 16 years old and went on to break numerous records, including being the highest run-scorer in both Test and One Day International cricket. If you have more questions about him or his career, feel free to ask!\n",
            "You: what are key statistics about his career\n",
            "Bot: Sachin Tendulkar had an illustrious cricket career that spanned 24 years, and he amassed some remarkable statistics. Here are some key highlights:\n",
            "\n",
            "1. **Test Matches**: \n",
            "   - Matches: 463\n",
            "   - Runs: 15,921\n",
            "   - Average: 53.78\n",
            "   - Centuries: 51\n",
            "   - Half-centuries: 68\n",
            "   - Highest Score: 248*\n",
            "\n",
            "2. **One Day Internationals (ODIs)**:\n",
            "   - Matches: 463\n",
            "   - Runs: 18,426\n",
            "   - Average: 44.83\n",
            "   - Centuries: 49\n",
            "   - Half-centuries: 96\n",
            "   - Highest Score: 200* (He was the first player to score a double century in ODIs)\n",
            "\n",
            "3. **T20 Internationals**:\n",
            "   - Matches: 1\n",
            "   - Runs: 10\n",
            "   - Average: 10.00\n",
            "   - Centuries: 0\n",
            "   - Half-centuries: 0\n",
            "   - Highest Score: 10\n",
            "\n",
            "4. **Career Highlights**:\n",
            "   - He is the only player to have scored 100 international centuries.\n",
            "   - Tendulkar played in six Cricket World Cups, and he was part of the Indian team that won the tournament in 2011.\n",
            "   - He holds the record for the most runs in World Cup history.\n",
            "\n",
            "5. **Awards and Honors**:\n",
            "   - He was awarded the Bharat Ratna, India's highest civilian award, in 2014.\n",
            "   - He received numerous accolades throughout his career, including the Rajiv Gandhi Khel Ratna award, the Padma Bhushan, and the Padma Vibhushan.\n",
            "\n",
            "These statistics reflect just how monumental his impact was on the game of cricket and his status as a national icon in India. If you want to know more about his specific matches or records, feel free to ask!\n",
            "You: which is the capital of India\n",
            "Bot: The capital of India is New Delhi. It serves as the seat of the government and is part of the larger National Capital Territory of Delhi. New Delhi is known for its historical landmarks, government buildings, and vibrant culture. It was officially designated as the capital in 1931, during British rule, and has since become a hub for political, cultural, and economic activities in India. If you're curious about specific places to visit in New Delhi or its history, just let me know!\n",
            "You: what are the news headlines today from India\n",
            "Bot: I'm sorry, but I don't have access to real-time news updates or current headlines as my training only includes information up until October 2023. However, you can easily check the latest news from India by visiting reputable news websites or using news apps. If you're interested in specific topics or events from before that date, I'd be happy to help with that!\n",
            "You: give statistics for God If Cricket \n",
            "Bot: It seems like you're referring to Sachin Tendulkar when you mention \"God of Cricket,\" a title often attributed to him due to his extraordinary contributions to the sport. Here are some statistics that highlight his remarkable career:\n",
            "\n",
            "1. **Test Matches**: \n",
            "   - Matches: 463\n",
            "   - Runs: 15,921\n",
            "   - Batting Average: 53.78\n",
            "   - Centuries: 51\n",
            "   - Half-centuries: 68\n",
            "   - Highest Score: 248*\n",
            "\n",
            "2. **One Day Internationals (ODIs)**:\n",
            "   - Matches: 463\n",
            "   - Runs: 18,426\n",
            "   - Batting Average: 44.83\n",
            "   - Centuries: 49\n",
            "   - Half-centuries: 96\n",
            "   - Highest Score: 200* (He was the first player to score a double century in ODIs)\n",
            "\n",
            "3. **T20 Internationals**:\n",
            "   - Matches: 1\n",
            "   - Runs: 10\n",
            "   - Batting Average: 10.00\n",
            "   - Centuries: 0\n",
            "   - Half-centuries: 0\n",
            "   - Highest Score: 10\n",
            "\n",
            "4. **Career Achievements**:\n",
            "   - First player to score 100 international centuries.\n",
            "   - Part of the Indian team that won the ICC Cricket World Cup in 2011.\n",
            "   - Holds the record for the most runs in both Tests and ODIs.\n",
            "\n",
            "5. **Awards and Honors**:\n",
            "   - Bharat Ratna, India's highest civilian award, in 2014.\n",
            "   - Rajiv Gandhi Khel Ratna award, Padma Bhushan, and Padma Vibhushan.\n",
            "\n",
            "Sachin's impact on cricket is immense, and he continues to be an inspiration for millions of fans around the world. If there's anything else you'd like to know about him or cricket in general, feel free to ask!\n",
            "You: quit\n",
            "Conversation ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YTkm0qHaPiYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2 - Storing last few interactions"
      ],
      "metadata": {
        "id": "aJJ4AmXKDX7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example of Conversation Buffer Window Memory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "\n",
        "# Initialize memory with a window size of 3\n",
        "memory = ConversationBufferWindowMemory(k=3)\n",
        "\n",
        "# Create a ConversationChain with the LLM and memory\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "\n",
        "print(\"Start chatting with the AI. This conversation will remember the last 3 interactions. Type 'exit' or 'quit' to end the conversation.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in ['exit', 'quit']:\n",
        "        break\n",
        "    try:\n",
        "        response = conversation.invoke(user_input)\n",
        "        print(\"Bot:\", response['response'])\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"Conversation ended.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT-FIknlABB5",
        "outputId": "408bf6c2-2529-40c1-f260-68b5c93fc88c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start chatting with the AI. This conversation will remember the last 3 interactions. Type 'exit' or 'quit' to end the conversation.\n",
            "You: what is gen ai\n",
            "Bot: Gen AI, short for Generative Artificial Intelligence, refers to a class of AI systems designed to generate content, such as text, images, music, and even videos, based on the input they receive. These systems are trained on vast datasets and use deep learning techniques to understand patterns and create new outputs that mimic human-like creativity. Popular examples include language models like me, ChatGPT, which can generate coherent and contextually relevant text, and image generation models like DALL-E, which can create images from textual descriptions. Generative AI has applications in various fields, including entertainment, education, marketing, and more! If you have more questions about it or want to know specific examples, feel free to ask!\n",
            "You: quit\n",
            "Conversation ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Conversation Token Buffer Memory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "\n",
        "# Initialize memory with a token limit\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
        "\n",
        "# Create a ConversationChain with the LLM and memory\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "\n",
        "print(\"Start chatting with the AI. This conversation will remember based on a token limit. Type 'exit' or 'quit' to end the conversation.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in ['exit', 'quit']:\n",
        "        break\n",
        "    try:\n",
        "        response = conversation.invoke(user_input)\n",
        "        print(\"Bot:\", response['response'])\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"Conversation ended.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7ny8h2mGfjS",
        "outputId": "b9f23ef3-ec73-4281-ba2d-f5e692e1a289"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3524034976.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start chatting with the AI. This conversation will remember based on a token limit. Type 'exit' or 'quit' to end the conversation.\n",
            "You: explain in detail about gen-ai\n",
            "Bot: Generative AI, often referred to as Gen-AI, is a subset of artificial intelligence that focuses on creating new content based on input data. This technology has gained significant traction in recent years and encompasses various applications, including text, images, music, and even video generation.\n",
            "\n",
            "### Key Components of Generative AI:\n",
            "\n",
            "1. **Machine Learning Models**: At the core of generative AI are machine learning models, particularly those based on deep learning techniques. Two popular architectures used are:\n",
            "   - **Generative Adversarial Networks (GANs)**: These consist of two neural networks—the generator and the discriminator—working against each other. The generator creates new data instances, while the discriminator evaluates them against real data, helping the generator improve over time.\n",
            "   - **Variational Autoencoders (VAEs)**: These encode input data into a compressed format and then decode it to generate new samples, providing a probabilistic approach to data generation.\n",
            "\n",
            "2. **Training Data**: Generative AI models require vast amounts of training data to learn patterns and features. For instance, a model generating text might be trained on diverse text corpora, while an image-generating model might use thousands of images from various categories.\n",
            "\n",
            "3. **Applications**:\n",
            "   - **Text Generation**: Models like OpenAI's GPT (Generative Pre-trained Transformer) can generate coherent text, write stories, answer questions, and even engage in conversation, as we're doing now!\n",
            "   - **Image Generation**: Tools like DALL-E and Midjourney can create images from textual descriptions, allowing users to visualize concepts or ideas that may not exist.\n",
            "   - **Music Composition**: AI can also compose music, generating melodies and harmonies based on styles or genres provided by users.\n",
            "   - **Video Generation**: Emerging technologies are beginning to explore video generation, creating short clips or animations based on sequences of images or scripts.\n",
            "\n",
            "4. **Ethical Considerations**: The rise of generative AI also brings ethical questions, such as the potential for creating deepfakes, misinformation, and the implications of copyright when AI generates new content based on existing works.\n",
            "\n",
            "5. **Future Prospects**: As technology advances, generative AI is expected to become even more sophisticated, with improvements in realism, creativity, and interactivity. This could lead to enhanced tools for various industries, including entertainment, advertising, education, and more.\n",
            "\n",
            "Overall, generative AI represents a significant leap forward in how machines can understand and create content, offering exciting possibilities while also challenging us to navigate its ethical landscape. If you have any specific questions about a particular aspect of generative AI, feel free to ask!\n",
            "You: exaplain in detail agentic ai \n",
            "Bot: Agentic AI refers to artificial intelligence systems that possess a form of agency, meaning they can make decisions, take actions, and potentially influence their environment in a way that resembles human autonomy. Here are some detailed aspects of agentic AI:\n",
            "\n",
            "1. **Definition of Agency**: In the context of AI, agency refers to the ability of a system to act independently and make choices based on its programming and the data it has access to. This contrasts with non-agentic AI, which follows predefined rules or scripts without the ability to adapt or decide on its own.\n",
            "\n",
            "2. **Decision-Making**: Agentic AI systems utilize complex algorithms, often involving machine learning and deep learning, to analyze data and make decisions. They can weigh options, predict outcomes, and choose actions that align with specific goals or objectives.\n",
            "\n",
            "3. **Autonomy Levels**: Agentic AI can operate at various levels of autonomy. Some systems might require human oversight for critical decisions (like in healthcare or autonomous vehicles), while others might function independently in less sensitive scenarios (like chatbots or recommendation systems).\n",
            "\n",
            "4. **Examples in Practice**: \n",
            "   - **Autonomous Vehicles**: These AI systems must make real-time decisions based on sensor data, navigating roads, avoiding obstacles, and responding to unpredictable situations.\n",
            "   - **Personal Assistants**: Virtual assistants like Siri or Alexa can learn user preferences over time, adapting their responses and suggestions based on user interactions, showcasing a degree of agency.\n",
            "   - **Robotics**: Robots that operate in environments like warehouses can make decisions about how to optimize their paths, manage tasks, and interact with other robots or humans.\n",
            "\n",
            "5. **Ethical Considerations**: The concept of agentic AI raises important ethical questions. For instance, if an AI system makes a harmful decision, who is responsible? There are concerns about accountability, transparency, and the potential for biases in decision-making processes.\n",
            "\n",
            "6. **Future Directions**: As AI technology advances, the development of more sophisticated agentic AI systems is anticipated. This includes enhancing their ability to understand context, learn from fewer examples, and engage in more complex interactions with humans and environments.\n",
            "\n",
            "In summary, agentic AI represents a significant leap toward creating systems that can operate with a degree of independence, raising both exciting possibilities and complex ethical challenges. If you have any specific aspects of agentic AI you'd like to explore further, feel free to ask!\n",
            "You: what all did we discuss\n",
            "Bot: We haven’t had a detailed conversation yet in this session, but I'm here to chat about anything on your mind! We can talk about a variety of topics like technology, science, hobbies, or even your favorite books and movies. What would you like to discuss?\n",
            "You: quit\n",
            "Conversation ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Conversation Token Buffer Memory with tiktoken token counting\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "import tiktoken\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "\n",
        "# Initialize memory with a token limit\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
        "\n",
        "# Create a ConversationChain with the LLM and memory\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "\n",
        "# Load the encoding for the model to count tokens\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "\n",
        "print(\"Start chatting with the AI. This conversation will remember based on a token limit.\")\n",
        "print(\"The current token count will be displayed after each interaction.\")\n",
        "print(\"Type 'exit' or 'quit' to end the conversation.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in ['exit', 'quit']:\n",
        "        break\n",
        "    try:\n",
        "        response = conversation.invoke(user_input)\n",
        "        print(\"Bot:\", response['response'])\n",
        "\n",
        "        # Get the current memory string\n",
        "        memory_string = memory.load_memory_variables({})['history']\n",
        "\n",
        "        # Count tokens in the memory string\n",
        "        num_tokens_in_memory = len(encoding.encode(memory_string, disallowed_special=()))\n",
        "\n",
        "        print(f\"\\nCurrent token count in memory: {num_tokens_in_memory}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"Conversation ended.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er6lIhIkHtEa",
        "outputId": "9ab38c54-6a78-4b87-8b04-4ec53a27f618"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start chatting with the AI. This conversation will remember based on a token limit.\n",
            "The current token count will be displayed after each interaction.\n",
            "Type 'exit' or 'quit' to end the conversation.\n",
            "You: explain in detail about agentic ai\n",
            "Bot: Agentic AI refers to artificial intelligence systems that possess a degree of autonomy and decision-making capability, allowing them to act independently within certain defined parameters. Unlike traditional AI models that simply follow pre-programmed rules or respond to specific inputs, agentic AI can analyze situations, make decisions, and take actions based on its understanding of the environment and its goals.\n",
            "\n",
            "Here are some key characteristics and concepts associated with agentic AI:\n",
            "\n",
            "1. **Autonomy**: Agentic AI can operate independently without constant human oversight. This means it can assess a situation, learn from experiences, and make choices to achieve its objectives.\n",
            "\n",
            "2. **Goal-oriented behavior**: These AI systems are designed with specific objectives in mind. They can prioritize actions based on their predefined goals, which can be dynamic and adapt as circumstances change.\n",
            "\n",
            "3. **Learning and adaptation**: Agentic AI often employs machine learning techniques, enabling it to improve its performance over time by learning from data and experiences. This can include reinforcement learning, where the AI receives feedback based on its actions and adjusts its strategies accordingly.\n",
            "\n",
            "4. **Environment interaction**: Agentic AI interacts with its environment, which can be physical (like robots) or virtual (such as software agents). This interaction allows it to gather information, assess conditions, and make informed decisions.\n",
            "\n",
            "5. **Ethical considerations**: The development of agentic AI raises important ethical questions regarding responsibility, safety, and control. For instance, if an agentic AI makes a decision that leads to negative consequences, it raises the question of accountability—should the developers, the users, or the AI itself be held responsible?\n",
            "\n",
            "6. **Applications**: Agentic AI has potential applications across various fields, including autonomous vehicles, robotics, personalized digital assistants, and even complex systems like finance or healthcare management. In these domains, the ability to make independent decisions can lead to increased efficiency and effectiveness.\n",
            "\n",
            "7. **Risks and challenges**: While agentic AI holds promise, it also poses risks. The more autonomous an AI system becomes, the harder it may be to predict its actions. This unpredictability can lead to unintended consequences, making it crucial to establish robust safety measures and ethical frameworks.\n",
            "\n",
            "Agentic AI is a rapidly evolving field within artificial intelligence research, and its implications for society and technology are still being explored. As researchers continue to develop more sophisticated systems, ongoing discussions about governance, ethics, and control will be vital to ensure that these technologies are used beneficially and safely.\n",
            "\n",
            "Current token count in memory: 0\n",
            "\n",
            "You: give 5 interesting facts about gen-ai, just give bullet point with single words only, do nit give any sentence\n",
            "Bot: - Creativity  \n",
            "- Adaptability  \n",
            "- Language  \n",
            "- Automation  \n",
            "- Ethics  \n",
            "\n",
            "Current token count in memory: 44\n",
            "\n",
            "You: what did I ask before\n",
            "Bot: You asked for 5 interesting facts about gen-ai in the form of bullet points with single words only.\n",
            "\n",
            "Current token count in memory: 75\n",
            "\n",
            "You: quit\n",
            "Conversation ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using tiktoken to count tokens\n",
        "import tiktoken\n",
        "\n",
        "# Load the encoding for a specific model (e.g., \"gpt-4o-mini\")\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "\n",
        "# Example string\n",
        "text = \"This is a sample sentence to count tokens.\"\n",
        "\n",
        "# Encode the text to get tokens\n",
        "tokens = encoding.encode(text)\n",
        "\n",
        "# Get the number of tokens\n",
        "num_tokens = len(tokens)\n",
        "\n",
        "print(f\"The text: '{text}'\")\n",
        "print(f\"The tokens: {tokens}\")\n",
        "print(f\"Number of tokens: {num_tokens}\")\n",
        "\n",
        "# You can also directly count tokens without getting the tokens list\n",
        "num_tokens_direct = encoding.encode(text, disallowed_special=())\n",
        "print(f\"Number of tokens (direct): {len(num_tokens_direct)}\")"
      ],
      "metadata": {
        "id": "CVGqGMxaGfWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Conversation Summary Memory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "\n",
        "# Initialize memory with a summary LLM\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "\n",
        "# Create a ConversationChain with the LLM and memory\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
        "\n",
        "print(\"Start chatting with the AI. This conversation will summarize the interactions. Type 'exit' or 'quit' to end the conversation.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in ['exit', 'quit']:\n",
        "        break\n",
        "    try:\n",
        "        response = conversation.invoke(user_input)\n",
        "        print(\"Bot:\", response['response'])\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"Conversation ended.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoLKY_5BCZL-",
        "outputId": "cc3b08b4-f18a-493a-8d66-c1e130b1f0f7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start chatting with the AI. This conversation will summarize the interactions. Type 'exit' or 'quit' to end the conversation.\n",
            "You: hello\n",
            "Bot: Hello! How are you today? I'm here and ready to chat about anything on your mind. Whether it's a question, a topic of interest, or just a casual conversation, I'm all ears!\n",
            "You: quit\n",
            "Conversation ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import (\n",
        "    ConversationBufferMemory,\n",
        "    ConversationBufferWindowMemory,\n",
        "    ConversationSummaryMemory,\n",
        "    ConversationTokenBufferMemory,\n",
        ")\n",
        "import tiktoken\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "\n",
        "# --- Conversation Buffer Memory ---\n",
        "print(\"--- Conversation Buffer Memory Example ---\")\n",
        "buffer_memory = ConversationBufferMemory()\n",
        "buffer_conversation = ConversationChain(llm=llm, memory=buffer_memory, verbose=False)\n",
        "\n",
        "buffer_conversation.invoke(\"Hi there!\")\n",
        "buffer_conversation.invoke(\"What is your name?\")\n",
        "print(\"Buffer Memory History:\", buffer_memory.load_memory_variables({})['history'])\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- Conversation Buffer Window Memory ---\n",
        "print(\"--- Conversation Buffer Window Memory Example (k=2) ---\")\n",
        "window_memory = ConversationBufferWindowMemory(k=2)\n",
        "window_conversation = ConversationChain(llm=llm, memory=window_memory, verbose=False)\n",
        "\n",
        "window_conversation.invoke(\"Hi there!\")\n",
        "window_conversation.invoke(\"What is your name?\")\n",
        "window_conversation.invoke(\"What do you like to do?\")\n",
        "print(\"Window Memory History:\", window_memory.load_memory_variables({})['history'])\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- Conversation Summary Memory ---\n",
        "print(\"--- Conversation Summary Memory Example ---\")\n",
        "summary_memory = ConversationSummaryMemory(llm=llm)\n",
        "summary_conversation = ConversationChain(llm=llm, memory=summary_memory, verbose=False)\n",
        "\n",
        "summary_conversation.invoke(\"Hi there!\")\n",
        "summary_conversation.invoke(\"I'm working on a project about AI.\")\n",
        "summary_conversation.invoke(\"It's about natural language processing.\")\n",
        "print(\"Summary Memory History:\", summary_memory.load_memory_variables({})['history'])\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- Conversation Token Buffer Memory ---\n",
        "print(\"--- Conversation Token Buffer Memory Example (max_token_limit=50) ---\")\n",
        "token_buffer_memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
        "token_buffer_conversation = ConversationChain(llm=llm, memory=token_buffer_memory, verbose=False)\n",
        "\n",
        "token_buffer_conversation.invoke(\"This is a short message.\")\n",
        "token_buffer_conversation.invoke(\"This is a slightly longer message that might push the token limit.\")\n",
        "token_buffer_conversation.invoke(\"This is another message to see how the memory handles the limit.\")\n",
        "\n",
        "# Load the encoding for the model to count tokens\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "memory_string = token_buffer_memory.load_memory_variables({})['history']\n",
        "num_tokens_in_memory = len(encoding.encode(memory_string, disallowed_special=()))\n",
        "\n",
        "print(\"Token Buffer Memory History:\", memory_string)\n",
        "print(f\"Current token count in Token Buffer Memory: {num_tokens_in_memory}\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CNtVsrDFOVT",
        "outputId": "c918f681-94ea-4981-c59f-ea0f19680413"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Conversation Buffer Memory Example ---\n",
            "Buffer Memory History: Human: Hi there!\n",
            "AI: Hello! How are you doing today? I'm here to chat about anything on your mind—whether it's a question, a topic you're interested in, or just some friendly banter!\n",
            "Human: What is your name?\n",
            "AI: I don’t have a personal name like a human does, but you can call me AI or any name you prefer! I'm here to assist you with whatever you need. Is there something specific you’d like to talk about?\n",
            "------------------------------\n",
            "--- Conversation Buffer Window Memory Example (k=2) ---\n",
            "Window Memory History: Human: What is your name?\n",
            "AI: I don’t have a personal name like a human does, but you can call me AI or whatever you prefer! I’m here to assist you, so feel free to give me a name if you’d like. What would you like to chat about today?\n",
            "Human: What do you like to do?\n",
            "AI: As an AI, I don’t have personal likes or dislikes like a human does, but I really enjoy engaging in conversations and providing information! I love answering questions, sharing knowledge on a variety of topics, and helping people solve problems. If you have a specific interest or hobby, I’d love to discuss that with you! What do you enjoy doing?\n",
            "------------------------------\n",
            "--- Conversation Summary Memory Example ---\n",
            "Summary Memory History: The human greets the AI, and the AI responds warmly, offering to chat or share interesting facts. The human mentions working on a project about AI, and the AI expresses enthusiasm, asking about the specific aspect of AI the human is focusing on and offering to help with information. The human reveals the project is about natural language processing (NLP), to which the AI responds with excitement, explaining that NLP enables machines to understand human language and inquiring if the human is looking into specific applications like chatbots, sentiment analysis, or machine translation, while offering to share facts about algorithms, techniques, or recent breakthroughs in the field.\n",
            "------------------------------\n",
            "--- Conversation Token Buffer Memory Example (max_token_limit=50) ---\n",
            "Token Buffer Memory History: \n",
            "Current token count in Token Buffer Memory: 0\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPyCn5AhKUBs",
        "outputId": "c2a85405-3072-47b0-97bb-b3020e4f1013"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Conversation Buffer Memory Example ---\n",
            "Buffer Memory History: Human: Hi there!\n",
            "AI: Hello! How are you doing today? Anything exciting on your mind?\n",
            "Human: What is your name?\n",
            "AI: I’m known as ChatGPT! You can just call me that. I’m here to help answer your questions and chat about whatever you like. Do you have a favorite topic you want to discuss?\n",
            "------------------------------\n",
            "--- Conversation Buffer Window Memory Example (k=2) ---\n",
            "Window Memory History: Human: What is your name?\n",
            "AI: I don't have a personal name like humans do, but you can call me AI, Assistant, or even give me a fun nickname if you'd like! What do you prefer?\n",
            "Human: What do you like to do?\n",
            "AI: I enjoy engaging in conversations, answering questions, and helping you find information on a wide range of topics! I also love exploring creative ideas, sharing interesting facts, and assisting with problem-solving. What about you? What do you like to do in your free time?\n",
            "------------------------------\n",
            "--- Conversation Summary Memory Example ---\n",
            "Summary Memory History: The human greets the AI with \"Hi there!\" and the AI responds warmly, asking how the human is doing and offering to chat about any topic. The human mentions working on a project about AI, and the AI expresses excitement, asking what specific aspect of AI the human is focusing on and offering help with questions or ideas related to various areas of the field. The human specifies that the project is about natural language processing, to which the AI responds enthusiastically, noting the importance of NLP in AI interactions and inquiring if the human is focusing on specific applications like chatbots, sentiment analysis, or language translation, while offering further assistance.\n",
            "------------------------------\n",
            "--- Conversation Token Buffer Memory Example (max_token_limit=50) ---\n",
            "Token Buffer Memory History: \n",
            "Current token count in Token Buffer Memory: 0\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import (\n",
        "    ConversationBufferMemory,\n",
        "    ConversationBufferWindowMemory,\n",
        "    ConversationSummaryMemory,\n",
        "    ConversationTokenBufferMemory,\n",
        ")\n",
        "import tiktoken\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
        "\n",
        "# Initialize each memory type\n",
        "buffer_memory = ConversationBufferMemory()\n",
        "window_memory = ConversationBufferWindowMemory(k=2) # Window size of 2 for demonstration\n",
        "summary_memory = ConversationSummaryMemory(llm=llm)\n",
        "token_buffer_memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50) # Token limit of 50\n",
        "\n",
        "\n",
        "# Create ConversationChains for each memory type\n",
        "buffer_conversation = ConversationChain(llm=llm, memory=buffer_memory, verbose=False)\n",
        "window_conversation = ConversationChain(llm=llm, memory=window_memory, verbose=False)\n",
        "summary_conversation = ConversationChain(llm=llm, memory=summary_memory, verbose=False)\n",
        "token_buffer_conversation = ConversationChain(llm=llm, memory=token_buffer_memory, verbose=False)\n",
        "\n",
        "\n",
        "print(\"Start chatting with the AI. Your input will be processed by each memory type.\")\n",
        "print(\"Type 'exit' or 'quit' to end the conversation.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in ['exit', 'quit']:\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        print(\"\\n--- Processing with different memory types ---\")\n",
        "\n",
        "        # Process with Buffer Memory\n",
        "        buffer_response = buffer_conversation.invoke(user_input)\n",
        "        print(\"\\nBuffer Memory History:\", buffer_memory.load_memory_variables({})['history'])\n",
        "\n",
        "\n",
        "        # Process with Window Memory\n",
        "        window_response = window_conversation.invoke(user_input)\n",
        "        print(\"\\nWindow Memory History (k=2):\", window_memory.load_memory_variables({})['history'])\n",
        "\n",
        "\n",
        "        # Process with Summary Memory\n",
        "        summary_response = summary_conversation.invoke(user_input)\n",
        "        print(\"\\nSummary Memory History:\", summary_memory.load_memory_variables({})['history'])\n",
        "\n",
        "\n",
        "        # Process with Token Buffer Memory\n",
        "        token_buffer_response = token_buffer_conversation.invoke(user_input)\n",
        "        # Load the encoding for the model to count tokens\n",
        "        encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "        token_memory_string = token_buffer_memory.load_memory_variables({})['history']\n",
        "        num_tokens_in_token_memory = len(encoding.encode(token_memory_string, disallowed_special=()))\n",
        "        print(\"\\nToken Buffer Memory History (max_token_limit=50):\", token_memory_string)\n",
        "        print(f\"Current token count in Token Buffer Memory: {num_tokens_in_token_memory}\")\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"Conversation ended.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq2IP387KwM0",
        "outputId": "64aebf64-4a2d-45a9-ad0a-aea54ee99757"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start chatting with the AI. Your input will be processed by each memory type.\n",
            "Type 'exit' or 'quit' to end the conversation.\n",
            "You: hello I am human\n",
            "\n",
            "--- Processing with different memory types ---\n",
            "\n",
            "Buffer Memory History: Human: hello I am human\n",
            "AI: Hello, human! It's great to meet you. How's your day going so far? Anything interesting on your mind?\n",
            "\n",
            "Window Memory History (k=2): Human: hello I am human\n",
            "AI: Hello, human! It's great to meet you! How's your day going so far? If there's anything specific you'd like to chat about or any questions you have, feel free to ask!\n",
            "\n",
            "Summary Memory History: The human introduces themselves as human, and the AI responds warmly, stating that it is designed to chat and assist with various topics, and asks how the human's day is going.\n",
            "\n",
            "Token Buffer Memory History (max_token_limit=50): AI: Hello, Human! It's great to meet you! How's your day going so far? If there's anything specific you'd like to chat about or any questions you have, feel free to ask!\n",
            "Current token count in Token Buffer Memory: 40\n",
            "------------------------------\n",
            "You: Hello I am Sandip from Pune, explain in short about gen-ai\n",
            "\n",
            "--- Processing with different memory types ---\n",
            "\n",
            "Buffer Memory History: Human: hello I am human\n",
            "AI: Hello, human! It's great to meet you. How's your day going so far? Anything interesting on your mind?\n",
            "Human: Hello I am Sandip from Pune, explain in short about gen-ai\n",
            "AI: Hi Sandip! It's nice to meet you. Generative AI, or Gen-AI, refers to a type of artificial intelligence that can create new content, such as text, images, music, and more, based on the data it has been trained on. It uses algorithms and models, like neural networks, to understand patterns in the data and generate outputs that resemble human-created content. For example, models like GPT-3 and DALL-E can generate coherent text or create images from textual descriptions. It's a fascinating field that's rapidly evolving and has many applications in creative industries, customer service, and beyond. Do you have a specific aspect of generative AI you're curious about?\n",
            "\n",
            "Window Memory History (k=2): Human: hello I am human\n",
            "AI: Hello, human! It's great to meet you! How's your day going so far? If there's anything specific you'd like to chat about or any questions you have, feel free to ask!\n",
            "Human: Hello I am Sandip from Pune, explain in short about gen-ai\n",
            "AI: Hello, Sandip from Pune! Gen-AI, or Generative AI, refers to a class of artificial intelligence models designed to generate new content based on the data they have been trained on. This can include text, images, music, and more. One of the most well-known examples is language models like me, which can create human-like text based on prompts. Generative AI uses techniques like deep learning and neural networks to understand patterns in data and produce creative outputs. It's being used in various fields, from art and entertainment to business and education. If you have more specific questions about it, feel free to ask!\n",
            "\n",
            "Summary Memory History: The human introduces themselves as Sandip from Pune, and the AI responds warmly, explaining that it is designed to chat and assist with various topics. Sandip asks for a brief explanation of generative AI, and the AI explains that generative AI (Gen-AI) is a type of artificial intelligence that creates content like text, images, and music using algorithms and deep learning models. It mentions examples like GPT-3 and DALL-E, highlighting its applications in creative writing, art, coding, and customer service, and then asks how Sandip's day is going.\n",
            "\n",
            "Token Buffer Memory History (max_token_limit=50): \n",
            "Current token count in Token Buffer Memory: 0\n",
            "------------------------------\n",
            "You: who is Sachin Tendulkar\n",
            "\n",
            "--- Processing with different memory types ---\n",
            "\n",
            "Buffer Memory History: Human: hello I am human\n",
            "AI: Hello, human! It's great to meet you. How's your day going so far? Anything interesting on your mind?\n",
            "Human: Hello I am Sandip from Pune, explain in short about gen-ai\n",
            "AI: Hi Sandip! It's nice to meet you. Generative AI, or Gen-AI, refers to a type of artificial intelligence that can create new content, such as text, images, music, and more, based on the data it has been trained on. It uses algorithms and models, like neural networks, to understand patterns in the data and generate outputs that resemble human-created content. For example, models like GPT-3 and DALL-E can generate coherent text or create images from textual descriptions. It's a fascinating field that's rapidly evolving and has many applications in creative industries, customer service, and beyond. Do you have a specific aspect of generative AI you're curious about?\n",
            "Human: who is Sachin Tendulkar\n",
            "AI: Sachin Tendulkar is a former Indian cricketer and one of the greatest batsmen in the history of cricket. Born on April 24, 1973, in Mumbai, India, he made his debut for the Indian national team at the young age of 16, becoming the youngest player to play in a Test match. Over his illustrious career, which spanned 24 years, Tendulkar set numerous records, including being the highest run-scorer in both Test and One Day International (ODI) cricket. He scored 100 international centuries, a record that still stands. \n",
            "\n",
            "Tendulkar was known for his impeccable technique, incredible concentration, and ability to play a wide range of shots. He played a crucial role in many of India's victories and was part of the team that won the ICC Cricket World Cup in 2011. After retiring from cricket in 2013, he has been involved in various philanthropic activities and continues to inspire many aspiring cricketers around the world. Are you a cricket fan, or do you have a favorite player?\n",
            "\n",
            "Window Memory History (k=2): Human: Hello I am Sandip from Pune, explain in short about gen-ai\n",
            "AI: Hello, Sandip from Pune! Gen-AI, or Generative AI, refers to a class of artificial intelligence models designed to generate new content based on the data they have been trained on. This can include text, images, music, and more. One of the most well-known examples is language models like me, which can create human-like text based on prompts. Generative AI uses techniques like deep learning and neural networks to understand patterns in data and produce creative outputs. It's being used in various fields, from art and entertainment to business and education. If you have more specific questions about it, feel free to ask!\n",
            "Human: who is Sachin Tendulkar\n",
            "AI: Sachin Tendulkar is a former Indian cricketer widely regarded as one of the greatest batsmen in the history of cricket. Born on April 24, 1973, in Mumbai, India, he made his debut for the Indian national team at the young age of 16. Over his 24-year career, he set numerous records, including being the highest run-scorer in both Test and One Day International (ODI) cricket. Tendulkar was known for his exceptional technique, consistency, and ability to perform under pressure. He scored 100 international centuries, a feat that remains unmatched. After retiring in 2013, he has been involved in various philanthropic activities and continues to be a prominent figure in sports. Are you a cricket fan, Sandip?\n",
            "\n",
            "Summary Memory History: The human introduces themselves as Sandip from Pune, and the AI responds warmly, explaining that it is designed to chat and assist with various topics. Sandip asks for a brief explanation of generative AI, and the AI explains that generative AI (Gen-AI) is a type of artificial intelligence that creates content like text, images, and music using algorithms and deep learning models. It mentions examples like GPT-3 and DALL-E, highlighting its applications in creative writing, art, coding, and customer service, and then asks how Sandip's day is going. Sandip then asks about Sachin Tendulkar, and the AI describes him as a legendary Indian cricketer, regarded as one of the greatest batsmen in cricket history, who debuted at 16 and had a 24-year career filled with records, including being the highest run-scorer in Test and ODI formats. The AI also notes Tendulkar's sportsmanship and philanthropic efforts after retirement, and it inquires if Sandip is a cricket fan.\n",
            "\n",
            "Token Buffer Memory History (max_token_limit=50): \n",
            "Current token count in Token Buffer Memory: 0\n",
            "------------------------------\n",
            "You: quit\n",
            "Conversation ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import (\n",
        "    ConversationBufferMemory,\n",
        "    ConversationBufferWindowMemory,\n",
        "    ConversationSummaryMemory,\n",
        "    ConversationTokenBufferMemory,\n",
        ")\n",
        "import tiktoken\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
        "\n",
        "# Initialize each memory type\n",
        "buffer_memory = ConversationBufferMemory()\n",
        "window_memory = ConversationBufferWindowMemory(k=2) # Window size of 2 for demonstration\n",
        "summary_memory = ConversationSummaryMemory(llm=llm)\n",
        "token_buffer_memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50) # Token limit of 50\n",
        "\n",
        "\n",
        "# Create ConversationChains for each memory type\n",
        "buffer_conversation = ConversationChain(llm=llm, memory=buffer_memory, verbose=False)\n",
        "window_conversation = ConversationChain(llm=llm, memory=window_memory, verbose=False)\n",
        "summary_conversation = ConversationChain(llm=llm, memory=summary_memory, verbose=False)\n",
        "token_buffer_conversation = ConversationChain(llm=llm, memory=token_buffer_memory, verbose=False)\n",
        "\n",
        "\n",
        "print(\"Start chatting with the AI. Your input will be processed by each memory type.\")\n",
        "print(\"Type 'exit' or 'quit' to end the conversation and see the final memory states.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in ['exit', 'quit']:\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        # Process with each memory type\n",
        "        buffer_conversation.invoke(user_input)\n",
        "        window_conversation.invoke(user_input)\n",
        "        summary_conversation.invoke(user_input)\n",
        "        token_buffer_conversation.invoke(user_input)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"\\n--- Conversation ended. Final Memory States ---\")\n",
        "\n",
        "# Display Buffer Memory History\n",
        "print(\"\\nBuffer Memory History:\", buffer_memory.load_memory_variables({})['history'])\n",
        "\n",
        "# Display Window Memory History\n",
        "print(\"\\nWindow Memory History (k=2):\", window_memory.load_memory_variables({})['history'])\n",
        "\n",
        "# Display Summary Memory History\n",
        "print(\"\\nSummary Memory History:\", summary_memory.load_memory_variables({})['history'])\n",
        "\n",
        "# Display Token Buffer Memory History and token count\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "token_memory_string = token_buffer_memory.load_memory_variables({})['history']\n",
        "num_tokens_in_token_memory = len(encoding.encode(token_memory_string, disallowed_special=()))\n",
        "print(\"\\nToken Buffer Memory History (max_token_limit=50):\", token_memory_string)\n",
        "print(f\"Current token count in Token Buffer Memory: {num_tokens_in_token_memory}\")\n",
        "\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUe6U3oSLKxt",
        "outputId": "5db04423-fd9f-4611-a726-28a04f4992d0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start chatting with the AI. Your input will be processed by each memory type.\n",
            "Type 'exit' or 'quit' to end the conversation and see the final memory states.\n",
            "You: Hello gm\n",
            "You: what is gen-ai\n",
            "You: who is MS Dhoni\n",
            "You: quit\n",
            "\n",
            "--- Conversation ended. Final Memory States ---\n",
            "\n",
            "Buffer Memory History: Human: Hello gm\n",
            "AI: Hello! Good morning! How are you doing today? Anything exciting planned for your day?\n",
            "Human: what is gen-ai\n",
            "AI: Generative AI, often referred to as Gen-AI, is a type of artificial intelligence that focuses on creating content. This can include generating text, images, music, and even video. It uses algorithms and models, particularly deep learning techniques, to analyze patterns in existing data and then produce new, original content that resembles the input data.\n",
            "\n",
            "For example, in text generation, models like GPT-3 (which I am based on) can create human-like text based on prompts given to them. In the realm of images, models like DALL-E can generate pictures based on textual descriptions. The applications of generative AI are vast, ranging from creative writing and art to more practical uses like generating code or assisting in product design. \n",
            "\n",
            "Is there a specific aspect of generative AI you’re curious about?\n",
            "Human: who is MS Dhoni\n",
            "AI: MS Dhoni, full name Mahendra Singh Dhoni, is a former Indian cricketer and one of the most celebrated captains in the history of Indian cricket. Born on July 7, 1981, in Ranchi, India, he is known for his calm demeanor, sharp cricketing mind, and exceptional leadership skills.\n",
            "\n",
            "Dhoni made his debut for the Indian national team in 2004 and quickly became known for his aggressive batting style and wicketkeeping abilities. He served as the captain of the Indian team from 2007 to 2016 in limited-overs formats and from 2008 to 2014 in Test cricket. Under his captaincy, India won several prestigious tournaments, including the ICC T20 World Cup in 2007, the ICC Cricket World Cup in 2011, and the ICC Champions Trophy in 2013.\n",
            "\n",
            "In addition to his international career, Dhoni has been a key player in the Indian Premier League (IPL), where he has led the Chennai Super Kings (CSK) to multiple championships. He is widely regarded as one of the greatest finishers in limited-overs cricket and has a massive fan following.\n",
            "\n",
            "After retiring from international cricket in 2020, Dhoni continues to play in the IPL and remains involved in various business ventures and philanthropic activities. Is there something specific you would like to know about him?\n",
            "\n",
            "Window Memory History (k=2): Human: what is gen-ai\n",
            "AI: Generative AI, often referred to as Gen-AI, is a type of artificial intelligence that focuses on creating new content or data rather than simply analyzing or processing existing information. This can include generating text, images, music, and even video. \n",
            "\n",
            "For example, in the realm of text, models like me can generate human-like responses, write stories, or even create poetry based on prompts given by users. In the visual arts, generative AI can create original artwork or modify existing images in creative ways. \n",
            "\n",
            "The technology relies on complex algorithms and large datasets to learn patterns and styles, enabling it to produce outputs that can be surprisingly coherent and creative. It's a fascinating field that blends creativity with technology! Do you have a specific aspect of generative AI you’re curious about?\n",
            "Human: who is MS Dhoni\n",
            "AI: MS Dhoni, full name Mahendra Singh Dhoni, is a former Indian cricketer and one of the most celebrated captains in the history of cricket. Born on July 7, 1981, in Ranchi, India, he is known for his calm demeanor, sharp cricketing acumen, and exceptional leadership skills.\n",
            "\n",
            "Dhoni made his debut for the Indian national team in 2004 and quickly became known for his powerful batting and innovative wicketkeeping. He captained the Indian team from 2007 to 2016 in limited-overs formats and from 2008 to 2014 in Test cricket. Under his leadership, India won several prestigious tournaments, including the ICC T20 World Cup in 2007, the ICC Cricket World Cup in 2011, and the ICC Champions Trophy in 2013.\n",
            "\n",
            "In addition to his international success, Dhoni has also been a key player in the Indian Premier League (IPL), where he has led the Chennai Super Kings (CSK) to multiple titles. His ability to finish matches and make strategic decisions under pressure has earned him the nickname \"Captain Cool.\"\n",
            "\n",
            "Beyond cricket, Dhoni is known for his philanthropic efforts and has a keen interest in sports beyond cricket, including football and badminton. He has also ventured into business and has a passion for motorcycles. Is there something specific about MS Dhoni that you would like to know?\n",
            "\n",
            "Summary Memory History: The human greets the AI with \"Hello gm,\" and the AI responds with a cheerful good morning, asking how the human is doing and if they have anything exciting planned for the day. The human inquires about generative AI, and the AI explains that it refers to a type of artificial intelligence capable of creating new content, such as text and images, using advanced algorithms and neural networks, mentioning examples like GPT-3 and DALL-E, and asks if the human has specific interests in generative AI. The human then asks about MS Dhoni, and the AI describes him as a former Indian cricketer and captain of the Indian national team, known for his leadership and calm demeanor, highlighting his achievements, including winning major tournaments and playing for the Chennai Super Kings in the IPL. The AI concludes by asking if the human is a fan of cricket or MS Dhoni.\n",
            "\n",
            "Token Buffer Memory History (max_token_limit=50): \n",
            "Current token count in Token Buffer Memory: 0\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jdwi71dmME-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f07b54d"
      },
      "source": [
        "# Task\n",
        "Build a Gradio UI for a LangChain application that demonstrates different memory types (Conversation Buffer Memory, Conversation Buffer Window Memory, Conversation Summary Buffer Memory, and Conversation Token Buffer Memory). The UI should allow users to input text, see the response from each memory type's chain, and view the current state of each memory after each interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5db4714e"
      },
      "source": [
        "## Install gradio\n",
        "\n",
        "### Subtask:\n",
        "Install the Gradio library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9577f83"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the Gradio library. I will use pip to install it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0f86519",
        "outputId": "61a0a771-be78-4594-ad2c-30d3a1428d34"
      },
      "source": [
        "%pip install gradio -q\n",
        "print(\"Gradio installed.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6758425"
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "### Subtask:\n",
        "Import necessary libraries from LangChain and Gradio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a5a0a57"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary libraries for the Gradio application demonstrating different LangChain memory types.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cee8e151"
      },
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import (\n",
        "    ConversationBufferMemory,\n",
        "    ConversationBufferWindowMemory,\n",
        "    ConversationSummaryMemory,\n",
        "    ConversationTokenBufferMemory,\n",
        ")\n",
        "import gradio as gr\n",
        "import tiktoken"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fd7ae7c"
      },
      "source": [
        "## Initialize memory and chains\n",
        "\n",
        "### Subtask:\n",
        "Initialize the different LangChain memory types and ConversationChains as done previously.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e067864"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize the LLM, different memory types, and ConversationChains as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ce00347",
        "outputId": "d3d0264f-f13b-431c-ed68-1e6dd4257aa6"
      },
      "source": [
        "# Initialize the ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
        "\n",
        "# Initialize each memory type\n",
        "buffer_memory = ConversationBufferMemory()\n",
        "window_memory = ConversationBufferWindowMemory(k=2) # Window size of 2 for demonstration\n",
        "summary_memory = ConversationSummaryMemory(llm=llm)\n",
        "token_buffer_memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50) # Token limit of 50\n",
        "\n",
        "# Create ConversationChains for each memory type\n",
        "buffer_conversation = ConversationChain(llm=llm, memory=buffer_memory, verbose=False)\n",
        "window_conversation = ConversationChain(llm=llm, memory=window_memory, verbose=False)\n",
        "summary_conversation = ConversationChain(llm=llm, memory=summary_memory, verbose=False)\n",
        "token_buffer_conversation = ConversationChain(llm=llm, memory=token_buffer_memory, verbose=False)\n",
        "\n",
        "print(\"Memory types and ConversationChains initialized.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory types and ConversationChains initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e47fa220"
      },
      "source": [
        "## Define prediction function\n",
        "\n",
        "### Subtask:\n",
        "Create a Python function that takes user input, processes it through each memory type's ConversationChain, and returns the responses and updated memory states for each.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb0d5b61"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `predict` function to process user input through each memory type and return the responses and memory states.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4cb7c0c"
      },
      "source": [
        "def predict(user_input):\n",
        "    # Process with Buffer Memory\n",
        "    buffer_response = buffer_conversation.invoke(user_input)\n",
        "    buffer_history = buffer_memory.load_memory_variables({})['history']\n",
        "\n",
        "    # Process with Window Memory\n",
        "    window_response = window_conversation.invoke(user_input)\n",
        "    window_history = window_memory.load_memory_variables({})['history']\n",
        "\n",
        "    # Process with Summary Memory\n",
        "    summary_response = summary_conversation.invoke(user_input)\n",
        "    summary_history = summary_memory.load_memory_variables({})['history']\n",
        "\n",
        "    # Process with Token Buffer Memory\n",
        "    token_buffer_response = token_buffer_conversation.invoke(user_input)\n",
        "    token_history = token_buffer_memory.load_memory_variables({})['history']\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "    num_tokens_in_token_memory = len(encoding.encode(token_history, disallowed_special=()))\n",
        "\n",
        "    return {\n",
        "        \"Buffer Memory\": {\n",
        "            \"Response\": buffer_response['response'],\n",
        "            \"History\": buffer_history\n",
        "        },\n",
        "        \"Window Memory (k=2)\": {\n",
        "            \"Response\": window_response['response'],\n",
        "            \"History\": window_history\n",
        "        },\n",
        "        \"Summary Memory\": {\n",
        "            \"Response\": summary_response['response'],\n",
        "            \"History\": summary_history\n",
        "        },\n",
        "        \"Token Buffer Memory (max_token_limit=50)\": {\n",
        "            \"Response\": token_buffer_response['response'],\n",
        "            \"History\": token_history,\n",
        "            \"Token Count\": num_tokens_in_token_memory\n",
        "        }\n",
        "    }"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94102baa"
      },
      "source": [
        "## Build gradio interface\n",
        "\n",
        "### Subtask:\n",
        "Use the Gradio library to create a user interface with input and output components for the conversation and memory states.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bfe2821"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the Gradio input and output components and create the Gradio interface.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28cb4980"
      },
      "source": [
        "# Define input component\n",
        "input_text = gr.Textbox(label=\"Enter your message:\")\n",
        "\n",
        "# Define output components for each memory type\n",
        "buffer_response_output = gr.Textbox(label=\"Buffer Memory Response\")\n",
        "buffer_history_output = gr.Textbox(label=\"Buffer Memory History\", lines=5)\n",
        "\n",
        "window_response_output = gr.Textbox(label=\"Window Memory Response (k=2)\")\n",
        "window_history_output = gr.Textbox(label=\"Window Memory History (k=2)\", lines=5)\n",
        "\n",
        "summary_response_output = gr.Textbox(label=\"Summary Memory Response\")\n",
        "summary_history_output = gr.Textbox(label=\"Summary Memory History\", lines=5)\n",
        "\n",
        "token_buffer_response_output = gr.Textbox(label=\"Token Buffer Memory Response (max_token_limit=50)\")\n",
        "token_history_output = gr.Textbox(label=\"Token Buffer Memory History (max_token_limit=50)\", lines=5)\n",
        "token_count_output = gr.Textbox(label=\"Token Count in Token Buffer Memory\")\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=input_text,\n",
        "    outputs=[\n",
        "        buffer_response_output,\n",
        "        buffer_history_output,\n",
        "        window_response_output,\n",
        "        window_history_output,\n",
        "        summary_response_output,\n",
        "        summary_history_output,\n",
        "        token_buffer_response_output,\n",
        "        token_history_output,\n",
        "        token_count_output,\n",
        "    ],\n",
        "    title=\"LangChain Memory Type Demonstration\"\n",
        ")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dd3111b"
      },
      "source": [
        "**Reasoning**:\n",
        "Launch the Gradio interface to allow user interaction with the different memory types.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "a4aad836",
        "outputId": "159d936a-c788-4dc9-83ee-4798f2ce20cb"
      },
      "source": [
        "# Launch the Gradio interface\n",
        "iface.launch()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://25988446b97f428b56.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://25988446b97f428b56.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "392729c2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The Gradio library was successfully installed and necessary libraries for building the LangChain and Gradio application were imported.\n",
        "*   Different LangChain memory types (Conversation Buffer Memory, Conversation Buffer Window Memory, Conversation Summary Buffer Memory, and Conversation Token Buffer Memory) and their corresponding ConversationChains were initialized.\n",
        "*   A Python function `predict` was created to process user input through each memory type's chain, returning responses and memory states, including the token count for the Token Buffer Memory.\n",
        "*   A Gradio user interface was built with input and output components to display user messages, responses from each memory type, and the historical state of each memory, which was then successfully launched.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The constructed Gradio UI provides a practical way to visually compare how different LangChain memory types manage conversation history and affect subsequent responses.\n",
        "*   Further steps could involve adding explanations within the Gradio UI to clarify the behavior and use cases of each memory type for users.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}