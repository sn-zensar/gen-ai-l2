{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b23tlXs_gh9T"
      },
      "outputs": [],
      "source": [
        "# Welcome to Langchain\n",
        "\n",
        "# Components\n",
        "    # 1 Models\n",
        "    # 2 Prompts\n",
        "    # 3 Chains\n",
        "    # 4 Memory\n",
        "    # 5 Vectors\n",
        "    # 6 Agents (Tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UflbYM4zhMWa"
      },
      "outputs": [],
      "source": [
        "# Prompt : Text, Instruction or Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t9N78KcxiSxA"
      },
      "outputs": [],
      "source": [
        "# Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHST3TmViYn3",
        "outputId": "1c97616a-f0be-4784-b01c-15d68baf31e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.74)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.99.9)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m557.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-0.3.30\n"
          ]
        }
      ],
      "source": [
        "pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dTXlUsriFYJ",
        "outputId": "80091a7e-dd52-4925-f9b6-65059c6ecfa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question (type 'exit' or 'quit' to end): what is python\n",
            "<class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
            "<class 'langchain_core.messages.ai.AIMessage'>\n",
            "Enter your question (type 'exit' or 'quit' to end): quit\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "model = ChatOpenAI(model='gpt-4.1-nano')\n",
        "\n",
        "while True:\n",
        "    user_prompt = input(\"Enter your question (type 'exit' or 'quit' to end): \")\n",
        "    if user_prompt.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "    response = model.invoke(user_prompt)\n",
        "    print(type(model))\n",
        "    print(type(response))\n",
        "\n",
        "    #print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC6qsE1biGYw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b3qW6CHiIzM"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the ChatOpenAI model\n",
        "model = ChatOpenAI(model='gpt-4.1-nano')\n",
        "\n",
        "while True:\n",
        "    user_prompt = input(\"Enter your question (type 'exit' or 'quit' to end): \")\n",
        "    if user_prompt.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "    response = model.invoke(user_prompt)\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhl-uu5Tjz1V"
      },
      "outputs": [],
      "source": [
        "# Model (to comunicate with various models)\n",
        "# Prompt\n",
        "\n",
        "      # 1 Prompt Templates\n",
        "      # 2 Chat Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-vt8nFR_neVN"
      },
      "outputs": [],
      "source": [
        "# 1 Static"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itHX_Grhkz1K",
        "outputId": "d0ffa44e-aa1f-4bb7-c673-e699447269ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
            "input_variables=[] input_types={} partial_variables={} template='Explain Python in 3 bullet points.'\n",
            "Input Prompt to LLM is :  Explain Python in 3 bullet points.\n",
            "- Python is a high-level, interpreted programming language known for its clear and readable syntax.\n",
            "- It supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\n",
            "- Python is widely used for web development, data analysis, artificial intelligence, automation, and scripting, thanks to its extensive standard library and active community.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "static_prompt = PromptTemplate.from_template(\"Explain Python in 3 bullet points.\")\n",
        "\n",
        "model = ChatOpenAI(model='gpt-4.1-nano')\n",
        "\n",
        "# Format the prompt template into a string\n",
        "formatted_prompt = static_prompt.format()\n",
        "\n",
        "\n",
        "print(type(static_prompt))\n",
        "\n",
        "#print(type(formatted_prompt))\n",
        "\n",
        "print(static_prompt)\n",
        "\n",
        "print(\"Input Prompt to LLM is : \", formatted_prompt)\n",
        "\n",
        "response = model.invoke(formatted_prompt)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XPUVyekhlVkF"
      },
      "outputs": [],
      "source": [
        "# Dynamic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6zAvZqPniKH",
        "outputId": "fe04c7b5-74ac-4932-d4df-3841795cc471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Prompt to LLM is :  Explain Prompts in Langchain in 5 bullet points.\n",
            "- **Definition**: Prompts in Langchain are templates or instructions designed to guide language models in generating specific, desired outputs.\n",
            "\n",
            "- **Template Structure**: They often include placeholders or variables that can be dynamically filled with user input or context, enabling flexible and reusable prompts.\n",
            "\n",
            "- **Type Variations**: Prompts can be simple text snippets, conversational prompts, or complex multi-turn templates, depending on the use case.\n",
            "\n",
            "- **Integration**: Langchain's prompt components facilitate seamless composition of prompts with language models and chaining of multiple prompts for complex workflows.\n",
            "\n",
            "- **Customization & Optimization**: Users can tailor prompts to enhance model performance, including prompt engineering techniques like few-shot examples, to improve response relevance and accuracy.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "dynamic_prompt = PromptTemplate.from_template(\"Explain {topic} in 5 bullet points.\")\n",
        "\n",
        "topic = dynamic_prompt.format(topic=\"Prompts in Langchain\")\n",
        "\n",
        "model = ChatOpenAI(model='gpt-4.1-nano')\n",
        "\n",
        "# Format the prompt template into a string\n",
        "formatted_prompt = dynamic_prompt.format(topic=\"Prompts in Langchain\")\n",
        "\n",
        "\n",
        "#print(type(my_prompt))\n",
        "\n",
        "#print(type(formatted_prompt))\n",
        "\n",
        "#print(my_prompt)\n",
        "\n",
        "print(\"Input Prompt to LLM is : \", formatted_prompt)\n",
        "\n",
        "response = model.invoke(formatted_prompt)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIXtNn0eoH9m",
        "outputId": "bdb0443a-ebf2-4f29-b7e2-e42961cd3aad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the topic you want me to explain (type 'exit' or 'quit' to end): temparature value in llm\n",
            "\n",
            "Input Prompt to LLM is :  Explain temparature value in llm in 2 bullet points.\n",
            "\n",
            " - Temperature in an LLM (Large Language Model) controls the randomness or creativity of the generated output; lower values produce more deterministic and focused responses, while higher values increase diversity and variability.  \n",
            "- It influences the probability distribution from which the next token is sampled, effectively adjusting how conservative or exploratory the model's predictions are during text generation.\n",
            "Enter the topic you want me to explain (type 'exit' or 'quit' to end): quit\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "dynamic_prompt = PromptTemplate.from_template(\"Explain {topic} in 2 bullet points.\")\n",
        "\n",
        "model = ChatOpenAI(model='gpt-4.1-nano', temperature=0.0)\n",
        "\n",
        "while True:\n",
        "    user_topic = input(\"Enter the topic you want me to explain (type 'exit' or 'quit' to end): \")\n",
        "    if user_topic.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Format the prompt template with the user's topic\n",
        "    formatted_prompt = dynamic_prompt.format(topic=user_topic)\n",
        "\n",
        "    print(\"\\nInput Prompt to LLM is : \", formatted_prompt)\n",
        "\n",
        "    response = model.invoke(formatted_prompt)\n",
        "    print(\"\\n\", response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "drKGxRJQqGy5"
      },
      "outputs": [],
      "source": [
        "# Dynamic Example 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1ORyDl4qfgU",
        "outputId": "d9aec128-3a9b-4c70-e24e-26ad7eadae37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the topic you want me to write a poem on (type 'exit' or 'quit' to end): rain\n",
            "\n",
            "Input Prompt to LLM is :  Write a 8 lines poem on rain.\n",
            "\n",
            " Gentle Janitor of the silent sky,  \n",
            "You paint the earth with tears as you fall by,  \n",
            "Whispering secrets in shimmering thread,  \n",
            "Awakening sweets in a blooming bed.  \n",
            "\n",
            "A lullaby in torrents grand and wide,  \n",
            "Refreshing us in every stride,  \n",
            "Companion of the morning's gentle grace,  \n",
            "Rain's tender touch your heartfelt embrace.\n",
            "Enter the topic you want me to write a poem on (type 'exit' or 'quit' to end): quit\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "dynamic_prompt = PromptTemplate.from_template(\"Write a 8 lines poem on {topic}.\")\n",
        "\n",
        "model = ChatOpenAI(model='gpt-4.1-nano', temperature=1.5)\n",
        "\n",
        "while True:\n",
        "    user_topic = input(\"Enter the topic you want me to write a poem on (type 'exit' or 'quit' to end): \")\n",
        "    if user_topic.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Format the prompt template with the user's topic\n",
        "    formatted_prompt = dynamic_prompt.format(topic=user_topic)\n",
        "\n",
        "    print(\"\\nInput Prompt to LLM is : \", formatted_prompt)\n",
        "\n",
        "    response = model.invoke(formatted_prompt)\n",
        "    print(\"\\n\", response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl7Df2iwqv15"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "dynamic_prompt = PromptTemplate.from_template(\"Write a 8 lines poem on {topic}.\")\n",
        "\n",
        "model = ChatOpenAI(model='gpt-4.1-nano', temperature=1.5)\n",
        "\n",
        "while True:\n",
        "    user_topic = input(\"Enter the topic you want me to write a poem on (type 'exit' or 'quit' to end): \")\n",
        "    if user_topic.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Format the prompt template with the user's topic\n",
        "    formatted_prompt = dynamic_prompt.format(topic=user_topic)\n",
        "\n",
        "    print(\"\\nInput Prompt to LLM is : \", formatted_prompt)\n",
        "\n",
        "    response = model.invoke(formatted_prompt)\n",
        "    print(\"\\n\", response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "LYbzHJ_ougUh"
      },
      "outputs": [],
      "source": [
        "# Dynamic Prompt Example 4\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp1bLjRZuzhv"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEc6GinKuiwN",
        "outputId": "71181aea-4e98-49a2-9cba-7e966074f98f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the topic you want me to explain (type 'exit' or 'quit' to end): python\n",
            "\n",
            "Input Prompt to LLM is (Human Message):  Explain python in 2 bullet points.\n",
            "\n",
            " AI Response - Python is a high-level, interpreted programming language known for its readability and simplicity, making it accessible for beginners and efficient for developers.  \n",
            "- It supports multiple programming paradigms, including procedural, object-oriented, and functional programming, and is widely used in web development, data analysis, artificial intelligence, and automation.\n",
            "Enter the topic you want me to explain (type 'exit' or 'quit' to end): java\n",
            "\n",
            "Input Prompt to LLM is (Human Message):  Explain java in 2 bullet points.\n",
            "\n",
            " AI Response - Java is a high-level, object-oriented programming language designed for portability, allowing code to run on any device with a Java Virtual Machine (JVM).  \n",
            "- It is widely used for building web applications, mobile apps (especially Android), and enterprise software due to its robustness, security features, and platform independence.\n",
            "Enter the topic you want me to explain (type 'exit' or 'quit' to end): cricket\n",
            "\n",
            "Input Prompt to LLM is (Human Message):  Explain cricket in 2 bullet points.\n",
            "\n",
            " AI Response - Cricket is a bat-and-ball sport played between two teams of eleven players each, where the main objective is to score runs by hitting a pitched ball and running between wickets.  \n",
            "- The game involves batting, bowling, and fielding, with the team scoring the most runs or dismissing the opposition to win.\n",
            "Enter the topic you want me to explain (type 'exit' or 'quit' to end): football\n",
            "\n",
            "Input Prompt to LLM is (Human Message):  Explain football in 2 bullet points.\n",
            "\n",
            " AI Response - Football (soccer) is a team sport played between two teams of eleven players each, where the objective is to score goals by getting a spherical ball into the opposing team's net using any part of the body except the hands and arms.  \n",
            "- It is the world's most popular sport, played and watched by millions globally, with major tournaments like the FIFA World Cup showcasing its widespread appeal.\n",
            "Enter the topic you want me to explain (type 'exit' or 'quit' to end): quit\n",
            "[HumanMessage(content='python', additional_kwargs={}, response_metadata={}), AIMessage(content='- Python is a high-level, interpreted programming language known for its readability and simplicity, making it accessible for beginners and efficient for developers.  \\n- It supports multiple programming paradigms, including procedural, object-oriented, and functional programming, and is widely used in web development, data analysis, artificial intelligence, and automation.', additional_kwargs={}, response_metadata={}), HumanMessage(content='java', additional_kwargs={}, response_metadata={}), AIMessage(content='- Java is a high-level, object-oriented programming language designed for portability, allowing code to run on any device with a Java Virtual Machine (JVM).  \\n- It is widely used for building web applications, mobile apps (especially Android), and enterprise software due to its robustness, security features, and platform independence.', additional_kwargs={}, response_metadata={}), HumanMessage(content='cricket', additional_kwargs={}, response_metadata={}), AIMessage(content='- Cricket is a bat-and-ball sport played between two teams of eleven players each, where the main objective is to score runs by hitting a pitched ball and running between wickets.  \\n- The game involves batting, bowling, and fielding, with the team scoring the most runs or dismissing the opposition to win.', additional_kwargs={}, response_metadata={}), HumanMessage(content='football', additional_kwargs={}, response_metadata={}), AIMessage(content=\"- Football (soccer) is a team sport played between two teams of eleven players each, where the objective is to score goals by getting a spherical ball into the opposing team's net using any part of the body except the hands and arms.  \\n- It is the world's most popular sport, played and watched by millions globally, with major tournaments like the FIFA World Cup showcasing its widespread appeal.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='quit', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "dynamic_prompt = PromptTemplate.from_template(\"Explain {topic} in 2 bullet points.\")\n",
        "\n",
        "model = ChatOpenAI(model='gpt-4.1-nano', temperature=0.0)\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "    user_topic = input(\"Enter the topic you want me to explain (type 'exit' or 'quit' to end): \")\n",
        "    chat_history.append(HumanMessage(content=user_topic))\n",
        "    if user_topic.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Format the prompt template with the user's topic\n",
        "    formatted_prompt = dynamic_prompt.format(topic=user_topic)\n",
        "\n",
        "    print(\"\\nInput Prompt to LLM is (Human Message): \", formatted_prompt)\n",
        "\n",
        "    response = model.invoke(formatted_prompt)\n",
        "    chat_history.append(AIMessage(content=response.content))\n",
        "    print(\"\\n AI Response\", response.content)\n",
        "\n",
        "print(chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGXlM2DlulLl",
        "outputId": "e0ca5b4a-6f23-4b3e-9762-45218c218483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the topic you want me to explain (type 'exit' or 'quit' to end): python\n",
            "\n",
            "Input Prompt to LLM is (Human Message):  Explain python in 2 bullet points.\n",
            "\n",
            " AI Response - Python is a high-level, interpreted programming language known for its readability and simplicity, making it accessible for beginners and efficient for developers.  \n",
            "- It supports multiple programming paradigms, including procedural, object-oriented, and functional programming, and is widely used in web development, data analysis, artificial intelligence, and automation.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "dynamic_prompt = PromptTemplate.from_template(\"Explain {topic} in 2 bullet points.\")\n",
        "\n",
        "model = ChatOpenAI(model='gpt-4.1-nano', temperature=0.0)\n",
        "\n",
        "chat_history = [\n",
        "    SystemMessage(content=\"You are a helpful AI assistant.\")\n",
        "]\n",
        "\n",
        "# SystemMessage(content=\"You are a {topic} expert.\")\n",
        "while True:\n",
        "    user_topic = input(\"Enter the topic you want me to explain (type 'exit' or 'quit' to end): \")\n",
        "    chat_history.append(HumanMessage(content=user_topic))\n",
        "    if user_topic.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Format the prompt template with the user's topic\n",
        "    formatted_prompt = dynamic_prompt.format(topic=user_topic)\n",
        "\n",
        "    print(\"\\nInput Prompt to LLM is (Human Message): \", formatted_prompt)\n",
        "\n",
        "    response = model.invoke(formatted_prompt)\n",
        "    chat_history.append(AIMessage(content=response.content))\n",
        "    print(\"\\n AI Response\", response.content)\n",
        "\n",
        "print(chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "iIKB2Jx5yHpm"
      },
      "outputs": [],
      "source": [
        "# 2 Chat Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5YBSpNZzGbz",
        "outputId": "1fb3931e-fadc-436e-d062-dc62e530d5a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "messages=[SystemMessage(content='You are a helpful AI expert.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain {topic} in simple language for begginers.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "chat_template = ChatPromptTemplate([\n",
        "    SystemMessage(content=\"You are a helpful AI expert.\"),\n",
        "    HumanMessage(content=\"Explain {topic} in simple language for begginers.\")\n",
        "])\n",
        "\n",
        "prompt = chat_template.invoke({\"topic\": \"Python\"})\n",
        "\n",
        "print(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmyvVesd8Bmx",
        "outputId": "a64afd2f-812b-4cdd-fea7-a9f211dce7ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "messages=[SystemMessage(content='You are a helpful Programing Language expert.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain Python in simple language for begginers.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "chat_template = ChatPromptTemplate([\n",
        "   ('system', 'You are a helpful {domain} expert.'),\n",
        "   ('human', 'Explain {topic} in simple language for begginers.')\n",
        "])\n",
        "\n",
        "prompt = chat_template.invoke({\"domain\":\"Programing Language\",\"topic\": \"Python\"})\n",
        "\n",
        "print(prompt)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15nvWmcZ9tgs",
        "outputId": "c679776f-3a49-4750-b30d-37ed0feef2a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "chat_template = ChatPromptTemplate([\n",
        "   ('system', 'You are a helpful {domain} expert.'),\n",
        "   ('human', 'Explain {topic} in simple language for begginers.')\n",
        "])\n",
        "\n",
        "prompt = chat_template.invoke({\"domain\":\"Programing Language\",\"topic\": \"Java\"})\n",
        "\n",
        "print(type(chat_template))\n",
        "\n",
        "#response = model.invoke(prompt)\n",
        "#print(\"\\n Model Response\", response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_ow-cpO8YhQ",
        "outputId": "94ad92a5-efda-419d-9f23-714790690f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Model Response Sure! In LangChain, **MessagesPlaceholder** is a special tool used when building chat-based applications. \n",
            "\n",
            "**Think of it like a placeholder or a blank space** in a conversation where you can insert multiple messages later. It helps your program know where to put or expect a series of messages, especially when the number or content of those messages isn't fixed in advance.\n",
            "\n",
            "### Simple analogy:\n",
            "Imagine you're writing a story, and you leave a blank space where different characters might speak. Later, you fill in that blank with the actual dialogue. Similarly, **MessagesPlaceholder** acts as that blank space in your chat flow, ready to be filled with messages when needed.\n",
            "\n",
            "### Why is it useful?\n",
            "- When you want your chatbot to handle multiple messages or conversations dynamically.\n",
            "- To organize complex chat flows where parts of the conversation are flexible.\n",
            "\n",
            "### Example:\n",
            "Suppose you're building a chatbot that can handle multiple user inputs before giving a final answer. You can use **MessagesPlaceholder** to hold all those user messages, and then process them together.\n",
            "\n",
            "---\n",
            "\n",
            "**In summary:**  \n",
            "**MessagesPlaceholder** is a tool in LangChain that acts as a flexible spot in your chat flow to hold or insert multiple messages later, making your chat applications more dynamic and organized.\n",
            "\n",
            "If you'd like, I can show you a simple code example!\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "chat_template = ChatPromptTemplate([\n",
        "   ('system', 'You are a helpful {domain} expert.'),\n",
        "   ('human', 'Explain {topic} in simple language for begginers.')\n",
        "])\n",
        "\n",
        "prompt = chat_template.invoke({\"domain\":\"Gen-AI\",\"topic\": \"MessagesPlaceholder in langchain\"})\n",
        "\n",
        "\n",
        "response = model.invoke(prompt)\n",
        "print(\"\\n Model Response\", response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73KboCZ9-0cW",
        "outputId": "937fd7af-dda0-4398-8d6d-bb9bacaf7559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your message (type 'exit' or 'quit' to end): python\n",
            "\n",
            "AI Response: Hello! It looks like your message didn't come through. Could you please resend your question or request? I'm here to help!\n",
            "Enter your message (type 'exit' or 'quit' to end): quit\n",
            "\n",
            "Final Chat History:\n",
            "Human: python\n",
            "AI: Hello! It looks like your message didn't come through. Could you please resend your question or request? I'm here to help!\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "\n",
        "HumanMessage(content=\"Explain Python in simple language for begginers.\")\n",
        "AIMessage(content=\"Python is a high-level, interpreted programming language known for its readability and simplicity, making it accessible for beginners and efficient for developers.\")\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    HumanMessage(content=\"{user_input}\")\n",
        "])\n",
        "\n",
        "model = ChatOpenAI(model='gpt-4.1-nano', temperature=0.0)\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter your message (type 'exit' or 'quit' to end): \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Prepare the prompt with current chat history and user input\n",
        "    prompt = chat_template.invoke({\"chat_history\": chat_history, \"user_input\": user_input})\n",
        "\n",
        "    # Invoke the model\n",
        "    response = model.invoke(prompt)\n",
        "\n",
        "    # Append user message and AI response to chat history\n",
        "    chat_history.append(HumanMessage(content=user_input))\n",
        "    chat_history.append(AIMessage(content=response.content))\n",
        "\n",
        "    print(\"\\nAI Response:\", response.content)\n",
        "\n",
        "print(\"\\nFinal Chat History:\")\n",
        "for message in chat_history:\n",
        "    if isinstance(message, HumanMessage):\n",
        "        print(f\"Human: {message.content}\")\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(f\"AI: {message.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "dqAFbyb29Qw6"
      },
      "outputs": [],
      "source": [
        "# Example for Chat Prompt Templates with MessagePlaceHolders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6KguAHd-0CG"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Ensure the API key is set as an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)\n",
        "\n",
        "# Define the chat prompt template (system + human)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful AI assistant that responds in English.\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Format the prompt with the question\n",
        "formatted_prompt = prompt.format_messages(question=\"What is Python?\")\n",
        "\n",
        "# Invoke the model with the formatted prompt\n",
        "response = llm.invoke(formatted_prompt)\n",
        "\n",
        "print(response.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
