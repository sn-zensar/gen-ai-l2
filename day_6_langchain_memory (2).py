# -*- coding: utf-8 -*-
"""Day_6_Langchain_Memory.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AwsdIlh6RnMb63VEsuvI5-ubVgteBJ9U
"""

# ConversationBufferMemory (Full History)

# Step 1: Install required packages
!pip install -U langchain langchain-openai

# 1 ConversationBufferMemory

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI
from google.colab import userdata
import os

# Step 3: Set OpenAI Key
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

memory = ConversationBufferMemory()
chain = ConversationChain(llm=ChatOpenAI(), memory=memory)

response = chain.predict(input="Hello, my name is Sandip")
print(response)

response = chain.predict(input="What‚Äôs my name?")
print(response)  # ‚úÖ Bot says: "Your name is Sandip."

# 2 ConversationBufferMemory

# STEP 1: Install if needed
#!pip install -U langchain langchain-openai

# STEP 2: Import libraries
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI
from google.colab import userdata
import os

# STEP 3: Set OpenAI API Key
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# STEP 4: Initialize LLM and memory
llm = ChatOpenAI()
memory = ConversationBufferMemory()
chain = ConversationChain(llm=llm, memory=memory)

# STEP 5: Start interactive chat loop
print("üí¨ Chat started! Type 'exit' to quit.\n")
while True:
    user_input = input("You: ")
    if user_input.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break
    response = chain.predict(input=user_input)
    print("Bot:", response)

# 3 ConversationBufferWindowMemory

# STEP 1: Install if needed
#!pip install -U langchain langchain-openai

# STEP 2: Import libraries
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI
from google.colab import userdata
import os

# STEP 3: Set OpenAI API Key
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# STEP 4: Initialize LLM and memory
llm = ChatOpenAI()


memory = ConversationBufferWindowMemory(k=2)  # remembers only last 2 exchanges
chain = ConversationChain(llm=llm, memory=memory)


# STEP 5: Start interactive chat loop
print("üí¨ Chat started! Type 'exit' to quit.\n")
while True:
    user_input = input("You: ")
    if user_input.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break
    response = chain.predict(input=user_input)
    print("Bot:", response)

#4 ConversationSummaryMemory

# STEP 1: Install if needed
#!pip install -U langchain langchain-openai

# STEP 2: Import libraries
from langchain.memory import ConversationSummaryMemory
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI
from google.colab import userdata
import os

# STEP 3: Set OpenAI API Key
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# STEP 4: Initialize LLM and memory
llm = ChatOpenAI()



memory = ConversationSummaryMemory(llm=llm)
chain = ConversationChain(llm=llm, memory=memory)



# STEP 5: Start interactive chat loop
print("üí¨ Chat started! Type 'exit' to quit.\n")
while True:
    user_input = input("You: ")
    if user_input.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break
    response = chain.predict(input=user_input)
    print("Bot:", response)







# 5 ConversationSummaryBufferMemory

# STEP 1: Install if needed
#!pip install -U langchain langchain-openai

# STEP 2: Import libraries

from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationChain
from langchain_openai import ChatOpenAI
from google.colab import userdata
import os

# STEP 3: Set OpenAI API Key
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# STEP 4: Initialize LLM and memory
llm = ChatOpenAI()



memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=500)
chain = ConversationChain(llm=llm, memory=memory)




# STEP 5: Start interactive chat loop
print("üí¨ Chat started! Type 'exit' to quit.\n")
while True:
    user_input = input("You: ")
    if user_input.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break
    response = chain.predict(input=user_input)
    print("Bot:", response)

# 3 ConversationBufferWindowMemory

# Step 2: Imports
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.memory import ConversationBufferMemory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import AIMessage, HumanMessage
from google.colab import userdata
import os

# Step 3: Set OpenAI Key
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# Step 4: Setup LLM and prompt
llm = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Here is the conversation so far:\n\n{history}"),
    ("human", "{input}")
])

chain = prompt | llm

# Step 5: Define a proper chat history interface
class CustomChatHistory(BaseChatMessageHistory):
    def __init__(self, memory):
        self.memory = memory

    @property
    def messages(self):
        return self.memory.chat_memory.messages

    def add_user_message(self, message):
        self.memory.chat_memory.add_user_message(message)

    def add_ai_message(self, message):
        self.memory.chat_memory.add_ai_message(message)

    def add_message(self, message):
        self.memory.chat_memory.messages.append(message)

    def add_messages(self, messages):
        self.memory.chat_memory.messages.extend(messages)

    def clear(self):
        self.memory.clear()


# Step 6: Memory setup (must use return_messages=True)
buffer_memory = ConversationBufferMemory(return_messages=True)

# Step 7: Chain + memory wrapper
chat_with_memory = RunnableWithMessageHistory(
    chain,
    lambda session_id: CustomChatHistory(buffer_memory),
    input_messages_key="input",
    history_messages_key="history"
)

# Step 8: Chat loop
print("üí¨ Chat started! Type 'exit' to quit.")
while True:
    user_input = input("You: ")
    if user_input.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break
    response = chat_with_memory.invoke(
        {"input": user_input},
        config={"configurable": {"session_id": "chat"}}
    )
    print("Bot:", response.content)

# ConversationSummaryMemory (Summarized History)

# STEP 1: Install required packages
#!pip install -U langchain langchain-openai

# STEP 2: Import libraries
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.memory import ConversationSummaryMemory
from langchain_core.chat_history import BaseChatMessageHistory
from google.colab import userdata
import os

# STEP 3: Set your OpenAI API key securely
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# STEP 4: Setup LLM and prompt with injected history
llm = ChatOpenAI()
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Here is the conversation so far:\n\n{history}"),
    ("human", "{input}")
])
chain = prompt | llm

# STEP 5: Create a wrapper class to adapt memory
class SummaryMemoryWrapper(BaseChatMessageHistory):
    def __init__(self, memory):
        self.memory = memory

    @property
    def messages(self):
        return self.memory.chat_memory.messages

    def add_user_message(self, msg):
        self.memory.chat_memory.add_user_message(msg)

    def add_ai_message(self, msg):
        self.memory.chat_memory.add_ai_message(msg)

    def add_message(self, msg):
        self.memory.chat_memory.messages.append(msg)

    def add_messages(self, msgs):
        self.memory.chat_memory.messages.extend(msgs)

    def clear(self):
        self.memory.clear()

# STEP 6: Initialize memory with return_messages
summary_memory = ConversationSummaryMemory(llm=llm, return_messages=True)

# STEP 7: Wrap chain with message-aware memory
chat_with_memory = RunnableWithMessageHistory(
    chain,
    lambda session_id: SummaryMemoryWrapper(summary_memory),
    input_messages_key="input",
    history_messages_key="history"
)

# STEP 8: Run interactive chatbot
print("üí¨ Chat started! Type 'exit' to quit.")
while True:
    user_input = input("You: ")
    if user_input.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break
    response = chat_with_memory.invoke(
        {"input": user_input},
        config={"configurable": {"session_id": "chat"}}
    )
    print("Bot:", response.content)

# Imp => FAISS (Facebook AI Similarity Search), Vectors, Embeddings

# 1

# STEP 1: Install required packages
#!pip install -U langchain openai faiss-cpu

# STEP 2: Import modules
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from google.colab import userdata
import os

# STEP 3: Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# STEP 4: Provide 3 small text samples
texts = [
    "Cats are cute.",
    "Dogs are cute, smart and loyal.",
    "Pizza is delicious."
]

# STEP 5: Initialize embeddings model
embeddings = OpenAIEmbeddings()

# STEP 6: Print embeddings for each text
print("üî¢ Text Embeddings (first 5 values shown):")
for text in texts:
    vector = embeddings.embed_query(text)
    print(f"\nüìÑ Text: {text}")
    print(f"üìà Embedding: {vector[:5]}")

# STEP 7: Store in FAISS vector database
vectorstore = FAISS.from_texts(texts, embedding=embeddings)

# STEP 8: Ask a simple query
query = "What is tasty?"
results = vectorstore.similarity_search(query, k=1)

# STEP 9: Show the best match
print(f"\nüîç Query: {query}")
print("‚úÖ Most Relevant Match:")
print(f"‚û°Ô∏è {results[0].page_content}")

#2

# STEP 1: Install dependencies (run this in Colab)
#!pip install -U langchain openai faiss-cpu

# STEP 2: Import modules
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from google.colab import userdata
import os

# STEP 3: Set your OpenAI API Key from Colab's userdata
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# STEP 4: Prepare your text data
texts = [
    "Artificial Intelligence is transforming industries.",
    "Machine Learning is a subset of AI.",
    "Deep Learning uses neural networks.",
    "Python is a popular programming language for AI.",
    "AI can improve healthcare, finance, and education."
]

# STEP 5: Initialize the embedding model
embeddings = OpenAIEmbeddings()

# STEP 6: Generate and print embeddings for each text
print("üî¢ Generated Embeddings:")
for text in texts:
    vector = embeddings.embed_query(text)
    print(f"\nüìÑ Text: {text}")
    print(f"üìà Embedding (first 5 values): {vector[:5]}")  # Print first 5 values only

# STEP 7: Store text + embeddings in FAISS vector database
vectorstore = FAISS.from_texts(texts, embedding=embeddings)

# STEP 8: Perform a semantic search using a query
query = "How does AI help in finance?"
docs = vectorstore.similarity_search(query, k=2)

# STEP 9: Print the search query and top matching results
print(f"\nüîç Query: {query}")
print("\nüìö Top Matches:")
for i, doc in enumerate(docs, 1):
    print(f"{i}. {doc.page_content}")

# 3

# STEP 1: Install dependencies (if not done)
# !pip install openai langchain faiss-cpu

# STEP 2: Import modules
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter

from google.colab import userdata
import os

# STEP 3: Set your OpenAI API Key
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")


# STEP 3: Prepare your text data
texts = [
    "Artificial Intelligence is transforming industries.",
    "Machine Learning is a subset of AI.",
    "Deep Learning uses neural networks.",
    "Python is a popular programming language for AI.",
    "AI can improve healthcare, finance, and education."
]

# STEP 4: Generate embeddings
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts(texts, embedding=embeddings)

# STEP 5: Perform a search based on a question/query
query = "How does AI help in finance?"
docs = vectorstore.similarity_search(query, k=2)  # top 2 results

# STEP 6: Print the results
print(f"\nüîç Query: {query}")
print("\nüìö Top Matches:")
for i, doc in enumerate(docs, 1):
    print(f"{i}. {doc.page_content}")

# 4 with file (upload)

# STEP 1: Install necessary packages
!pip install -U langchain langchain-openai faiss-cpu pypdf

# STEP 2: Import modules
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from google.colab import userdata
import os

# STEP 3: Set OpenAI API Key securely
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# STEP 4: Upload your PDF (use Colab‚Äôs upload widget)
from google.colab import files
uploaded = files.upload()

# STEP 5: Load and read the PDF file
filename = next(iter(uploaded))
loader = PyPDFLoader(filename)
pages = loader.load()

# STEP 6: Split the document into small chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(pages)

# STEP 7: Embed the chunks
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(chunks, embeddings)

# STEP 8: Ask a question (semantic search)
query = "What is the main purpose of this document?"
results = vectorstore.similarity_search(query, k=2)

# STEP 9: Show the results
print(f"\nüîç Query: {query}")
print("\nüìö Top Matching Chunks:")
for i, doc in enumerate(results, 1):
    print(f"\n{i}. {doc.page_content.strip()[:500]}")  # show first 500 chars

# Without upload and with user input questions below:

# STEP 1: Install dependencies
#!pip install -U langchain langchain-openai faiss-cpu pypdf

# STEP 2: Import libraries
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from google.colab import userdata
import os

# STEP 3: Set OpenAI API key securely from Colab userdata
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# STEP 4: Set your PDF file path (file must be in /content/ already)
file_path = "/content/spark-def-guide.pdf"  # ‚Üê change to your actual filename

# STEP 5: Load and read the PDF
loader = PyPDFLoader(file_path)
pages = loader.load()

# STEP 6: Split PDF into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(pages)

# STEP 7: Generate embeddings and store in FAISS
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(chunks, embeddings)

# STEP 8: Ask user to type their question
query = input("\nüîç Type your question: ")

# STEP 9: Search for top matches using semantic similarity
results = vectorstore.similarity_search(query, k=2)

# STEP 10: Display the results
print(f"\nüîç Your Query: {query}")
print("üìö Top Matching Chunks:\n")
for i, doc in enumerate(results, 1):
    print(f"{i}. {doc.page_content.strip()[:500]}\n{'-'*60}")

# talk to pdf / ask questions - in a loop below:

# STEP 1: Install dependencies
#!pip install -U langchain langchain-openai faiss-cpu pypdf

# STEP 2: Import libraries
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from google.colab import userdata
import os

# STEP 3: Set your OpenAI API Key securely
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# STEP 4: Set the path to your PDF file (must be present in /content/)
file_path = "/content/spark-def-guide.pdf"  # Change this if your filename is different

# STEP 5: Load and process the PDF
loader = PyPDFLoader(file_path)
pages = loader.load()

# STEP 6: Split the document into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(pages)

# STEP 7: Generate embeddings and store them in FAISS
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(chunks, embeddings)

# STEP 8: Start semantic search loop
print("üìò PDF loaded. You can now ask questions about its content.")
print("Type 'exit' or 'quit' to stop.\n")

while True:
    query = input("üîç Your question: ")
    if query.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break

    results = vectorstore.similarity_search(query, k=2)

    print("\nüìö Top Matching Chunks:\n")
    for i, doc in enumerate(results, 1):
        print(f"{i}. {doc.page_content.strip()[:500]}")
        print("-" * 60)



# After the understanding of Embeddings & Vectors

# VectorStoreRetrieverMemory



# STEP 1: Install packages
!pip install -U langchain langchain-openai faiss-cpu

# STEP 2: Import libraries
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.memory import VectorStoreRetrieverMemory
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import HumanMessage, AIMessage
from google.colab import userdata
import os

# STEP 3: Set your OpenAI API Key
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# STEP 4: Initialize OpenAI LLM and Embeddings
llm = ChatOpenAI()
embeddings = OpenAIEmbeddings()

# STEP 5: Initialize FAISS vector store (
vectorstore = FAISS.from_texts(["initial placeholder"], embedding=embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
vector_memory = VectorStoreRetrieverMemory(retriever=retriever)

# STEP 6: Define prompt
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Use past conversation to answer:\n\n{history}"),
    ("human", "{input}")
])
chain = prompt | llm

# STEP 7: Hybrid memory wrapper (chat + vector)
class HybridChatHistory(BaseChatMessageHistory):
    def __init__(self):
        self._messages = []

    @property
    def messages(self):
        return self._messages

    def add_user_message(self, message):
        self._messages.append(HumanMessage(content=message))
        vector_memory.save_context({"input": message}, {})  # store in vector DB

    def add_ai_message(self, message):
        self._messages.append(AIMessage(content=message))
        vector_memory.save_context({}, {"output": message})  # store in vector DB

    def clear(self):
        self._messages.clear()

# STEP 8: Setup chain with hybrid memory
chat_with_memory = RunnableWithMessageHistory(
    chain,
    lambda session_id: HybridChatHistory(),
    input_messages_key="input",
    history_messages_key="history"
)

# STEP 9: Run interactive chat
print("üí¨ Chat started! Type 'exit' to quit.\n")
while True:
    user_input = input("You: ")
    if user_input.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break
    response = chat_with_memory.invoke({"input": user_input}, config={"configurable": {"session_id": "chat"}})
    print("Bot:", response.content)

# End of the Notebook

