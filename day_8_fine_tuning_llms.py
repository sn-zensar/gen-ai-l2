# -*- coding: utf-8 -*-
"""Day_8_Fine-Tuning_LLMs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K7wJnbyOa7ROHcFgHrgrkM2HDTGYvWs7
"""

# Fine Tuning :
"""
Fine-tuning means taking a pre-trained model (like gpt-3.5-turbo) and training it further on custom examples so it better understands your specific task, tone, or domain.

When to Fine-Tune?

You want a specific tone, style, or format in every output.

You want the model to follow strict instructions (e.g., output tables).

You want it to remember patterns (e.g., specific Q&A style).
"""

#huggingface api key => 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

# Simple Example using Huggingface LLM

# STEP 1: Install dependencies (only once)
!pip install -q transformers datasets

# STEP 2: Import required libraries
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import torch

# STEP 3: Define a small sentiment dataset
train_data = {
    "text": [
        "I love this product!",
        "This is the worst experience ever.",
        "It was okay, not great.",
        "Absolutely fantastic!",
        "I will never buy this again.",
        "Such a waste of money.",
        "I'm really happy with the service.",
        "Terrible support, very disappointed."
    ],
    "label": [1, 0, 1, 1, 0, 0, 1, 0]  # 1 = Positive, 0 = Negative
}

dataset = Dataset.from_dict(train_data)

# STEP 4: Load tokenizer and model
model_name = "distilbert-base-uncased"
tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)

# STEP 5: Tokenize dataset
def preprocess(examples):
    return tokenizer(examples["text"], padding=True, truncation=True)

tokenized_dataset = dataset.map(preprocess, batched=True)

# STEP 6: Training arguments (NO wandb)
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    logging_dir="./logs",
    report_to="none"  # üëà Disables wandb
)

# STEP 7: Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

# STEP 8: Train
trainer.train()

def chat():
    print("üó®Ô∏è  Sentiment Bot is ready! Type 'exit' to quit.")
    while True:
        text = input("You: ")
        if text.lower() == "exit":
            print("üëã Goodbye!")
            break
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            logits = model(**inputs).logits
        prediction = torch.argmax(logits, dim=1).item()
        sentiment = "Positive üòä" if prediction == 1 else "Negative üòû"
        print("Sentiment:", sentiment)

chat()

# Multiclass Sentiment: Negative, Neutral, Positive

# STEP 1: Install required packages
!pip install -q transformers datasets

# STEP 2: Imports
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
from datasets import Dataset
import torch

# STEP 3: Prepare labeled dataset with 3 classes
data = {
    "text": [
        "I love this product!",
        "This is the worst experience ever.",
        "It was okay, not great.",
        "Absolutely fantastic!",
        "I will never buy this again.",
        "Such a waste of money.",
        "I'm really happy with the service.",
        "Terrible support, very disappointed.",
        "I don‚Äôt feel strongly about this.",
        "It‚Äôs neither good nor bad."
    ],
    "label": [2, 0, 1, 2, 0, 0, 2, 0, 1, 1]  # 0 = Negative, 1 = Neutral, 2 = Positive
}
dataset = Dataset.from_dict(data)

# STEP 4: Tokenizer and preprocessing
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

def preprocess(example):
    return tokenizer(example["text"], truncation=True)

tokenized_dataset = dataset.map(preprocess)

# STEP 5: Load model for 3-label classification
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)

# STEP 6: Use data collator for padding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# STEP 7: Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=5,
    per_device_train_batch_size=2,
    logging_strategy="no",
    save_strategy="no",
    report_to="none"
)

# STEP 8: Train model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)
trainer.train()

# STEP 9: Simple inference function
def chat():
    label_map = {0: "Negative üòû", 1: "Neutral üòê", 2: "Positive üòä"}
    print("üó®Ô∏è  Sentiment Bot is ready! Type 'exit' to quit.")
    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            print("üëã Goodbye!")
            break
        inputs = tokenizer(user_input, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            outputs = model(**inputs)
            prediction = torch.argmax(outputs.logits, dim=1).item()
        print("Sentiment:", label_map[prediction])

# STEP 10: Run chatbot
chat()

# End of the Notebook

