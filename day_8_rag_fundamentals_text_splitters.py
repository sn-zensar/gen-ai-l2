# -*- coding: utf-8 -*-
"""Day_8_RAG_Fundamentals_Text_Splitters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-X8X95gsm2WCtCy44axDcPTEHXoXIS2s
"""

# Text Splitter

# Example 1 - Text Splitters

!pip install -q langchain langchain-openai langchain-community pypdf beautifulsoup4

# Load the data

from langchain_community.document_loaders import TextLoader

loader = TextLoader("/content/sample.txt")
docs = loader.load()
print("Original Length:", len(docs[0].page_content))

# Split the data

from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=50,
    separators=["\n\n", "\n", ".", " ", ""]
)

split_docs = splitter.split_documents(docs)

print(f"üîπ Total Chunks: {len(split_docs)}")
print("üìÑ Sample Chunk:\n", split_docs[0].page_content)

# Summarize each chunk

from google.colab import userdata
import os

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4o", temperature=0.3)

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a summarizer."),
    ("human", "Summarize the following:\n\n{chunk}")
])

chain = prompt | llm

for i, chunk in enumerate(split_docs[:3]):
    summary = chain.invoke({"chunk": chunk.page_content})
    print(f"üßæ Summary {i+1}:\n", summary.content, "\n")

# Custom Splitter - (Text File)

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Step 1: Load file
loader = TextLoader("/content/sample.txt")
docs = loader.load()

# Step 2: Customize splitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,               # Increase if using GPT-4
    chunk_overlap=100,           # Ensures context continuity
    separators=["\n\n", "\n", ".", " "]  # Try paragraph ‚Üí sentence ‚Üí word
)

# Step 3: Split
split_docs = splitter.split_documents(docs)

print(f"üîπ Chunks created: {len(split_docs)}")
print("üìÑ Sample Chunk:\n", split_docs[0].page_content)

# visualize chunk sizes: (To Test)

for i, chunk in enumerate(split_docs[:3]):
    print(f"\n--- Chunk {i+1} ({len(chunk.page_content)} characters) ---")
    print(chunk.page_content)

# Example for pdf - Gen-AI E-book - split using chapters / stages (in this book)

# chapter based generic code below:

# üì¶ Step 1: Install dependencies
!pip install -q langchain langchain-community pypdf

# üì• Step 2: Load and read your PDF
from langchain_community.document_loaders import PyPDFLoader

pdf_path = "/content/Databricks-Big-Book-Of-GenAI-FINAL.pdf"
loader = PyPDFLoader(pdf_path)
pages = loader.load_and_split()

# Combine all text into a single string
full_text = "\n".join([p.page_content for p in pages])

# üìñ Step 3: Split by Chapter Headings (e.g., "Chapter 1", "CHAPTER TWO")
import re

# Flexible regex for headings like "Chapter 1", "CHAPTER ONE", etc.
chapters = re.split(r'(Chapter\s+\w+.*?)\n', full_text, flags=re.IGNORECASE)

# Group titles + their content
grouped_chapters = [
    {"title": chapters[i].strip(), "content": chapters[i+1].strip()}
    for i in range(1, len(chapters)-1, 2)
]

print(f"‚úÖ Chapters Detected: {len(grouped_chapters)}")
print("üìÑ First Chapter Title:", grouped_chapters[0]['title'])

# üß± Step 4: Chunk each chapter using RecursiveCharacterTextSplitter
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

chunked_chapters = []
for ch in grouped_chapters:
    chunks = splitter.split_text(ch["content"])
    for i, chunk in enumerate(chunks):
        chunked_chapters.append({
            "chapter": ch["title"],
            "chunk_index": i,
            "text": chunk
        })

# üîç Step 5: View few final chunks
print(f"\nüîπ Total Chunks: {len(chunked_chapters)}")
print(f"üß© Sample Chunk:\n{chunked_chapters[0]}")

# Updated Code: Split by "Stage X" Headings

# üì¶ Step 1: Install necessary packages
!pip install -q langchain langchain-community pypdf

# üì• Step 2: Load the uploaded PDF
from langchain_community.document_loaders import PyPDFLoader

pdf_path = "/content/Databricks-Big-Book-Of-GenAI-FINAL.pdf"
loader = PyPDFLoader(pdf_path)
pages = loader.load_and_split()

# Combine page texts into one long string
full_text = "\n".join([p.page_content for p in pages])

# üìñ Step 3: Split text by "Stage 0", "Stage 1", etc.
import re

# Regex pattern for "Stage" headings (e.g., Stage 0, STAGE ONE)
stages = re.split(r'(Stage\s+\w+.*?)\n', full_text, flags=re.IGNORECASE)

# Group into title-content pairs
grouped_stages = [
    {"title": stages[i].strip(), "content": stages[i+1].strip()}
    for i in range(1, len(stages)-1, 2)
]

print(f"‚úÖ Detected {len(grouped_stages)} stages.")
print("üîπ Sample Stage Title:", grouped_stages[0]['title'])

# üß± Step 4: Chunk each stage using RecursiveCharacterTextSplitter
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)

stage_chunks = []
for stage in grouped_stages:
    chunks = splitter.split_text(stage["content"])
    for i, chunk in enumerate(chunks):
        stage_chunks.append({
            "stage": stage["title"],
            "chunk_index": i,
            "text": chunk
        })

# üìÑ Step 5: View sample result
print(f"\nüî∏ Total chunks across all stages: {len(stage_chunks)}")
print(f"üß© Example Chunk:\n{stage_chunks[0]}")

# Lets summarize (each chapter) Stage Chunks (slow in execution)

# ‚úÖ Prompt and LLM (using latest syntax)
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

prompt = PromptTemplate.from_template(
    "Please summarize the following GenAI content:\n\n{text}\n\nSummary:"
)

# üß† Define the chain using | (pipe syntax)
summarizer_chain = prompt | llm

# ‚úÖ Run summaries using .invoke() (new standard)
summaries = []
for item in stage_chunks:
    response = summarizer_chain.invoke({"text": item["text"]})
    summaries.append({
        "stage": item["stage"],
        "chunk_index": item["chunk_index"],
        "summary": response.content.strip()
    })

# üîç Show sample
for s in summaries[:2]:
    print(f"\nüìå Stage: {s['stage']} | Chunk #{s['chunk_index']}\nüìù Summary:\n{s['summary']}")

# use FAISS and Vector db like chrome db

!pip install -U langchain langchain-openai langchain-community faiss-cpu chromadb

# code for both FAISS and Chroma db below

# Step 1: Imports
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS, Chroma
from langchain.chains import RetrievalQA
import tempfile

# Step 2: Sample content (simulating parsed PDF/book chunks)
sample_chunks = [
    {"stage": "Stage 0", "chunk_index": 0, "text": "Generative AI is a branch of AI focused on creating content like text, images, and code using large models."},
    {"stage": "Stage 1", "chunk_index": 1, "text": "Large Language Models such as GPT are trained on vast datasets and can generate human-like responses to prompts."}
]

# Step 3: LLM and summarization setup
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
summary_prompt = PromptTemplate.from_template("Summarize this:\n\n{text}\n\nSummary:")
summarizer_chain = summary_prompt | llm

# Step 4: Run summarization
summaries = []
for item in sample_chunks:
    response = summarizer_chain.invoke({"text": item["text"]})
    summaries.append({
        "stage": item["stage"],
        "chunk_index": item["chunk_index"],
        "summary": response.content.strip()
    })

# Step 5: Create Document objects
docs = [
    Document(page_content=summary["summary"], metadata={"stage": summary["stage"], "chunk": summary["chunk_index"]})
    for summary in summaries
]

# Step 6: Initialize embeddings
embedding_model = OpenAIEmbeddings()

# Step 7a: Store summaries in FAISS
faiss_vectorstore = FAISS.from_documents(docs, embedding_model)
faiss_qa = RetrievalQA.from_chain_type(llm=llm, retriever=faiss_vectorstore.as_retriever())

# Step 7b: Store summaries in Chroma
chroma_path = tempfile.mkdtemp()
chroma_vectorstore = Chroma.from_documents(docs, embedding_model, persist_directory=chroma_path)
chroma_vectorstore.persist()
chroma_qa = RetrievalQA.from_chain_type(llm=llm, retriever=chroma_vectorstore.as_retriever())

# Step 8: Query both vector stores
query = "What is Generative AI?"
faiss_response = faiss_qa.run(query)
chroma_response = chroma_qa.run(query)

# Step 9: Print results
print("\nüìå FAISS Response:")
print(faiss_response)

print("\nüìå Chroma Response:")
print(chroma_response)

# Now lets implement RAG Pipeline (with retrievers and llm)

# Full Code: PDF ‚Üí Chunk ‚Üí Embed ‚Üí RAG (FAISS + GPT)

# STEP 1: Imports
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnableSequence
from langchain_core.output_parsers import StrOutputParser

# STEP 2: Load PDF and split into chunks
loader = PyPDFLoader("/content/Databricks-Big-Book-Of-GenAI-FINAL.pdf")
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
chunks = splitter.split_documents(docs)

# STEP 3: Create FAISS vector store
embedding_model = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(chunks, embedding_model)
retriever = vectorstore.as_retriever()

# STEP 4: Create prompt template
prompt = ChatPromptTemplate.from_template("""
You are a helpful assistant. Use the following context to answer the question.

Context:
{context}

Question:
{question}
""")

# STEP 5: Define LLM and pipeline
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# Use RunnableParallel to pass only the question to the retriever
rag_chain = RunnableParallel({
    "context": lambda x: retriever.invoke(x["question"]),
    "question": lambda x: x["question"]
}) | prompt | llm | StrOutputParser()

# STEP 6: Ask your question
query = {"question": "What is GenAI according to this book?"}
response = rag_chain.invoke(query)

# STEP 7: Show result
print("üìò Answer from PDF (RAG):\n")
print(response)

# Full Code: PDF ‚Üí Chunk ‚Üí Embed ‚Üí RAG (Chroma + GPT)

# STEP 1: Install required packages
!pip install -U langchain langchain-openai langchain-community chromadb pypdf

# STEP 2: Imports
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnableSequence
from langchain_core.output_parsers import StrOutputParser

# STEP 3: Load and split PDF
loader = PyPDFLoader("/content/Databricks-Big-Book-Of-GenAI-FINAL.pdf")
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
chunks = splitter.split_documents(docs)

# STEP 4: Create Chroma DB (in-memory or persistent path)
embedding_model = OpenAIEmbeddings()
chroma_db = Chroma.from_documents(
    documents=chunks,
    embedding=embedding_model,
    persist_directory="/content/chroma_store"  # Change path to persist
)

retriever = chroma_db.as_retriever()

# STEP 5: Prompt template
prompt = ChatPromptTemplate.from_template("""
You are a helpful assistant. Use the following context to answer the question.

Context:
{context}

Question:
{question}
""")

# STEP 6: RAG chain (Retriever + LLM)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

rag_chain = RunnableParallel({
    "context": lambda x: retriever.invoke(x["question"]),
    "question": lambda x: x["question"]
}) | prompt | llm | StrOutputParser()

# STEP 7: Ask a question
query = {"question": "What are the stages mentioned in this GenAI book?"}
response = rag_chain.invoke(query)

# STEP 8: Output
print("üìò Answer from Chroma RAG:")
print(response)

# Now lets make the prompts dynamic

# RAG with Chroma + GPT ‚Äî Dynamic User Input Version

# STEP 1: Imports (if not already imported above)
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel
from langchain_core.output_parsers import StrOutputParser

# STEP 2: Load PDF and split (do once)
loader = PyPDFLoader("/content/Databricks-Big-Book-Of-GenAI-FINAL.pdf")
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
chunks = splitter.split_documents(docs)

# STEP 3: Create Chroma DB
embedding_model = OpenAIEmbeddings()
chroma_db = Chroma.from_documents(
    documents=chunks,
    embedding=embedding_model,
    persist_directory="/content/chroma_store"  # persist path
)
retriever = chroma_db.as_retriever()

# STEP 4: Define prompt and model
prompt = ChatPromptTemplate.from_template("""
You are a helpful assistant. Use the following context to answer the question.

Context:
{context}

Question:
{question}
""")

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# STEP 5: RAG Chain
rag_chain = RunnableParallel({
    "context": lambda x: retriever.invoke(x["question"]),
    "question": lambda x: x["question"]
}) | prompt | llm | StrOutputParser()

# STEP 6: Dynamic user input loop
print("üí¨ RAG Chat started! Type 'exit' to stop.\n")
while True:
    user_question = input("You: ")
    if user_question.lower() in ["exit", "quit"]:
        print("üëã Ending RAG chat.")
        break

    query = {"question": user_question}
    response = rag_chain.invoke(query)
    print("Bot:", response)

# Example